{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project: Teaching an LLM to Reason\n",
        "\n",
        "In this project, you will teach an LLM to use step-by-step reasoning to answer the question: \"How many X's are there in the word Y?\"\n",
        "\n",
        "Counting letters in a word is a surprisingly complex task for an LLM. Just as human beings would not be able to answer such a question for longer words without breaking down the word into its individual letters and then counting them, LLMs cannot be similarly expected to be able to respond without using smaller reasoning steps.\n",
        "\n",
        "For example, to count the number of o's in the word room, one could use the following reasoning:\n",
        "\n",
        "```\n",
        "Question: How many of the letter \"o\" are there in the word \"room\"\n",
        "Answer: 2\n",
        "Response:\n",
        "\n",
        "<reasoning>\n",
        "Letter-by-letter spelling:\n",
        "1. r - 0 o's so far\n",
        "2. o - 1 o's so far\n",
        "3. o - 2 o's so far\n",
        "4. m - 2 o's so far\n",
        "\n",
        "The letter \"o\" appears 2 times in the word \"room\".\n",
        "</reasoning>\n",
        "<answer>\n",
        "2\n",
        "</answer>\n",
        "```\n",
        "\n",
        "In this project we will use the reinforcement learning method GRPO (Group Relative Policy Optimization, of DeepSeek fame) to take a large language model that has been fine-tuned for following instructions and teach it how to break a word down into its letters and then count the requested letter.\n",
        "\n",
        "We will complete the following steps:\n",
        "\n",
        "* Set up the notebook\n",
        "* Create a letter-counting dataset\n",
        "* Create the reward functions\n",
        "* Train the model\n",
        "* View the results\n",
        "\n",
        "NOTE: This notebook will have you focus on several important aspects of training a GPRO model using LoRA:\n",
        "\n",
        "1. Configuring LoRA adapters for parameter-efficient fine tuning\n",
        "2. Selecting reward functions that help the model efficiently find its way to the correct answer (also called reward shaping)\n",
        "3. Finding hyperparameters that help the model increase the rewards earned more quickly and reliably\n",
        "4. Learning how to start with smaller experiments and to work your way up to longer experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the notebook\n",
        "\n",
        "We'll install dependencies needed for the project, namely `unsloth` and `vllm`, which are useful for fine-tuning LLMs with even just 15GB of VRAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 216 Î¼s (started: 2025-12-24 12:41:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Load ipython-autotime to see how long each cell take to run\n",
        "# No changes needed in this cell\n",
        "\n",
        "!pip install -q ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Dec 24 12:41:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
            "| N/A   28C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 334 ms (started: 2025-12-24 12:41:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Verify we have enough GPU memory to run this project (at least 15360MiB)\n",
        "# No changes needed in this cell\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMMENTS:\n",
        "\n",
        "#### lora_rank = 64 was chosen because it is a good middle ground choice between capacity and efficiency. Also becaue we had memory constrains.\n",
        "#### fast_interface=False.  I try multiple choices and was not able to make this run unless I have it as False.\n",
        "#### target_modules. Our task will requiere the different roles, the Attention layers and MLP layers. That is, we are giving the model the flexibility to learn what to attend to and how to process and format it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/voc/data/venv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 12-24 12:42:08 [__init__.py:241] Automatically detected platform cuda.\n",
            "ERROR 12-24 12:42:13 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.7: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.9.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1min 47s (started: 2025-12-24 12:41:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Load the `Qwen 2.5 3B Instruct`, and set parameters for the project\n",
        "# The first time unsloth is imported, it will do its magic and patch the modules\n",
        "# it works with. This may 2-5 minutes.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "import unsloth\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 256  # Increase if you get errors about the sequence length\n",
        "\n",
        "# Set the LoRA rank to an appropriate value\n",
        "# Read about setting LoRA rank:\n",
        "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "# lora_rank = **********  # Explain your choice\n",
        "lora_rank = 64\n",
        "\n",
        "# Load the Instruct model in 4-bit mode\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,  # We'll use quantization!\n",
        "    fast_inference=False,  # Changed from True to False\n",
        "    max_lora_rank=lora_rank,\n",
        "    gpu_memory_utilization=0.75,  # You can reduce this if you get an memory error\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        # Read about choosing adapters for LoRA:\n",
        "        # https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "        # Choose the target modules/adapters for your LoRA model\n",
        "        # ********** # Explain your choice\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth enables longer contexts\n",
        "    # See: https://github.com/unslothai/unsloth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try Prompt Engineering to Count Letters\n",
        "\n",
        "Let's work on the system prompt a little to see if we can get the model to count the number of the letter `g` in `engage`.\n",
        "\n",
        "\n",
        "Here you must:\n",
        "* Write clear instructions\n",
        "* Break the problem down into steps (Chain-of-Thought prompting)\n",
        "* Provide at least one example for the model to follow (Few-shot prompting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TEXT FOR COMPLETION ===\n",
            "<|im_start|>system\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "How many of the letter \"g\" are there in the word \"engage\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "=== GENERATED OUTPUT ===\n",
            "system\n",
            "\n",
            "user\n",
            "How many of the letter \"g\" are there in the word \"engage\"\n",
            "assistant\n",
            "In the word \"engage\", there is only one letter \"g\".\n",
            "time: 4.27 s (started: 2025-12-24 12:43:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# First, let's see what happens when we have a blank system prompt\n",
        "# No changes needed in this cell\n",
        "SYSTEM_PROMPT = \"\"\"\"\"\"\n",
        "USER_PROMPT = 'How many of the letter \"g\" are there in the word \"engage\"'\n",
        "\n",
        "# Convert the chat messages to a single string so the model can complete it\n",
        "text_for_completion = tokenizer.apply_chat_template(\n",
        "    conversation=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        },\n",
        "    ],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text_for_completion], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate the text completion using standard generate (not fast_generate)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the text input for the model and the model's output\n",
        "print(\"=== TEXT FOR COMPLETION ===\")\n",
        "print(text_for_completion)\n",
        "print(\"=== GENERATED OUTPUT ===\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without any prompting the model will generate an output such as this:\n",
        "\n",
        "```\n",
        "=== GENERATED OUTPUT ===\n",
        "There is one letter \"g\" in the word \"engage\".\n",
        "```\n",
        "\n",
        "Now let's work on the system prompt to help the model break this problem down into steps, which might help it get the right answer (2 `g`'s in `engage`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did your new prompt get the right answer? Did the model follow all of your instructions?\n",
        "\n",
        "Maybe yes, maybe no. Either way, we'll want the model to reliably complete this challenge. So let's use GRPO to help it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a letter-counting dataset\n",
        "\n",
        "To train a model, we'll first need to create a dataset. We'll use the HuggingFace `datasets` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['idea',\n",
              " 'glow',\n",
              " 'rust',\n",
              " 'maze',\n",
              " 'echo',\n",
              " 'wisp',\n",
              " 'veto',\n",
              " 'lush',\n",
              " 'gaze',\n",
              " 'knit']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 5.79 ms (started: 2025-12-24 12:43:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create a list of words of different lengths\n",
        "# No changes are needed in this cell.\n",
        "\n",
        "ALL_WORDS = [\n",
        "    \"idea\",\n",
        "    \"glow\",\n",
        "    \"rust\",\n",
        "    \"maze\",\n",
        "    \"echo\",\n",
        "    \"wisp\",\n",
        "    \"veto\",\n",
        "    \"lush\",\n",
        "    \"gaze\",\n",
        "    \"knit\",\n",
        "    \"fume\",\n",
        "    \"plow\",\n",
        "    \"void\",\n",
        "    \"oath\",\n",
        "    \"grim\",\n",
        "    \"crisp\",\n",
        "    \"lunar\",\n",
        "    \"fable\",\n",
        "    \"quest\",\n",
        "    \"verge\",\n",
        "    \"brawn\",\n",
        "    \"elude\",\n",
        "    \"aisle\",\n",
        "    \"ember\",\n",
        "    \"crave\",\n",
        "    \"ivory\",\n",
        "    \"mirth\",\n",
        "    \"knack\",\n",
        "    \"wryly\",\n",
        "    \"onset\",\n",
        "    \"mosaic\",\n",
        "    \"velvet\",\n",
        "    \"sphinx\",\n",
        "    \"radius\",\n",
        "    \"summit\",\n",
        "    \"banner\",\n",
        "    \"cipher\",\n",
        "    \"glisten\",\n",
        "    \"mantle\",\n",
        "    \"scarab\",\n",
        "    \"expose\",\n",
        "    \"fathom\",\n",
        "    \"tavern\",\n",
        "    \"fusion\",\n",
        "    \"relish\",\n",
        "    \"lantern\",\n",
        "    \"enchant\",\n",
        "    \"torrent\",\n",
        "    \"capture\",\n",
        "    \"orchard\",\n",
        "    \"eclipse\",\n",
        "    \"frescos\",\n",
        "    \"triumph\",\n",
        "    \"absolve\",\n",
        "    \"gossipy\",\n",
        "    \"prelude\",\n",
        "    \"whistle\",\n",
        "    \"resolve\",\n",
        "    \"zealous\",\n",
        "    \"mirage\",\n",
        "    \"aperture\",\n",
        "    \"sapphire\",\n",
        "]\n",
        "\n",
        "print(len(ALL_WORDS))\n",
        "\n",
        "ALL_WORDS[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'words': 'idea', 'letters': 'a', 'counts': 1}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 371 ms (started: 2025-12-24 12:43:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset as a Hugging Face Dataset using Dataset.from_generator\n",
        "# No changes needed in this cell\n",
        "\n",
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "\n",
        "# Go through the letters from the words (as well as letters not in the words),\n",
        "# and create a labelled dataset with all the different combinations.\n",
        "# For example for the word gaze:\n",
        "# 1. How many i's are in idea? <-- count should be 1\n",
        "# 2. How many d's are in idea? <-- count should be 1\n",
        "# 3. How many e's are in idea? <-- count should be 1\n",
        "# 4. How many a's are in idea? <-- count should be 1\n",
        "# 5. How many b's are in idea? <-- a letter not in word (count should be zero)\n",
        "def generate_records():\n",
        "    for word in ALL_WORDS:\n",
        "        for letter in sorted(set(word)):\n",
        "            yield {\"words\": word, \"letters\": letter, \"counts\": word.count(letter)}\n",
        "\n",
        "        # pick random letters not in the word\n",
        "        num_letters_not_in_word_left = int(len(word) // 7 + 1)\n",
        "\n",
        "        random.seed(hash(word))\n",
        "\n",
        "        all_letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "        random.shuffle(all_letters)\n",
        "        for letter in all_letters:\n",
        "            if letter not in word:\n",
        "                yield {\"words\": word, \"letters\": letter, \"counts\": 0}\n",
        "                num_letters_not_in_word_left -= 1\n",
        "            if num_letters_not_in_word_left == 0:\n",
        "                break\n",
        "\n",
        "\n",
        "ds = Dataset.from_generator(generate_records)\n",
        "\n",
        "# Show the first item\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'words': 'idea',\n",
              " 'letters': 'a',\n",
              " 'counts': 1,\n",
              " 'prompt': [{'content': \"\\nRespond in the following format:\\n<reasoning>\\nCounting the number of [letter_to_count]'s in the word [word]\\n1. [first letter] - [count of requested letter so far] so far\\n2. [second letter] - [count of requested letter so far] so far\\n...\\n</reasoning>\\n<answer>\\n[number]\\n</answer>\\n\",\n",
              "   'role': 'system'},\n",
              "  {'content': 'How many of the letter \"a\" are there in the word \"idea\"',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 33.4 ms (started: 2025-12-24 12:43:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Add the entire prompt (system + user) and the answer to the dataset\n",
        "# We'll use a prompt that spells out the word letter-by-letter\n",
        "# No changes needed in this cell\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Simple CoT prompt (zero-shot)\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "Counting the number of [letter_to_count]'s in the word [word]\n",
        "1. [first letter] - [count of requested letter so far] so far\n",
        "2. [second letter] - [count of requested letter so far] so far\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "[number]\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "ds = ds.map(\n",
        "    lambda x: {  # type: ignore\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": 'How many of the letter \"{}\" are there in the word \"{}\"'.format(\n",
        "                    x[\"letters\"], x[\"words\"]\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Counting the number of a's in the word idea\n",
            "1. i - 0 so far\n",
            "2. d - 1 so far\n",
            "3. e - 2 so far\n",
            "4. a - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "time: 4.45 s (started: 2025-12-24 12:43:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's see how well the model runs out-of-the-box\n",
        "# No changes needed in this cell\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    ds[0][\"prompt\"], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using standard generate method\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Reward Functions\n",
        "\n",
        "One goal of creating reward functions is to guide the model toward behaviors that help it reach its goal (counting the occurrences of a letter within a word) more easily. Since there is more than one way to carry out any step-by-step task (e.g. whether or not you use bullet points to separate your steps), there's a bit of judgement involved in choosing what behaviors to reward, i.e. how do we provide partial credit or \"shape\" our rewards?\n",
        "\n",
        "In this case we will encourage the model to (whether or not this structure is best):\n",
        "* use numbers for bullet points when spelling out the word\n",
        "* to spell the word correctly\n",
        "* to count the requested letter correctly\n",
        "* to use the requested reasoning format\n",
        "* to get the final answer correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numbering reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.0, 0.5]\n",
            "time: 2.91 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's work on a function that the numbering in the bullet points is correct\n",
        "# When using GRPO, we lean on reward functions that are relatively easy to\n",
        "# compute, thus removing the need to have a second large model just for\n",
        "# evaluation.\n",
        "# In this case, we'll use regular expressions quite a bit.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_letter_numbering(response):\n",
        "    \"\"\"Extract the numbers at the beginning of the line\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    4. a - 2 so far\n",
        "    5. l - 2 so far\n",
        "    returns [1, 2, 3, 4, 5]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # We use a regular expression to find lines of the form:\n",
        "    # '\\n[number]. [letter]'\n",
        "    pattern = r\"\\n(\\d+). [a-z]\"\n",
        "\n",
        "    # Use `re` to find all matches of the pattern in the response\n",
        "    # matches = **********\n",
        "    matches = re.findall(pattern, response)\n",
        "    if matches:\n",
        "        return [int(m) for m in matches]\n",
        "    return []\n",
        "\n",
        "\n",
        "assert extract_letter_numbering(\n",
        "    \"\"\"\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. a - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == [1, 2, 3, 4, 5]\n",
        "\n",
        "\n",
        "def numbering_reward_func(completions, words, **kwargs) -> list[float]:\n",
        "    \"\"\"Provides a reward for getting the numbering at the beginning of the line correct\n",
        "\n",
        "    1. g - 1 so far <-- Good in-order numbering\n",
        "    2. o - 1 so far <-- Good in-order numbering\n",
        "    3. a - 2 so far <-- Good in-order numbering\n",
        "    3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "    1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "    1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "\n",
        "    \"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "    for response, word in zip(responses, words):\n",
        "        reward = 0\n",
        "\n",
        "        for ix, spell_number in enumerate(extract_letter_numbering(response)):\n",
        "            line_number = ix + 1\n",
        "\n",
        "            # Get points for in-order numbering\n",
        "            if spell_number == line_number:\n",
        "                # TODO: Provide a reward for in-order numbering\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward += ***********\n",
        "                reward += 1\n",
        "            # Otherwise lose points\n",
        "            else:\n",
        "                # TODO: Provide a reward for out-of-order numbering\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward -= ***********\n",
        "                reward -= 1\n",
        "\n",
        "            # Lose extra points for continuing beyond the length of the word\n",
        "            if line_number > len(word):  # We use the index of the line\n",
        "                # TODO: Provide a reward for continuing beyond the length of the word\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward -= ***********\n",
        "                reward -= 2\n",
        "\n",
        "        res.append(reward / len(word))\n",
        "    return res\n",
        "\n",
        "\n",
        "res = numbering_reward_func(\n",
        "    completions=[\n",
        "        [\n",
        "            {  # Worse response\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far <-- Good in-order numbering\n",
        "2. o - 1 so far <-- Good in-order numbering\n",
        "3. a - 2 so far <-- Good in-order numbering\n",
        "3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {  # Better response\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far <-- Good in-order numbering\n",
        "2. o - 1 so far <-- Good in-order numbering\n",
        "3. a - 2 so far <-- Good in-order numbering\n",
        "3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            },\n",
        "        ],\n",
        "    ],\n",
        "    words=[\"goal\", \"goal\"],\n",
        ")\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spelling reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-234.0, -210.0]\n",
            "time: 46.3 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward correct spelling of the word\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_spelling(response):\n",
        "    \"\"\"Extract the spelling from the response\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    3. l - 2 so far\n",
        "    5. l - 2 so far\n",
        "    Returns \"goall\"\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"\\n\\d+. ([a-z])\"\n",
        "    matches = re.findall(pattern, response, flags=re.IGNORECASE)\n",
        "    if matches:\n",
        "        return \"\".join([m for m in matches])\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "extract_spelling(\n",
        "    \"\"\"Here is a letter by letter spelling:\n",
        "\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "3. l - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == \"goall\"\n",
        "\n",
        "\n",
        "def spelling_reward_func(completions, words, **kwargs) -> list[float]:\n",
        "    \"\"\"A spelling reward function.\"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for word, response in zip(words, responses):\n",
        "        reward = 0.0\n",
        "\n",
        "        # Provide a reward for exactly correct spelling\n",
        "        # reward += **********\n",
        "        if response.strip().lower() == word.lower():\n",
        "            reward += 10  # Large bonus for perfect spelling\n",
        "\n",
        "        # Provide a reward for each letter of difference in length\n",
        "        # reward -= **********\n",
        "        length_diff = abs(len(response.strip()) - len(word))\n",
        "        reward -= length_diff * 0.5  # Penalty per character difference\n",
        "\n",
        "\n",
        "        # Provide a reward for each letter that is not in the target word\n",
        "        # reward -= **********\n",
        "        word_counter = Counter(word.lower())\n",
        "        response_counter = Counter(response.strip().lower())\n",
        "        extra_letters = sum((response_counter - word_counter).values())\n",
        "        reward -= extra_letters * 1  # Penalty for extra letters\n",
        "\n",
        "        # Provide a reward for each letter that is in the target word but not in the response\n",
        "        # reward -= **********\n",
        "        missing_letters = sum((word_counter - response_counter).values())\n",
        "        reward -= missing_letters * 1  # Penalty for missing letters\n",
        "\n",
        "        res.append(reward)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = spelling_reward_func(\n",
        "    completions=[\n",
        "        [  # Worse response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. l - 2 so far\n",
        "5. l - 2 so far\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        [  # Better Response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. l - 2 so far\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    words=[\"goal\", \"goal\"],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.2, 0.2]\n",
            "time: 49.4 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's reward the model for properly counting the occurrences of a letter in a word\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "# No changes needed in this cell, but feel free to experiment with variations on the prompt\n",
        "\n",
        "\n",
        "def get_resp_letters_and_counts(response):\n",
        "    \"\"\"Extract the letters and counts from the response\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    4. a - 2 so far\n",
        "    5. l - 2 so far\n",
        "    returns [('g', 1), ('o', 1), ('a', 2), ('a', 2), ('l', 2)]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"\\n(\\d+)\\. ([a-z])\\D*(\\d+)\"\n",
        "\n",
        "    # Find strings matching e.g. \"2. a - 2 so far\"\n",
        "    matches = re.findall(pattern, response, flags=re.IGNORECASE)\n",
        "\n",
        "    if not matches:\n",
        "        return []\n",
        "\n",
        "    return [\n",
        "        (matched_letter, matched_count_so_far)\n",
        "        for _, matched_letter, matched_count_so_far in matches\n",
        "    ]\n",
        "\n",
        "\n",
        "assert get_resp_letters_and_counts(\n",
        "    \"\"\"\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. a - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == [(\"g\", \"1\"), (\"o\", \"1\"), (\"a\", \"2\"), (\"a\", \"2\"), (\"l\", \"2\")]\n",
        "\n",
        "\n",
        "def counting_reward_func(completions, letters, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    # Iterate over each of the letter-response pairs\n",
        "    for letter, response in zip(letters, responses):\n",
        "        reward = 0\n",
        "\n",
        "        letters_and_counts = get_resp_letters_and_counts(response)\n",
        "\n",
        "        # If there are no matches, provide a negative reward\n",
        "        if not letters_and_counts:\n",
        "            res.append(-1)\n",
        "            continue\n",
        "\n",
        "        # Start counting the matching letters\n",
        "        actual_count = 0\n",
        "        for resp_letter, resp_count in letters_and_counts:\n",
        "            # If there's a match, count the letter\n",
        "            if letter == resp_letter:\n",
        "                actual_count += 1\n",
        "\n",
        "            # If the count is accurate, add a reward, else subtract a reward\n",
        "            # if ... **********\n",
        "            # else ... **********\n",
        "                if int(resp_count) == actual_count:\n",
        "                    reward += 1\n",
        "                else:\n",
        "                    reward -= 1\n",
        "\n",
        "        # Return the reward normalized by the length of the matches\n",
        "        # res.append(**********)\n",
        "        res.append(reward / len(letters_and_counts))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "res = counting_reward_func(\n",
        "    completions=[\n",
        "        [  # Worse response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\\nHere is a letter by letter spelling:\n",
        "\n",
        "1. g - 0 so far\n",
        "2. o - 0 so far\n",
        "3. a - 1 so far\n",
        "4. a - 2 so far\n",
        "5. l - 0 so far\n",
        "\n",
        "\\n</reasoning>\\n<answer>\\nThis is my answer.\\n</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        [  # Better response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\\nHere is a letter by letter spelling:\n",
        "\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 1 so far\n",
        "4. a - 1 so far\n",
        "5. l - 1 so far\n",
        "\n",
        "\\n</reasoning>\\n<answer>\\nThis is my answer.\\n</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    letters=[\"g\", \"g\"],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Formatting reward functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0, 1.0]\n",
            "time: 50.4 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward the model for providing the response in a specific format\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"Extracts the string between <answer> and </answer> tags.\"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"<answer>(.*?)</answer>\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "assert (\n",
        "    extract_xml_answer(\"\"\"\n",
        "<reasoning>\n",
        "This is my reasoning.\n",
        "</reasoning>\n",
        "<answer>SUPERCALIFRAGILISTICEXPIALIDOCIOUS</answer>\n",
        "\"\"\")\n",
        "    == \"SUPERCALIFRAGILISTICEXPIALIDOCIOUS\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"\\s*<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for completion in completions:\n",
        "        reward = 0.0\n",
        "\n",
        "        # Extract the response content\n",
        "        response = completion[0][\"content\"]\n",
        "\n",
        "        # Check if the response matches the pattern\n",
        "        match = re.match(pattern, response, flags=re.MULTILINE | re.DOTALL)\n",
        "\n",
        "        # If it matches, return 0.5, otherwise return 0.0\n",
        "        # if ... **********\n",
        "        if match:\n",
        "            reward += 0.5\n",
        "        # Extract the answer from the response\n",
        "        # extracted_answer = **********\n",
        "        extracted_answer = extract_xml_answer(response)\n",
        "        # If the answer is an integer, add 0.5 to the reward\n",
        "        # if ... **********\n",
        "        try:\n",
        "            int(extracted_answer)\n",
        "            reward += 0.5\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "        res.append(reward)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = format_reward_func(\n",
        "    completions=[\n",
        "        [{\"content\": \"This is my answer\"}],\n",
        "        [\n",
        "            {\n",
        "                \"content\": \"<reasoning>\\nThis is my reasoning.\\n</reasoning>\\n<answer>\\n3\\n</answer>\"\n",
        "            }\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task correctness reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many...\n",
            "Answer: 0\n",
            "Response: <reasoning>.../reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "[0.0, 1.0]\n",
            "time: 43.8 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward the model for providing the correct answer\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def correct_answer_reward_func(prompts, completions, counts, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward the final answer if it is correct.\"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    # Print a nice summary of the first prompt, answer, and response to see while training\n",
        "    print(f\"\"\"\n",
        "{\"-\" * 20}\n",
        "Question: {prompts[0][-1][\"content\"]}\n",
        "Answer: {counts[0]}\n",
        "Response: {responses[0]}\n",
        "Extracted: {extracted_responses[0]}\n",
        "Correct: {str(extracted_responses[0]) == str(counts[0])}!\n",
        "    \"\"\")\n",
        "\n",
        "    res = [\n",
        "        # Provide reward for exactly correct answer\n",
        "        # **********  # Complete the list comprehension\n",
        "        1.0 if str(r) == str(a) else 0.0\n",
        "        for r, a in zip(extracted_responses, counts)\n",
        "    ]\n",
        "    return res\n",
        "\n",
        "\n",
        "res = correct_answer_reward_func(\n",
        "    prompts=[\n",
        "        [{\"content\": \"\"\"How many...\"\"\"}],\n",
        "        [{\"content\": \"\"\"How many...\"\"\"}],\n",
        "    ],\n",
        "    completions=[\n",
        "        [{\"content\": \"\"\"<reasoning>.../reasoning>\\n<answer>\\n3\\n</answer>\"\"\"}],\n",
        "        [{\"content\": \"\"\"<reasoning>.../reasoning>\\n<answer>\\n3\\n</answer>\"\"\"}],\n",
        "    ],\n",
        "    letters=[\"g\", \"g\"],\n",
        "    counts=[0, 3],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List the reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 50.5 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# List out the reward functions we will use\n",
        "# No changes needed in this cell\n",
        "\n",
        "REWARD_FUNCS = [\n",
        "    numbering_reward_func,\n",
        "    spelling_reward_func,\n",
        "    counting_reward_func,\n",
        "    format_reward_func,\n",
        "    correct_answer_reward_func,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "Now set up GRPO Trainer and configurations!\n",
        "\n",
        "As you run the trainer, the goal is to see the various `reward` columns increase.\n",
        "\n",
        "After 50 steps or more, you may notice some of the reward standard deviations begin to decrease, meaning that the different predictions are starting to converge on solutions that give similar rewards. If your model has learned the task, then you'll see the `correct_answer_reward_function` increase to its highest value (check the function to see what that is).\n",
        "\n",
        "Here is an example, which successfully converged on a higher reward. Note, the values you see here will probably be different from yours, especially if your reward amounts are different.\n",
        "\n",
        "| Step | Training Loss | reward   | reward_std | ... | kl      | rewards / correct_answer_reward_function / mean | rewards / correct_answer_reward_function / std |\n",
        "|------|---------------|----------|------------|-----|---------|------------------------------------------|-----------------------------------------|\n",
        "| 1    | 0.000000      | 7.961805 | 2.368493   | ... | 0.020369| 0.875000                                 | 1.024695                                |\n",
        "| 2    | 0.000000      | 7.937500 | 1.352467   | ... | 0.016483| 0.875000                                 | 1.024695                                |\n",
        "| 3    | 0.000000      | 1.894792 | 6.462189   | ... | 0.013677| 0.375000                                 | 0.806226                                |\n",
        "| ...  | ...           | ...      | ...        | ... | ...     | ...                                      | ...                                     |\n",
        "| 398  | 0.000100      | 13.000000| 0.000000   | ... | 0.088529| 2.000000                                 | 0.000000                                |\n",
        "| 399  | 0.000100      | 13.000000| 0.000000   | ... | 0.088617| 2.000000                                 | 0.000000                                |\n",
        "| 400  | 0.000100      | 13.000000| 0.000000   | ... | 0.096202| 2.000000                                 | 0.000000                                |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 49.9 ms (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Fill in the GRPO Parameters we'll use throughout this project\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "# Read about the GRPO params here https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "COMMON_GRPO_TRAINING_PARAMS = dict(\n",
        "    # Set appropriate values for `learning_rate` and `beta`\n",
        "    # See: https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "    # See: https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "    learning_rate=5e-5,  # Conservative learning rate for stable training\n",
        "    beta=0.1,  # KL divergence penalty weight (typical range: 0.01-0.5)\n",
        "    # Set the batch size appropriately for your hardware. For GRPO there are a number of parameters to set.\n",
        "    # If you are not sure about your GPU, assume you have a T4. See the memory specs here:\n",
        "    # https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/T4%20Product%20Brief.pdf\n",
        "    per_device_train_batch_size=4,  # per_device_train_batch_size / num_generations determines the number of simultaneous prompts to consider.\n",
        "    # Note: Set per_device_train_batch_size to at most 16 on the Vocareum T4 for best stability\n",
        "    num_generations=4,  # Determines the number of completions/generations to compute for each single prompt\n",
        "    gradient_accumulation_steps=4,  # This parameter allow us to consider multiple steps in a single optimization step\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,  # Set to 1 for a full training run\n",
        "    save_steps=250,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"none\",  # Setting this value lets us use Weights and Biases\n",
        "    output_dir=\"outputs\",\n",
        "    use_vllm=False,  # vll speeds up inference! See https://github.com/vllm-project/vllm\n",
        "    torch_compile=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick train\n",
        "\n",
        "Let's train the model for just 5 steps (`max_steps=5`). As it runs we can double check we've set up our prompts correctly before running for a longer amount of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 1 | Total steps = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n",
            "Unsloth: Input IDs of shape torch.Size([16, 262]) with length 262 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 262]) with length 262 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"glisten\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word glisten\n",
            "1. g - 1 so far\n",
            "2. l - 1 so far\n",
            "3. i - 1 so far\n",
            "4. s - 1 so far\n",
            "5. t - 1 so far\n",
            "6. e - 1 so far\n",
            "7. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 02:25, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / numbering_reward_func / mean</th>\n",
              "      <th>rewards / numbering_reward_func / std</th>\n",
              "      <th>rewards / spelling_reward_func / mean</th>\n",
              "      <th>rewards / spelling_reward_func / std</th>\n",
              "      <th>rewards / counting_reward_func / mean</th>\n",
              "      <th>rewards / counting_reward_func / std</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / correct_answer_reward_func / mean</th>\n",
              "      <th>rewards / correct_answer_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-330.869629</td>\n",
              "      <td>92.427345</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.756696</td>\n",
              "      <td>0.266554</td>\n",
              "      <td>-333.656250</td>\n",
              "      <td>141.137131</td>\n",
              "      <td>0.217411</td>\n",
              "      <td>0.324859</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.403113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-262.907990</td>\n",
              "      <td>21.924595</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>-265.406250</td>\n",
              "      <td>59.312233</td>\n",
              "      <td>0.123264</td>\n",
              "      <td>0.164670</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.516398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-270.829468</td>\n",
              "      <td>30.091831</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.891369</td>\n",
              "      <td>0.213485</td>\n",
              "      <td>-273.375000</td>\n",
              "      <td>54.992275</td>\n",
              "      <td>0.216667</td>\n",
              "      <td>0.263459</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.512348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-263.430542</td>\n",
              "      <td>25.092510</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.136083</td>\n",
              "      <td>-265.500000</td>\n",
              "      <td>28.481573</td>\n",
              "      <td>-0.097222</td>\n",
              "      <td>0.162161</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.447214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-251.695618</td>\n",
              "      <td>42.488770</td>\n",
              "      <td>77.375000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.375000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.874256</td>\n",
              "      <td>0.220756</td>\n",
              "      <td>-254.281250</td>\n",
              "      <td>59.116543</td>\n",
              "      <td>0.086384</td>\n",
              "      <td>0.306983</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word sapphire\n",
            "1. s - 0 so far\n",
            "2. a - 1 so far\n",
            "3. p - 1 so far\n",
            "4. a - 2 so far\n",
            "5. r - 2 so far\n",
            "6. p - 3 so far\n",
            "7. h - 3 so far\n",
            "8. a - 4 so far\n",
            "9. y - 4 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "Extracted: 4\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"absolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - 0 so far\n",
            "3. s - 1 so far\n",
            "4. p - 0 so far\n",
            "5. o - 1 so far\n",
            "6. l - 1 so far\n",
            "7. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word \"mirage\"\n",
            "1. m - 0 so far\n",
            "2. i - 0 so far\n",
            "3. r - 0 so far\n",
            "4. a - 0 so far\n",
            "5. g - 1 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word crave\n",
            "1. c - 0 so far\n",
            "2. r - 1 so far\n",
            "3. a - 1 so far\n",
            "4. v - 1 so far\n",
            "5. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "time: 3min 36s (started: 2025-12-24 12:43:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Train for just a few steps for a few minutes\n",
        "# This will allow us to observe the results and make any changes to our reward functions\n",
        "# before starting a longer run. Note, you won't see much change in the average.\n",
        "# reward values\n",
        "# No changes are needed here\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# Short train to check on reward functions\n",
        "training_args = GRPOConfig(\n",
        "    **COMMON_GRPO_TRAINING_PARAMS,\n",
        "    # We'll just run for a modest 5 steps to make sure everything works and to\n",
        "    # estimate the amount of time it will take to run the full training.\n",
        "    max_steps=5,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=REWARD_FUNCS,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "trainer_res = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available columns: dict_keys(['loss', 'grad_norm', 'learning_rate', 'num_tokens', 'completions/mean_length', 'completions/min_length', 'completions/max_length', 'completions/clipped_ratio', 'completions/mean_terminated_length', 'completions/min_terminated_length', 'completions/max_terminated_length', 'rewards/numbering_reward_func/mean', 'rewards/numbering_reward_func/std', 'rewards/spelling_reward_func/mean', 'rewards/spelling_reward_func/std', 'rewards/counting_reward_func/mean', 'rewards/counting_reward_func/std', 'rewards/format_reward_func/mean', 'rewards/format_reward_func/std', 'rewards/correct_answer_reward_func/mean', 'rewards/correct_answer_reward_func/std', 'reward', 'reward_std', 'frac_reward_zero_std', 'completion_length', 'kl', 'epoch', 'step'])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.47 s (started: 2025-12-24 12:47:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show the total (sum) of the rewards as well as the correct_answer_reward_func (means with in the batch)\n",
        "# No changes needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want to graph other columns, check these out\n",
        "print(f\"available columns: {trainer.state.log_history[0].keys()}\")\n",
        "\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "log_df[\"reward\"].plot()\n",
        "log_df[\"rewards/correct_answer_reward_func/mean\"].plot()\n",
        "\n",
        "# Show the legend\n",
        "plt.legend([\"reward\", \"rewards/correct_answer_reward_func/mean\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slower train (1+ hour)\n",
        "\n",
        "If everything looks good, let's go for a longer training session!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n",
            "Unsloth: Input IDs of shape torch.Size([16, 306]) with length 306 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 306]) with length 306 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"glisten\"\n",
            "Answer: 1\n",
            "Response: missupupmiss down downterupterterupterterup downmissmissmissterupmissmissmissmissmissmiss downmissmissteravavavteravteravavavav.Textter.Text.Text.Text.Text.Textter.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Textter.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Textter.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text.Text\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 10:26, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / numbering_reward_func / mean</th>\n",
              "      <th>rewards / numbering_reward_func / std</th>\n",
              "      <th>rewards / spelling_reward_func / mean</th>\n",
              "      <th>rewards / spelling_reward_func / std</th>\n",
              "      <th>rewards / counting_reward_func / mean</th>\n",
              "      <th>rewards / counting_reward_func / std</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / correct_answer_reward_func / mean</th>\n",
              "      <th>rewards / correct_answer_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>41.598200</td>\n",
              "      <td>-1393.750000</td>\n",
              "      <td>59.797073</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>103.995481</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1392.750000</td>\n",
              "      <td>85.624763</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.230800</td>\n",
              "      <td>-10.625000</td>\n",
              "      <td>2.976869</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>8.076927</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>4.010403</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.960200</td>\n",
              "      <td>-10.843750</td>\n",
              "      <td>1.687500</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>7.400515</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.843750</td>\n",
              "      <td>2.681534</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.379600</td>\n",
              "      <td>-10.531250</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.948930</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.531250</td>\n",
              "      <td>2.125000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.181900</td>\n",
              "      <td>-9.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>10.454723</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.500000</td>\n",
              "      <td>2.183270</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>11.820400</td>\n",
              "      <td>-8.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>29.551118</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.192600</td>\n",
              "      <td>-9.906250</td>\n",
              "      <td>1.312500</td>\n",
              "      <td>1.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>12.981584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.906250</td>\n",
              "      <td>3.023347</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.501900</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.754711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>4.794600</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>11.986470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>1.688194</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.420200</td>\n",
              "      <td>-8.531250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>6.050378</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.531250</td>\n",
              "      <td>1.901480</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.623000</td>\n",
              "      <td>-8.875000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>6.557382</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.875000</td>\n",
              "      <td>1.688194</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.936600</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.841518</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.276700</td>\n",
              "      <td>-8.875000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>3.191673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.875000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.359500</td>\n",
              "      <td>-8.875000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>3.398676</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.875000</td>\n",
              "      <td>1.688194</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.165000</td>\n",
              "      <td>-8.718750</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.912499</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.718750</td>\n",
              "      <td>1.914582</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.988500</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.471230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.831200</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.578111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>4.185100</td>\n",
              "      <td>-10.968750</td>\n",
              "      <td>1.187500</td>\n",
              "      <td>2.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.462738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.968750</td>\n",
              "      <td>1.596546</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>5.910200</td>\n",
              "      <td>-9.687500</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>14.775377</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.687500</td>\n",
              "      <td>1.982213</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.690600</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.187500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.187500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.226623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>4.381600</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.954111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.920300</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.300771</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>1.688194</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>3.154300</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>7.885849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>1.688194</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>3.475100</td>\n",
              "      <td>-11.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.687869</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.125000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.172900</td>\n",
              "      <td>-15.187500</td>\n",
              "      <td>10.375000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>2.932149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-14.187500</td>\n",
              "      <td>20.778896</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>3.033900</td>\n",
              "      <td>-12.312500</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.584838</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-11.312500</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.055100</td>\n",
              "      <td>-17.625000</td>\n",
              "      <td>14.500000</td>\n",
              "      <td>5.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.562500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>2.637782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-16.625000</td>\n",
              "      <td>21.008333</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>4.394400</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.625000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.625000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.985942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>4.229400</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.573483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>0.670820</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.722200</td>\n",
              "      <td>-8.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.812500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.812500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.305527</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.125000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.800100</td>\n",
              "      <td>-9.937500</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000267</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.937500</td>\n",
              "      <td>2.379601</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>4.310800</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.777095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>2.323790</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>4.485500</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.213644</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>2.569046</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>4.374700</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.936752</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>5.280500</td>\n",
              "      <td>-7.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.437500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.437500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>13.201171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.750000</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>4.561100</td>\n",
              "      <td>-8.562500</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.402737</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.562500</td>\n",
              "      <td>1.631717</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>4.515500</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.288684</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>4.271600</td>\n",
              "      <td>-11.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.679050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.125000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>4.385800</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.964442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>4.208100</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.520314</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>3.957000</td>\n",
              "      <td>-11.031250</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.892457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.031250</td>\n",
              "      <td>4.638494</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>3.754600</td>\n",
              "      <td>-9.125000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.386458</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.125000</td>\n",
              "      <td>3.074085</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>5.618300</td>\n",
              "      <td>-11.375000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>14.045696</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>2.334524</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>4.478100</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.195347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>4.485400</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.213555</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.797900</td>\n",
              "      <td>-13.156250</td>\n",
              "      <td>6.312500</td>\n",
              "      <td>4.687500</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.687500</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>4.494652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-12.156250</td>\n",
              "      <td>12.267463</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>4.463900</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.159776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.789300</td>\n",
              "      <td>-10.156250</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.473352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.156250</td>\n",
              "      <td>1.044330</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>3.044200</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.610490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.750000</td>\n",
              "      <td>3.464102</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.737400</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.562500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.562500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.343430</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.225800</td>\n",
              "      <td>-50.000000</td>\n",
              "      <td>79.001740</td>\n",
              "      <td>13.437500</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.437500</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>0.564435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-49.062500</td>\n",
              "      <td>131.980911</td>\n",
              "      <td>-0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.113100</td>\n",
              "      <td>-11.593750</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>5.282780</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.593750</td>\n",
              "      <td>2.010131</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>4.468900</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.172216</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>15.673500</td>\n",
              "      <td>-71.125000</td>\n",
              "      <td>122.947166</td>\n",
              "      <td>15.187500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.187500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>39.183733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-70.187500</td>\n",
              "      <td>245.687073</td>\n",
              "      <td>-0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.555500</td>\n",
              "      <td>-16.062500</td>\n",
              "      <td>13.129853</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>3.888813</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-15.062500</td>\n",
              "      <td>26.182611</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>4.433000</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.082409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.289800</td>\n",
              "      <td>-20.218750</td>\n",
              "      <td>21.937500</td>\n",
              "      <td>8.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>0.724440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.218750</td>\n",
              "      <td>43.695145</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>3.521900</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.804647</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.313400</td>\n",
              "      <td>-42.468750</td>\n",
              "      <td>66.104584</td>\n",
              "      <td>9.125000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.125000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-41.468750</td>\n",
              "      <td>132.554581</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.617900</td>\n",
              "      <td>-20.031250</td>\n",
              "      <td>13.635269</td>\n",
              "      <td>9.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.062500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1.544871</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-19.031250</td>\n",
              "      <td>30.300974</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.268700</td>\n",
              "      <td>-16.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>5.625000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.625000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>3.171701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-15.500000</td>\n",
              "      <td>25.890154</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.228800</td>\n",
              "      <td>-10.031250</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>5.572052</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.031250</td>\n",
              "      <td>1.521718</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>3.095300</td>\n",
              "      <td>-10.093750</td>\n",
              "      <td>2.029006</td>\n",
              "      <td>5.437500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.437500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.738221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.093750</td>\n",
              "      <td>3.489120</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.531500</td>\n",
              "      <td>-11.875000</td>\n",
              "      <td>1.858678</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>6.328762</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.875000</td>\n",
              "      <td>2.717229</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.387600</td>\n",
              "      <td>-11.781250</td>\n",
              "      <td>0.351175</td>\n",
              "      <td>5.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>5.968896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.781250</td>\n",
              "      <td>0.682367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.414500</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>4.812500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.812500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>6.036342</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.625000</td>\n",
              "      <td>1.024695</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>3.747100</td>\n",
              "      <td>-10.125000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.367838</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.125000</td>\n",
              "      <td>1.746425</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>3.316100</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.290171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>0.670820</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.348000</td>\n",
              "      <td>-7.656250</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.869954</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.656250</td>\n",
              "      <td>1.220912</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>4.347200</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.868047</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>4.445200</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.113029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>4.382300</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.955693</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>2.323790</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>3.659500</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.148779</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>2.569046</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>4.309500</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.773786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>2.323790</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.503900</td>\n",
              "      <td>-8.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.759722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.125000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>4.488100</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.220191</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>4.489400</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.223436</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>4.469900</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.174860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>4.489100</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.222636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>2.190890</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>4.349800</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.874598</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>4.279100</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.697719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>4.361300</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.903220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.983800</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.459516</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>4.299800</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.749524</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>3.696500</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.241236</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>4.390100</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.975214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>3.629500</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.073863</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>3.315000</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.437500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.287448</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>3.741100</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.352797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>4.200000</td>\n",
              "      <td>-8.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.500032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>3.772500</td>\n",
              "      <td>-8.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.431361</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.125000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>3.663600</td>\n",
              "      <td>-8.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.159040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>4.432000</td>\n",
              "      <td>-10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.080046</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>3.153800</td>\n",
              "      <td>-9.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.187500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.884429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.250000</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>4.418500</td>\n",
              "      <td>-11.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.046176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.125000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>4.437200</td>\n",
              "      <td>-7.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.092925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.750000</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>3.657900</td>\n",
              "      <td>-10.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.144702</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.750000</td>\n",
              "      <td>1.341641</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>3.620800</td>\n",
              "      <td>-9.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.312500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.052022</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.625000</td>\n",
              "      <td>2.012461</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>4.057100</td>\n",
              "      <td>-11.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.142630</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-10.500000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.412400</td>\n",
              "      <td>-10.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.030997</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.375000</td>\n",
              "      <td>0.670820</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"absolve\"\n",
            "Answer: 0\n",
            "Response: coniocoÈ™\n",
            "\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"frescos\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"v\" are there in the word \"absolve\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"eclipse\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"echo\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"d\" are there in the word \"void\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"aperture\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"fume\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"f\" are there in the word \"frescos\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"b\" are there in the word \"banner\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"f\" are there in the word \"capture\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"sphinx\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"k\" are there in the word \"knack\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"relish\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"zealous\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"idea\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"tavern\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"torrent\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"brawn\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"fathom\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"u\" are there in the word \"echo\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"enchant\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"lush\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"fusion\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"x\" are there in the word \"maze\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"onset\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"veto\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"torrent\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"k\" are there in the word \"resolve\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"lunar\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"sapphire\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"triumph\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"zealous\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"verge\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"onset\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"crisp\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"lantern\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"fusion\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 278]) with length 278 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 278]) with length 278 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"onset\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"wisp\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"radius\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"j\" are there in the word \"elude\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"enchant\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"orchard\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"maze\"\n",
            "Answer: 1\n",
            "Response: ç€ä»–ä¼¼ä¹Žåœ¨å€¾è¯‰æ—¥å¸¸çäº‹ï¼Œè¯·æ‚¨æä¾›ä¸€ä¸ªå…·ä½“çš„é—®é¢˜æˆ–æŒ‡ä»¤ï¼Œæˆ‘å°†å¾ˆä¹æ„ä¸ºæ‚¨è§£ç­”æˆ–æä¾›å¸®åŠ©ã€‚\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"gossipy\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"q\" are there in the word \"ivory\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"capture\"\n",
            "Answer: 1\n",
            "Response: ç¤ž Johnston\n",
            "\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"prelude\"\n",
            "Answer: 2\n",
            "Response: isty\n",
            "\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crisp\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"enchant\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"scarab\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"oath\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"prelude\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"grim\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"oath\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"aperture\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"rust\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"aperture\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"torrent\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"gossipy\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"veto\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"sphinx\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"resolve\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"absolve\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"orchard\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"banner\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"fathom\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"triumph\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"elude\"\n",
            "Answer: 2\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"quest\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"grim\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"enchant\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"fusion\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"z\" are there in the word \"aperture\"\n",
            "Answer: 0\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"glow\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"knack\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"eclipse\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"capture\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"gossipy\"\n",
            "Answer: 1\n",
            "Response: \n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "time: 11min 7s (started: 2025-12-24 13:07:59 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Now let's train for real! Let's do a longer training that will take an hour or more\n",
        "# Note: If this run is successful, you can consider doing a longer train\n",
        "# to see what happens, but that's beyond the scope of this project.\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Fix the recompilation limit error\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.reset()\n",
        "\n",
        "# Full training\n",
        "training_args = GRPOConfig(\n",
        "    **COMMON_GRPO_TRAINING_PARAMS,\n",
        "    # Configure the maximum number of steps to take about 30mins of time for\n",
        "    # a medium-sized experiment. (See how long the previous example took and\n",
        "    # scale up appropriately using your best guess.)\n",
        "    # max_steps=**********,  # ~60min\n",
        "    max_steps = 100,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=REWARD_FUNCS,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "trainer_res = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available columns: dict_keys(['loss', 'grad_norm', 'learning_rate', 'num_tokens', 'completions/mean_length', 'completions/min_length', 'completions/max_length', 'completions/clipped_ratio', 'completions/mean_terminated_length', 'completions/min_terminated_length', 'completions/max_terminated_length', 'rewards/numbering_reward_func/mean', 'rewards/numbering_reward_func/std', 'rewards/spelling_reward_func/mean', 'rewards/spelling_reward_func/std', 'rewards/counting_reward_func/mean', 'rewards/counting_reward_func/std', 'rewards/format_reward_func/mean', 'rewards/format_reward_func/std', 'rewards/correct_answer_reward_func/mean', 'rewards/correct_answer_reward_func/std', 'reward', 'reward_std', 'frac_reward_zero_std', 'completion_length', 'kl', 'epoch', 'step'])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 146 ms (started: 2025-12-24 13:20:26 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show the total (sum) of the rewards as well as the correct_answer_reward_func (means with in the batch)\n",
        "# Do you see the rewards increasing? Does the model get the correct answer\n",
        "# more frequently toward the end?\n",
        "# No changes needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want to graph other columns, check these out\n",
        "print(f\"available columns: {trainer.state.log_history[0].keys()}\")\n",
        "\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "log_df[\"reward\"].plot()\n",
        "log_df[\"rewards/correct_answer_reward_func/mean\"].plot()\n",
        "\n",
        "# Show the legend\n",
        "plt.legend([\"reward\", \"rewards/correct_answer_reward_func/mean\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View the results\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('grpo_saved_lora/tokenizer_config.json',\n",
              " 'grpo_saved_lora/special_tokens_map.json',\n",
              " 'grpo_saved_lora/chat_template.jinja',\n",
              " 'grpo_saved_lora/vocab.json',\n",
              " 'grpo_saved_lora/merges.txt',\n",
              " 'grpo_saved_lora/added_tokens.json',\n",
              " 'grpo_saved_lora/tokenizer.json')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.53 s (started: 2025-12-24 13:21:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Save the LoRA adapters\n",
        "# No changes needed in this cell\n",
        "\n",
        "# Save the LoRA model\n",
        "model.save_pretrained(\"grpo_saved_lora\")\n",
        "tokenizer.save_pretrained(\"grpo_saved_lora\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.05 ms (started: 2025-12-24 13:22:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create a function to run both the original model and the updated model\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "def compare_old_and_new_model(messages):\n",
        "    from vllm import SamplingParams\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "    old = (\n",
        "        model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "        )[0]\n",
        "        .outputs[0]\n",
        "        .text\n",
        "    )\n",
        "\n",
        "    new = (\n",
        "        model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "            lora_request=model.load_lora(\"grpo_saved_lora\"),\n",
        "        )[0]\n",
        "        .outputs[0]\n",
        "        .text\n",
        "    )\n",
        "\n",
        "    print(\"===OLD===\\n\")\n",
        "    print(old)\n",
        "\n",
        "    print(\"\\n\\n===NEW===\\n\")\n",
        "    print(new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the old and new models on the letter-counting task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROMPT ===\n",
            "<|im_start|>system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "How many of the letter \"a\" are there in the word \"idea\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "=== FINE-TUNED MODEL OUTPUT ===\n",
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "\n",
            "time: 560 ms (started: 2025-12-24 13:22:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's try spelling the first word from the dataset\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Load the first item from the dataset (index 0) and compare the old and new models\n",
        "# **********\n",
        "text = tokenizer.apply_chat_template(\n",
        "    ds[0][\"prompt\"], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== PROMPT ===\")\n",
        "print(text)\n",
        "print(\"\\n=== FINE-TUNED MODEL OUTPUT ===\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model is better at spelling and counter letters in words! Depending on your reward functions, the size of your model, and the amount of steps trained, results may vary.\n",
        "\n",
        "For about an hour of training time, your model may not be perfect (or maybe it is), but it's definitely moving in the right direction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make sure the model did not forget basic facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== QUESTION ===\n",
            "What is the capital of Spain?\n",
            "\n",
            "=== FINE-TUNED MODEL ANSWER ===\n",
            "system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "user\n",
            "What is the capital of Spain?\n",
            "assistant\n",
            "The capital of Spain is Madrid.\n",
            "time: 745 ms (started: 2025-12-24 13:22:22 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's see if the model still remembers some of the facts from its original training\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Ask both the old and new models a question the model is likely to know,\n",
        "# e.g. a well-known capital city\n",
        "# **********\n",
        "question = \"What is the capital of Spain?\"\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": question}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== QUESTION ===\")\n",
        "print(question)\n",
        "print(\"\\n=== FINE-TUNED MODEL ANSWER ===\")\n",
        "print(output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great job! Congrats on completing the project! ðŸŽ‰ðŸ¤—"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (venv2)",
      "language": "python",
      "name": "venv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
