{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project: Teaching an LLM to Reason\n",
        "\n",
        "In this project, you will teach an LLM to use step-by-step reasoning to answer the question: \"How many X's are there in the word Y?\"\n",
        "\n",
        "Counting letters in a word is a surprisingly complex task for an LLM. Just as human beings would not be able to answer such a question for longer words without breaking down the word into its individual letters and then counting them, LLMs cannot be similarly expected to be able to respond without using smaller reasoning steps.\n",
        "\n",
        "For example, to count the number of o's in the word room, one could use the following reasoning:\n",
        "\n",
        "```\n",
        "Question: How many of the letter \"o\" are there in the word \"room\"\n",
        "Answer: 2\n",
        "Response:\n",
        "\n",
        "<reasoning>\n",
        "Letter-by-letter spelling:\n",
        "1. r - 0 o's so far\n",
        "2. o - 1 o's so far\n",
        "3. o - 2 o's so far\n",
        "4. m - 2 o's so far\n",
        "\n",
        "The letter \"o\" appears 2 times in the word \"room\".\n",
        "</reasoning>\n",
        "<answer>\n",
        "2\n",
        "</answer>\n",
        "```\n",
        "\n",
        "In this project we will use the reinforcement learning method GRPO (Group Relative Policy Optimization, of DeepSeek fame) to take a large language model that has been fine-tuned for following instructions and teach it how to break a word down into its letters and then count the requested letter.\n",
        "\n",
        "We will complete the following steps:\n",
        "\n",
        "* Set up the notebook\n",
        "* Create a letter-counting dataset\n",
        "* Create the reward functions\n",
        "* Train the model\n",
        "* View the results\n",
        "\n",
        "NOTE: This notebook will have you focus on several important aspects of training a GPRO model using LoRA:\n",
        "\n",
        "1. Configuring LoRA adapters for parameter-efficient fine tuning\n",
        "2. Selecting reward functions that help the model efficiently find its way to the correct answer (also called reward shaping)\n",
        "3. Finding hyperparameters that help the model increase the rewards earned more quickly and reliably\n",
        "4. Learning how to start with smaller experiments and to work your way up to longer experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the notebook\n",
        "\n",
        "We'll install dependencies needed for the project, namely `unsloth` and `vllm`, which are useful for fine-tuning LLMs with even just 15GB of VRAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 213 Î¼s (started: 2025-12-25 21:38:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Load ipython-autotime to see how long each cell take to run\n",
        "# No changes needed in this cell\n",
        "\n",
        "!pip install -q ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Dec 25 21:38:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
            "| N/A   28C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "time: 348 ms (started: 2025-12-25 21:38:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Verify we have enough GPU memory to run this project (at least 15360MiB)\n",
        "# No changes needed in this cell\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMMENTS:\n",
        "\n",
        "#### lora_rank = 64 was chosen because it is a good middle ground choice between capacity and efficiency. Also becaue we had memory constrains.\n",
        "#### fast_interface=False.  I try multiple choices and was not able to make this run unless I have it as False.\n",
        "#### target_modules. Our task will requiere the different roles, the Attention layers and MLP layers. That is, we are giving the model the flexibility to learn what to attend to and how to process and format it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/voc/data/venv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 12-25 21:39:16 [__init__.py:241] Automatically detected platform cuda.\n",
            "ERROR 12-25 21:39:18 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.7: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.9.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 57.7 s (started: 2025-12-25 21:38:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Load the `Qwen 2.5 3B Instruct`, and set parameters for the project\n",
        "# The first time unsloth is imported, it will do its magic and patch the modules\n",
        "# it works with. This may 2-5 minutes.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "import unsloth\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 256  # Increase if you get errors about the sequence length\n",
        "\n",
        "# Set the LoRA rank to an appropriate value\n",
        "# Read about setting LoRA rank:\n",
        "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "# lora_rank = **********  # Explain your choice\n",
        "lora_rank = 64\n",
        "\n",
        "# Load the Instruct model in 4-bit mode\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,  # We'll use quantization!\n",
        "    fast_inference=False,  # Changed from True to False\n",
        "    max_lora_rank=lora_rank,\n",
        "    gpu_memory_utilization=0.75,  # You can reduce this if you get an memory error\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        # Read about choosing adapters for LoRA:\n",
        "        # https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "        # Choose the target modules/adapters for your LoRA model\n",
        "        # ********** # Explain your choice\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth enables longer contexts\n",
        "    # See: https://github.com/unslothai/unsloth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try Prompt Engineering to Count Letters\n",
        "\n",
        "Let's work on the system prompt a little to see if we can get the model to count the number of the letter `g` in `engage`.\n",
        "\n",
        "\n",
        "Here you must:\n",
        "* Write clear instructions\n",
        "* Break the problem down into steps (Chain-of-Thought prompting)\n",
        "* Provide at least one example for the model to follow (Few-shot prompting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TEXT FOR COMPLETION ===\n",
            "<|im_start|>system\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "How many of the letter \"g\" are there in the word \"engage\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "=== GENERATED OUTPUT ===\n",
            "system\n",
            "\n",
            "user\n",
            "How many of the letter \"g\" are there in the word \"engage\"\n",
            "assistant\n",
            "In the word \"engage\", there is only one letter \"g\".\n",
            "time: 3.3 s (started: 2025-12-25 21:39:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# First, let's see what happens when we have a blank system prompt\n",
        "# No changes needed in this cell\n",
        "SYSTEM_PROMPT = \"\"\"\"\"\"\n",
        "USER_PROMPT = 'How many of the letter \"g\" are there in the word \"engage\"'\n",
        "\n",
        "# Convert the chat messages to a single string so the model can complete it\n",
        "text_for_completion = tokenizer.apply_chat_template(\n",
        "    conversation=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        },\n",
        "    ],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text_for_completion], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate the text completion using standard generate (not fast_generate)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the text input for the model and the model's output\n",
        "print(\"=== TEXT FOR COMPLETION ===\")\n",
        "print(text_for_completion)\n",
        "print(\"=== GENERATED OUTPUT ===\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without any prompting the model will generate an output such as this:\n",
        "\n",
        "```\n",
        "=== GENERATED OUTPUT ===\n",
        "There is one letter \"g\" in the word \"engage\".\n",
        "```\n",
        "\n",
        "Now let's work on the system prompt to help the model break this problem down into steps, which might help it get the right answer (2 `g`'s in `engage`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TEXT FOR COMPLETION ===\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that counts letters in words step-by-step.\n",
            "\n",
            "To count letters, you should:\n",
            "1. Break the word down letter by letter\n",
            "2. Count how many times the target letter appears\n",
            "3. Provide your reasoning in <reasoning>...</reasoning> tags\n",
            "4. Provide your final answer in <answer>...</answer> tags\n",
            "\n",
            "Example:\n",
            "Question: How many of the letter \"o\" are there in the word \"room\"\n",
            "Answer: 2\n",
            "\n",
            "Response:\n",
            "<reasoning>\n",
            "Letter-by-letter spelling:\n",
            "1. r - 0 o's so far\n",
            "2. o - 1 o's so far\n",
            "3. o - 2 o's so far\n",
            "4. m - 2 o's so far\n",
            "\n",
            "The letter \"o\" appears 2 times in the word \"room\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "How many of the letter \"g\" are there in the word \"engage\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "=== GENERATED OUTPUT ===\n",
            "system\n",
            "You are a helpful assistant that counts letters in words step-by-step.\n",
            "\n",
            "To count letters, you should:\n",
            "1. Break the word down letter by letter\n",
            "2. Count how many times the target letter appears\n",
            "3. Provide your reasoning in <reasoning>...</reasoning> tags\n",
            "4. Provide your final answer in <answer>...</answer> tags\n",
            "\n",
            "Example:\n",
            "Question: How many of the letter \"o\" are there in the word \"room\"\n",
            "Answer: 2\n",
            "\n",
            "Response:\n",
            "<reasoning>\n",
            "Letter-by-letter spelling:\n",
            "1. r - 0 o's so far\n",
            "2. o - 1 o's so far\n",
            "3. o - 2 o's so far\n",
            "4. m - 2 o's so far\n",
            "\n",
            "The letter \"o\" appears 2 times in the word \"room\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"g\" are there in the word \"engage\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Letter-by-letter spelling:\n",
            "1. e - 0 g's so far\n",
            "2. a - 1 g' (third letter) so far\n",
            "3. n - 1 g' (fourth letter) so far\n",
            "4. g - 2 g's so far (fifth letter)\n",
            "5. e - 2 g's so far\n",
            "6. n - 3 g's so far\n",
            "7. g - 4 g's so far\n",
            "\n",
            "The letter \"g\" appears 4 times in the word \"engage\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "time: 8.29 s (started: 2025-12-25 21:39:51 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's work on a new system prompt that will help the model break this problem\n",
        "# down into steps, for example, using \"letter-by-letter\" spelling.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "# Use a CoT prompt with at least one example\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that counts letters in words step-by-step.\n",
        "\n",
        "To count letters, you should:\n",
        "1. Break the word down letter by letter\n",
        "2. Count how many times the target letter appears\n",
        "3. Provide your reasoning in <reasoning>...</reasoning> tags\n",
        "4. Provide your final answer in <answer>...</answer> tags\n",
        "\n",
        "Example:\n",
        "Question: How many of the letter \"o\" are there in the word \"room\"\n",
        "Answer: 2\n",
        "\n",
        "Response:\n",
        "<reasoning>\n",
        "Letter-by-letter spelling:\n",
        "1. r - 0 o's so far\n",
        "2. o - 1 o's so far\n",
        "3. o - 2 o's so far\n",
        "4. m - 2 o's so far\n",
        "\n",
        "The letter \"o\" appears 2 times in the word \"room\".\n",
        "</reasoning>\n",
        "<answer>\n",
        "2\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT = 'How many of the letter \"g\" are there in the word \"engage\"'\n",
        "\n",
        "# Convert the chat messages to a single string so the model can complete it\n",
        "text_for_completion = tokenizer.apply_chat_template(\n",
        "    conversation=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        },\n",
        "    ],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text_for_completion], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate the text completion using standard generate (not fast_generate)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Print the text input for the model and the model's output\n",
        "print(\"=== TEXT FOR COMPLETION ===\")\n",
        "print(text_for_completion)\n",
        "print(\"=== GENERATED OUTPUT ===\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did your new prompt get the right answer? Did the model follow all of your instructions?\n",
        "\n",
        "Maybe yes, maybe no. Either way, we'll want the model to reliably complete this challenge. So let's use GRPO to help it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a letter-counting dataset\n",
        "\n",
        "To train a model, we'll first need to create a dataset. We'll use the HuggingFace `datasets` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['idea',\n",
              " 'glow',\n",
              " 'rust',\n",
              " 'maze',\n",
              " 'echo',\n",
              " 'wisp',\n",
              " 'veto',\n",
              " 'lush',\n",
              " 'gaze',\n",
              " 'knit']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 5.69 ms (started: 2025-12-25 21:40:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create a list of words of different lengths\n",
        "# No changes are needed in this cell.\n",
        "\n",
        "ALL_WORDS = [\n",
        "    \"idea\",\n",
        "    \"glow\",\n",
        "    \"rust\",\n",
        "    \"maze\",\n",
        "    \"echo\",\n",
        "    \"wisp\",\n",
        "    \"veto\",\n",
        "    \"lush\",\n",
        "    \"gaze\",\n",
        "    \"knit\",\n",
        "    \"fume\",\n",
        "    \"plow\",\n",
        "    \"void\",\n",
        "    \"oath\",\n",
        "    \"grim\",\n",
        "    \"crisp\",\n",
        "    \"lunar\",\n",
        "    \"fable\",\n",
        "    \"quest\",\n",
        "    \"verge\",\n",
        "    \"brawn\",\n",
        "    \"elude\",\n",
        "    \"aisle\",\n",
        "    \"ember\",\n",
        "    \"crave\",\n",
        "    \"ivory\",\n",
        "    \"mirth\",\n",
        "    \"knack\",\n",
        "    \"wryly\",\n",
        "    \"onset\",\n",
        "    \"mosaic\",\n",
        "    \"velvet\",\n",
        "    \"sphinx\",\n",
        "    \"radius\",\n",
        "    \"summit\",\n",
        "    \"banner\",\n",
        "    \"cipher\",\n",
        "    \"glisten\",\n",
        "    \"mantle\",\n",
        "    \"scarab\",\n",
        "    \"expose\",\n",
        "    \"fathom\",\n",
        "    \"tavern\",\n",
        "    \"fusion\",\n",
        "    \"relish\",\n",
        "    \"lantern\",\n",
        "    \"enchant\",\n",
        "    \"torrent\",\n",
        "    \"capture\",\n",
        "    \"orchard\",\n",
        "    \"eclipse\",\n",
        "    \"frescos\",\n",
        "    \"triumph\",\n",
        "    \"absolve\",\n",
        "    \"gossipy\",\n",
        "    \"prelude\",\n",
        "    \"whistle\",\n",
        "    \"resolve\",\n",
        "    \"zealous\",\n",
        "    \"mirage\",\n",
        "    \"aperture\",\n",
        "    \"sapphire\",\n",
        "]\n",
        "\n",
        "print(len(ALL_WORDS))\n",
        "\n",
        "ALL_WORDS[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'words': 'idea', 'letters': 'a', 'counts': 1}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 195 ms (started: 2025-12-25 21:40:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset as a Hugging Face Dataset using Dataset.from_generator\n",
        "# No changes needed in this cell\n",
        "\n",
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "\n",
        "# Go through the letters from the words (as well as letters not in the words),\n",
        "# and create a labelled dataset with all the different combinations.\n",
        "# For example for the word gaze:\n",
        "# 1. How many i's are in idea? <-- count should be 1\n",
        "# 2. How many d's are in idea? <-- count should be 1\n",
        "# 3. How many e's are in idea? <-- count should be 1\n",
        "# 4. How many a's are in idea? <-- count should be 1\n",
        "# 5. How many b's are in idea? <-- a letter not in word (count should be zero)\n",
        "def generate_records():\n",
        "    for word in ALL_WORDS:\n",
        "        for letter in sorted(set(word)):\n",
        "            yield {\"words\": word, \"letters\": letter, \"counts\": word.count(letter)}\n",
        "\n",
        "        # pick random letters not in the word\n",
        "        num_letters_not_in_word_left = int(len(word) // 7 + 1)\n",
        "\n",
        "        random.seed(hash(word))\n",
        "\n",
        "        all_letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "        random.shuffle(all_letters)\n",
        "        for letter in all_letters:\n",
        "            if letter not in word:\n",
        "                yield {\"words\": word, \"letters\": letter, \"counts\": 0}\n",
        "                num_letters_not_in_word_left -= 1\n",
        "            if num_letters_not_in_word_left == 0:\n",
        "                break\n",
        "\n",
        "\n",
        "ds = Dataset.from_generator(generate_records)\n",
        "\n",
        "# Show the first item\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'words': 'idea',\n",
              " 'letters': 'a',\n",
              " 'counts': 1,\n",
              " 'prompt': [{'content': \"\\nRespond in the following format:\\n<reasoning>\\nCounting the number of [letter_to_count]'s in the word [word]\\n1. [first letter] - [count of requested letter so far] so far\\n2. [second letter] - [count of requested letter so far] so far\\n...\\n</reasoning>\\n<answer>\\n[number]\\n</answer>\\n\",\n",
              "   'role': 'system'},\n",
              "  {'content': 'How many of the letter \"a\" are there in the word \"idea\"',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 14.1 ms (started: 2025-12-25 21:40:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Add the entire prompt (system + user) and the answer to the dataset\n",
        "# We'll use a prompt that spells out the word letter-by-letter\n",
        "# No changes needed in this cell\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Simple CoT prompt (zero-shot)\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "Counting the number of [letter_to_count]'s in the word [word]\n",
        "1. [first letter] - [count of requested letter so far] so far\n",
        "2. [second letter] - [count of requested letter so far] so far\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "[number]\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "ds = ds.map(\n",
        "    lambda x: {  # type: ignore\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": 'How many of the letter \"{}\" are there in the word \"{}\"'.format(\n",
        "                    x[\"letters\"], x[\"words\"]\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Counting the number of a's in the word idea\n",
            "1. i - 0 so far\n",
            "2. d - 1 so far\n",
            "3. e - 2 so far\n",
            "4. a - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "time: 4.38 s (started: 2025-12-25 21:40:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's see how well the model runs out-of-the-box\n",
        "# No changes needed in this cell\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    ds[0][\"prompt\"], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using standard generate method\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Reward Functions\n",
        "\n",
        "One goal of creating reward functions is to guide the model toward behaviors that help it reach its goal (counting the occurrences of a letter within a word) more easily. Since there is more than one way to carry out any step-by-step task (e.g. whether or not you use bullet points to separate your steps), there's a bit of judgement involved in choosing what behaviors to reward, i.e. how do we provide partial credit or \"shape\" our rewards?\n",
        "\n",
        "In this case we will encourage the model to (whether or not this structure is best):\n",
        "* use numbers for bullet points when spelling out the word\n",
        "* to spell the word correctly\n",
        "* to count the requested letter correctly\n",
        "* to use the requested reasoning format\n",
        "* to get the final answer correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numbering reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.125, 0.625]\n",
            "time: 3.57 ms (started: 2025-12-25 21:40:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's work on a function that the numbering in the bullet points is correct\n",
        "# When using GRPO, we lean on reward functions that are relatively easy to\n",
        "# compute, thus removing the need to have a second large model just for\n",
        "# evaluation.\n",
        "# In this case, we'll use regular expressions quite a bit.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_letter_numbering(response):\n",
        "    \"\"\"Extract the numbers at the beginning of the line\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    4. a - 2 so far\n",
        "    5. l - 2 so far\n",
        "    returns [1, 2, 3, 4, 5]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # We use a regular expression to find lines of the form:\n",
        "    # '\\n[number]. [letter]'\n",
        "    pattern = r\"\\n(\\d+). [a-z]\"\n",
        "\n",
        "    # Use `re` to find all matches of the pattern in the response\n",
        "    # matches = **********\n",
        "    matches = re.findall(pattern, response)\n",
        "    if matches:\n",
        "        return [int(m) for m in matches]\n",
        "    return []\n",
        "\n",
        "\n",
        "assert extract_letter_numbering(\n",
        "    \"\"\"\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. a - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == [1, 2, 3, 4, 5]\n",
        "\n",
        "\n",
        "def numbering_reward_func(completions, words, **kwargs) -> list[float]:\n",
        "    \"\"\"Provides a reward for getting the numbering at the beginning of the line correct\n",
        "\n",
        "    1. g - 1 so far <-- Good in-order numbering\n",
        "    2. o - 1 so far <-- Good in-order numbering\n",
        "    3. a - 2 so far <-- Good in-order numbering\n",
        "    3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "    1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "    1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "\n",
        "    \"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "    for response, word in zip(responses, words):\n",
        "        reward = 0\n",
        "\n",
        "        for ix, spell_number in enumerate(extract_letter_numbering(response)):\n",
        "            line_number = ix + 1\n",
        "\n",
        "            # Get points for in-order numbering\n",
        "            if spell_number == line_number:\n",
        "                # TODO: Provide a reward for in-order numbering\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward += ***********\n",
        "                reward += 1\n",
        "            # Otherwise lose points\n",
        "            else:\n",
        "                # TODO: Provide a reward for out-of-order numbering\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward -= ***********\n",
        "                reward -= 0.5\n",
        "\n",
        "            # Lose extra points for continuing beyond the length of the word\n",
        "            if line_number > len(word):  # We use the index of the line\n",
        "                # TODO: Provide a reward for continuing beyond the length of the word\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward -= ***********\n",
        "                reward -= 1\n",
        "\n",
        "        res.append(reward / len(word))\n",
        "    return res\n",
        "\n",
        "\n",
        "res = numbering_reward_func(\n",
        "    completions=[\n",
        "        [\n",
        "            {  # Worse response\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far <-- Good in-order numbering\n",
        "2. o - 1 so far <-- Good in-order numbering\n",
        "3. a - 2 so far <-- Good in-order numbering\n",
        "3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {  # Better response\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far <-- Good in-order numbering\n",
        "2. o - 1 so far <-- Good in-order numbering\n",
        "3. a - 2 so far <-- Good in-order numbering\n",
        "3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            },\n",
        "        ],\n",
        "    ],\n",
        "    words=[\"goal\", \"goal\"],\n",
        ")\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spelling reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.15000000000000002, 10.0]\n",
            "time: 38.9 ms (started: 2025-12-25 21:40:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward correct spelling of the word\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_spelling(response):\n",
        "    \"\"\"Extract the spelling from the response\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    3. l - 2 so far\n",
        "    5. l - 2 so far\n",
        "    Returns \"goall\"\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"\\n\\d+\\. ([a-z])\"\n",
        "    matches = re.findall(pattern, response, flags=re.IGNORECASE)\n",
        "    if matches:\n",
        "        return \"\".join([m for m in matches])\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "extract_spelling(\n",
        "    \"\"\"Here is a letter by letter spelling:\n",
        "\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "3. l - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == \"goall\"\n",
        "\n",
        "\n",
        "def spelling_reward_func(completions, words, **kwargs) -> list[float]:\n",
        "    \"\"\"A spelling reward function.\"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for word, response in zip(words, responses):\n",
        "        reward = 0.0\n",
        "\n",
        "        # Provide a reward for exactly correct spelling\n",
        "        # reward += **********\n",
        "        extracted_spelling = extract_spelling(response)\n",
        "\n",
        "        if extracted_spelling.lower() == word.lower():\n",
        "            reward += 10  # Large bonus for perfect spelling\n",
        "\n",
        "        # Provide a reward for each letter of difference in length\n",
        "        # reward -= **********\n",
        "        length_diff = abs(len(extracted_spelling) - len(word))\n",
        "        reward -= length_diff * 0.05  # Penalty per character difference\n",
        "\n",
        "\n",
        "        # Provide a reward for each letter that is not in the target word\n",
        "        # reward -= **********\n",
        "        word_counter = Counter(word.lower())\n",
        "        spelling_counter = Counter(extracted_spelling.lower())\n",
        "        extra_letters = sum((spelling_counter - word_counter).values())\n",
        "        reward -= extra_letters * 0.1  # Penalty for extra letters\n",
        "\n",
        "        # Provide a reward for each letter that is in the target word but not in the response\n",
        "        # reward -= **********\n",
        "        missing_letters = sum((word_counter - spelling_counter).values())\n",
        "        reward -= missing_letters * 0.1  # Penalty for missing letters\n",
        "\n",
        "        res.append(reward)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = spelling_reward_func(\n",
        "    completions=[\n",
        "        [  # Worse response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. l - 2 so far\n",
        "5. l - 2 so far\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        [  # Better Response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. l - 2 so far\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    words=[\"goal\", \"goal\"],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.1, 0.2]\n",
            "time: 61.8 ms (started: 2025-12-25 21:40:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's reward the model for properly counting the occurrences of a letter in a word\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "# No changes needed in this cell, but feel free to experiment with variations on the prompt\n",
        "\n",
        "\n",
        "def get_resp_letters_and_counts(response):\n",
        "    \"\"\"Extract the letters and counts from the response\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    4. a - 2 so far\n",
        "    5. l - 2 so far\n",
        "    returns [('g', 1), ('o', 1), ('a', 2), ('a', 2), ('l', 2)]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"\\n(\\d+)\\. ([a-z])\\D*(\\d+)\"\n",
        "\n",
        "    # Find strings matching e.g. \"2. a - 2 so far\"\n",
        "    matches = re.findall(pattern, response, flags=re.IGNORECASE)\n",
        "\n",
        "    if not matches:\n",
        "        return []\n",
        "\n",
        "    return [\n",
        "        (matched_letter, matched_count_so_far)\n",
        "        for _, matched_letter, matched_count_so_far in matches\n",
        "    ]\n",
        "\n",
        "\n",
        "assert get_resp_letters_and_counts(\n",
        "    \"\"\"\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. a - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == [(\"g\", \"1\"), (\"o\", \"1\"), (\"a\", \"2\"), (\"a\", \"2\"), (\"l\", \"2\")]\n",
        "\n",
        "\n",
        "def counting_reward_func(completions, letters, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    # Iterate over each of the letter-response pairs\n",
        "    for letter, response in zip(letters, responses):\n",
        "        reward = 0\n",
        "\n",
        "        letters_and_counts = get_resp_letters_and_counts(response)\n",
        "\n",
        "        # If there are no matches, provide a negative reward\n",
        "        if not letters_and_counts:\n",
        "            res.append(-0.5)\n",
        "            continue\n",
        "\n",
        "        # Start counting the matching letters\n",
        "        actual_count = 0\n",
        "        for resp_letter, resp_count in letters_and_counts:\n",
        "            # If there's a match, count the letter\n",
        "            if letter == resp_letter:\n",
        "                actual_count += 1\n",
        "\n",
        "            # If the count is accurate, add a reward, else subtract a reward\n",
        "            # if ... **********\n",
        "            # else ... **********\n",
        "                if int(resp_count) == actual_count:\n",
        "                    reward += 1\n",
        "                else:\n",
        "                    reward -= 0.5\n",
        "\n",
        "        # Return the reward normalized by the length of the matches\n",
        "        # res.append(**********)\n",
        "        res.append(reward / len(letters_and_counts))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "res = counting_reward_func(\n",
        "    completions=[\n",
        "        [  # Worse response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\\nHere is a letter by letter spelling:\n",
        "\n",
        "1. g - 0 so far\n",
        "2. o - 0 so far\n",
        "3. a - 1 so far\n",
        "4. a - 2 so far\n",
        "5. l - 0 so far\n",
        "\n",
        "\\n</reasoning>\\n<answer>\\nThis is my answer.\\n</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        [  # Better response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\\nHere is a letter by letter spelling:\n",
        "\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 1 so far\n",
        "4. a - 1 so far\n",
        "5. l - 1 so far\n",
        "\n",
        "\\n</reasoning>\\n<answer>\\nThis is my answer.\\n</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    letters=[\"g\", \"g\"],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Formatting reward functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0, 25.0]\n",
            "time: 59.4 ms (started: 2025-12-25 21:40:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward the model for providing the response in a specific format\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"Extracts the string between <answer> and </answer> tags.\"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"<answer>(.*?)</answer>\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    \n",
        "    return \"\"\n",
        "\n",
        "assert (\n",
        "    extract_xml_answer(\"\"\"\n",
        "<reasoning>\n",
        "This is my reasoning.\n",
        "</reasoning>\n",
        "<answer>SUPERCALIFRAGILISTICEXPIALIDOCIOUS</answer>\n",
        "\"\"\")\n",
        "    == \"SUPERCALIFRAGILISTICEXPIALIDOCIOUS\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"\\s*<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for completion in completions:\n",
        "        reward = 0.0\n",
        "\n",
        "        # Extract the response content\n",
        "        response = completion[0][\"content\"]\n",
        "\n",
        "        # Check if the response matches the pattern\n",
        "        match = re.match(pattern, response, flags=re.MULTILINE | re.DOTALL)\n",
        "\n",
        "        # If it matches, return 0.5, otherwise return 0.0\n",
        "        # if ... **********\n",
        "        if match:\n",
        "            reward += 20.0\n",
        "        # Extract the answer from the response\n",
        "        # extracted_answer = **********\n",
        "            extracted_answer = extract_xml_answer(response)\n",
        "        # If the answer is an integer, add 0.5 to the reward\n",
        "        # if ... **********\n",
        "            try:\n",
        "                int(extracted_answer)\n",
        "                reward += 5.0\n",
        "            except ValueError:\n",
        "                reward -= 0.0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "\n",
        "        res.append(reward)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = format_reward_func(\n",
        "    completions=[\n",
        "        [{\"content\": \"This is my answer\"}],\n",
        "        [\n",
        "            {\n",
        "                \"content\": \"<reasoning>\\nThis is my reasoning.\\n</reasoning>\\n<answer>\\n3\\n</answer>\"\n",
        "            }\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task correctness reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many...\n",
            "Answer: 0\n",
            "Response: <reasoning>.../reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "[-0.5, 2.0]\n",
            "time: 57.1 ms (started: 2025-12-25 21:40:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward the model for providing the correct answer\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def correct_answer_reward_func(prompts, completions, counts, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward the final answer if it is correct.\"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    # Print a nice summary of the first prompt, answer, and response to see while training\n",
        "    print(f\"\"\"\n",
        "{\"-\" * 20}\n",
        "Question: {prompts[0][-1][\"content\"]}\n",
        "Answer: {counts[0]}\n",
        "Response: {responses[0]}\n",
        "Extracted: {extracted_responses[0]}\n",
        "Correct: {str(extracted_responses[0]) == str(counts[0])}!\n",
        "    \"\"\")\n",
        "\n",
        "    res = [\n",
        "        # Provide reward for exactly correct answer\n",
        "        # **********  # Complete the list comprehension\n",
        "        2.0 if str(r) == str(a) else -0.5\n",
        "        for r, a in zip(extracted_responses, counts)\n",
        "    ]\n",
        "    return res\n",
        "\n",
        "\n",
        "res = correct_answer_reward_func(\n",
        "    prompts=[\n",
        "        [{\"content\": \"\"\"How many...\"\"\"}],\n",
        "        [{\"content\": \"\"\"How many...\"\"\"}],\n",
        "    ],\n",
        "    completions=[\n",
        "        [{\"content\": \"\"\"<reasoning>.../reasoning>\\n<answer>\\n3\\n</answer>\"\"\"}],\n",
        "        [{\"content\": \"\"\"<reasoning>.../reasoning>\\n<answer>\\n3\\n</answer>\"\"\"}],\n",
        "    ],\n",
        "    letters=[\"g\", \"g\"],\n",
        "    counts=[0, 3],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List the reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 56.8 ms (started: 2025-12-25 21:40:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# List out the reward functions we will use\n",
        "# No changes needed in this cell\n",
        "\n",
        "REWARD_FUNCS = [\n",
        "    numbering_reward_func,\n",
        "    spelling_reward_func,\n",
        "    counting_reward_func,\n",
        "    format_reward_func,\n",
        "    correct_answer_reward_func,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "Now set up GRPO Trainer and configurations!\n",
        "\n",
        "As you run the trainer, the goal is to see the various `reward` columns increase.\n",
        "\n",
        "After 50 steps or more, you may notice some of the reward standard deviations begin to decrease, meaning that the different predictions are starting to converge on solutions that give similar rewards. If your model has learned the task, then you'll see the `correct_answer_reward_function` increase to its highest value (check the function to see what that is).\n",
        "\n",
        "Here is an example, which successfully converged on a higher reward. Note, the values you see here will probably be different from yours, especially if your reward amounts are different.\n",
        "\n",
        "| Step | Training Loss | reward   | reward_std | ... | kl      | rewards / correct_answer_reward_function / mean | rewards / correct_answer_reward_function / std |\n",
        "|------|---------------|----------|------------|-----|---------|------------------------------------------|-----------------------------------------|\n",
        "| 1    | 0.000000      | 7.961805 | 2.368493   | ... | 0.020369| 0.875000                                 | 1.024695                                |\n",
        "| 2    | 0.000000      | 7.937500 | 1.352467   | ... | 0.016483| 0.875000                                 | 1.024695                                |\n",
        "| 3    | 0.000000      | 1.894792 | 6.462189   | ... | 0.013677| 0.375000                                 | 0.806226                                |\n",
        "| ...  | ...           | ...      | ...        | ... | ...     | ...                                      | ...                                     |\n",
        "| 398  | 0.000100      | 13.000000| 0.000000   | ... | 0.088529| 2.000000                                 | 0.000000                                |\n",
        "| 399  | 0.000100      | 13.000000| 0.000000   | ... | 0.088617| 2.000000                                 | 0.000000                                |\n",
        "| 400  | 0.000100      | 13.000000| 0.000000   | ... | 0.096202| 2.000000                                 | 0.000000                                |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 60.5 ms (started: 2025-12-25 21:40:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Fill in the GRPO Parameters we'll use throughout this project\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "# Read about the GRPO params here https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "COMMON_GRPO_TRAINING_PARAMS = dict(\n",
        "    # Set appropriate values for `learning_rate` and `beta`\n",
        "    # See: https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "    # See: https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "    learning_rate=5e-5,  # Conservative learning rate for stable training\n",
        "    beta=0.1,  # KL divergence penalty weight (typical range: 0.01-0.5)\n",
        "    # Set the batch size appropriately for your hardware. For GRPO there are a number of parameters to set.\n",
        "    # If you are not sure about your GPU, assume you have a T4. See the memory specs here:\n",
        "    # https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/T4%20Product%20Brief.pdf\n",
        "    per_device_train_batch_size=4,  # per_device_train_batch_size / num_generations determines the number of simultaneous prompts to consider.\n",
        "    # Note: Set per_device_train_batch_size to at most 16 on the Vocareum T4 for best stability\n",
        "    num_generations=4,  # Determines the number of completions/generations to compute for each single prompt\n",
        "    gradient_accumulation_steps=4,  # This parameter allow us to consider multiple steps in a single optimization step\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,  # Set to 1 for a full training run\n",
        "    save_steps=250,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"none\",  # Setting this value lets us use Weights and Biases\n",
        "    output_dir=\"outputs\",\n",
        "    use_vllm=False,  # vll speeds up inference! See https://github.com/vllm-project/vllm\n",
        "    torch_compile=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick train\n",
        "\n",
        "Let's train the model for just 5 steps (`max_steps=5`). As it runs we can double check we've set up our prompts correctly before running for a longer amount of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 1 | Total steps = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n",
            "Unsloth: Input IDs of shape torch.Size([16, 262]) with length 262 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 262]) with length 262 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"glisten\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word glisten\n",
            "1. g - 1 so far\n",
            "2. l - 1 so far\n",
            "3. i - 1 so far\n",
            "4. s - 1 so far\n",
            "5. t - 1 so far\n",
            "6. e - 1 so far\n",
            "7. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 01:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / numbering_reward_func / mean</th>\n",
              "      <th>rewards / numbering_reward_func / std</th>\n",
              "      <th>rewards / spelling_reward_func / mean</th>\n",
              "      <th>rewards / spelling_reward_func / std</th>\n",
              "      <th>rewards / counting_reward_func / mean</th>\n",
              "      <th>rewards / counting_reward_func / std</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / correct_answer_reward_func / mean</th>\n",
              "      <th>rewards / correct_answer_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>29.989027</td>\n",
              "      <td>4.267745</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.903125</td>\n",
              "      <td>0.264870</td>\n",
              "      <td>2.328125</td>\n",
              "      <td>4.576715</td>\n",
              "      <td>0.226525</td>\n",
              "      <td>0.316368</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.531250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>30.390800</td>\n",
              "      <td>3.078007</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.085391</td>\n",
              "      <td>3.531250</td>\n",
              "      <td>5.178896</td>\n",
              "      <td>0.140799</td>\n",
              "      <td>0.138208</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30.309671</td>\n",
              "      <td>3.432523</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.921131</td>\n",
              "      <td>0.215555</td>\n",
              "      <td>3.571875</td>\n",
              "      <td>5.144770</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.253929</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>30.893402</td>\n",
              "      <td>2.852598</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.947917</td>\n",
              "      <td>0.079786</td>\n",
              "      <td>4.850000</td>\n",
              "      <td>5.319492</td>\n",
              "      <td>-0.029514</td>\n",
              "      <td>0.113796</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>69977792.000000</td>\n",
              "      <td>31.164890</td>\n",
              "      <td>1.795184</td>\n",
              "      <td>91.625000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.625000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>174944482.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.178125</td>\n",
              "      <td>5.305122</td>\n",
              "      <td>0.080516</td>\n",
              "      <td>0.163559</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word sapphire\n",
            "1. s - 0 so far\n",
            "2. a - 1 so far\n",
            "3. p - 1 so far\n",
            "4. a - 2 so far\n",
            "5. r - 2 so far\n",
            "6. p - 3 so far\n",
            "7. h - 3 so far\n",
            "8. a - 4 so far\n",
            "9. y - 4 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "Extracted: 4\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"absolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - 0 so far\n",
            "3. s - 1 so far\n",
            "4. p - 0 so far\n",
            "5. o - 1 so far\n",
            "6. l - 1 so far\n",
            "7. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word \"mirage\"\n",
            "1. m - 0 so far\n",
            "2. i - 0 so far\n",
            "3. r - 0 so far\n",
            "4. a - 0 so far\n",
            "5. g - 1 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word crave\n",
            "1. c - 0 so far\n",
            "2. r - 1 so far\n",
            "3. a - 1 so far\n",
            "4. v - 1 so far\n",
            "5. e - 1 so far\n",
            "The count of letter \"r\" in the word \"crave\" is 1.\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "time: 2min 34s (started: 2025-12-25 21:40:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Train for just a few steps for a few minutes\n",
        "# This will allow us to observe the results and make any changes to our reward functions\n",
        "# before starting a longer run. Note, you won't see much change in the average.\n",
        "# reward values\n",
        "# No changes are needed here\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# Short train to check on reward functions\n",
        "training_args = GRPOConfig(\n",
        "    **COMMON_GRPO_TRAINING_PARAMS,\n",
        "    # We'll just run for a modest 5 steps to make sure everything works and to\n",
        "    # estimate the amount of time it will take to run the full training.\n",
        "    max_steps=5,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=REWARD_FUNCS,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "trainer_res = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available columns: dict_keys(['loss', 'grad_norm', 'learning_rate', 'num_tokens', 'completions/mean_length', 'completions/min_length', 'completions/max_length', 'completions/clipped_ratio', 'completions/mean_terminated_length', 'completions/min_terminated_length', 'completions/max_terminated_length', 'rewards/numbering_reward_func/mean', 'rewards/numbering_reward_func/std', 'rewards/spelling_reward_func/mean', 'rewards/spelling_reward_func/std', 'rewards/counting_reward_func/mean', 'rewards/counting_reward_func/std', 'rewards/format_reward_func/mean', 'rewards/format_reward_func/std', 'rewards/correct_answer_reward_func/mean', 'rewards/correct_answer_reward_func/std', 'reward', 'reward_std', 'frac_reward_zero_std', 'completion_length', 'kl', 'epoch', 'step'])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.12 s (started: 2025-12-25 21:42:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show the total (sum) of the rewards as well as the correct_answer_reward_func (means with in the batch)\n",
        "# No changes needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want to graph other columns, check these out\n",
        "print(f\"available columns: {trainer.state.log_history[0].keys()}\")\n",
        "\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "log_df[\"reward\"].plot()\n",
        "log_df[\"rewards/correct_answer_reward_func/mean\"].plot()\n",
        "\n",
        "# Show the legend\n",
        "plt.legend([\"reward\", \"rewards/correct_answer_reward_func/mean\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slower train (1+ hour)\n",
        "\n",
        "If everything looks good, let's go for a longer training session!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n",
            "Unsloth: Input IDs of shape torch.Size([16, 306]) with length 306 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 306]) with length 306 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"glisten\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word glisten\n",
            "1. g - 1 so far\n",
            "2. l - 0 so far\n",
            "3. i - 0 so far\n",
            "4. s - 0 so far\n",
            "5. t - 0 so far\n",
            "6. e - 0 so far\n",
            "7. n - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 58:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / numbering_reward_func / mean</th>\n",
              "      <th>rewards / numbering_reward_func / std</th>\n",
              "      <th>rewards / spelling_reward_func / mean</th>\n",
              "      <th>rewards / spelling_reward_func / std</th>\n",
              "      <th>rewards / counting_reward_func / mean</th>\n",
              "      <th>rewards / counting_reward_func / std</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / correct_answer_reward_func / mean</th>\n",
              "      <th>rewards / correct_answer_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>44297196.000000</td>\n",
              "      <td>27.496653</td>\n",
              "      <td>4.973111</td>\n",
              "      <td>95.187500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>88.200005</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>110742988.000000</td>\n",
              "      <td>0.929688</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.734375</td>\n",
              "      <td>4.101452</td>\n",
              "      <td>0.176339</td>\n",
              "      <td>0.117361</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>97581280.000000</td>\n",
              "      <td>29.442711</td>\n",
              "      <td>1.018863</td>\n",
              "      <td>86.375000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86.375000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>243953196.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.243750</td>\n",
              "      <td>4.631230</td>\n",
              "      <td>0.136458</td>\n",
              "      <td>0.081699</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>179175328.000000</td>\n",
              "      <td>30.569382</td>\n",
              "      <td>1.404429</td>\n",
              "      <td>90.875000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90.875000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>447938286.000000</td>\n",
              "      <td>0.934524</td>\n",
              "      <td>0.211624</td>\n",
              "      <td>3.531250</td>\n",
              "      <td>5.176964</td>\n",
              "      <td>0.197359</td>\n",
              "      <td>0.232874</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>63136968.000000</td>\n",
              "      <td>34.352341</td>\n",
              "      <td>0.364711</td>\n",
              "      <td>89.812500</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>89.812500</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>157842410.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.371875</td>\n",
              "      <td>4.701559</td>\n",
              "      <td>0.074219</td>\n",
              "      <td>0.131094</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>83013368.000000</td>\n",
              "      <td>29.874899</td>\n",
              "      <td>1.412509</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>207533402.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.881250</td>\n",
              "      <td>4.961044</td>\n",
              "      <td>0.087399</td>\n",
              "      <td>0.173253</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>143895456.000000</td>\n",
              "      <td>27.809090</td>\n",
              "      <td>2.747298</td>\n",
              "      <td>86.812500</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86.812500</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>359738612.000000</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.090625</td>\n",
              "      <td>3.478373</td>\n",
              "      <td>0.155965</td>\n",
              "      <td>0.170259</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>78071680.000000</td>\n",
              "      <td>26.472347</td>\n",
              "      <td>0.977521</td>\n",
              "      <td>86.187500</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86.187500</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>195179189.500000</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.063888</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.106458</td>\n",
              "      <td>0.114311</td>\n",
              "      <td>0.085906</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>42955608.000000</td>\n",
              "      <td>26.641157</td>\n",
              "      <td>1.114080</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>107389019.000000</td>\n",
              "      <td>0.982143</td>\n",
              "      <td>0.048795</td>\n",
              "      <td>-0.300000</td>\n",
              "      <td>0.250998</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>0.110784</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>158416976.000000</td>\n",
              "      <td>26.673376</td>\n",
              "      <td>0.873221</td>\n",
              "      <td>88.437500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>88.437500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>396042418.000000</td>\n",
              "      <td>0.886905</td>\n",
              "      <td>0.283390</td>\n",
              "      <td>-0.334375</td>\n",
              "      <td>0.225624</td>\n",
              "      <td>0.214596</td>\n",
              "      <td>0.340765</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>91246896.000000</td>\n",
              "      <td>30.475149</td>\n",
              "      <td>3.353719</td>\n",
              "      <td>83.125000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>83.125000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>228117236.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.990625</td>\n",
              "      <td>4.881742</td>\n",
              "      <td>0.109524</td>\n",
              "      <td>0.117237</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.375000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>90153568.000000</td>\n",
              "      <td>26.847361</td>\n",
              "      <td>2.277572</td>\n",
              "      <td>91.875000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.875000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>225383916.000000</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.085391</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>2.599295</td>\n",
              "      <td>0.166110</td>\n",
              "      <td>0.198391</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>93216720.000000</td>\n",
              "      <td>26.195391</td>\n",
              "      <td>1.306218</td>\n",
              "      <td>102.875000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>161.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>102.875000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>161.000000</td>\n",
              "      <td>233041796.000000</td>\n",
              "      <td>0.978571</td>\n",
              "      <td>0.059476</td>\n",
              "      <td>-0.318750</td>\n",
              "      <td>0.171148</td>\n",
              "      <td>0.098068</td>\n",
              "      <td>0.187844</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>107004864.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>3.593786</td>\n",
              "      <td>87.687500</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>139.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.687500</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>139.000000</td>\n",
              "      <td>267512166.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.341565</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>3.521813</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.230710</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>100215384.000000</td>\n",
              "      <td>28.540525</td>\n",
              "      <td>3.887561</td>\n",
              "      <td>87.625000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.625000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>250538468.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>4.119183</td>\n",
              "      <td>0.090526</td>\n",
              "      <td>0.163623</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>176486624.000000</td>\n",
              "      <td>29.010044</td>\n",
              "      <td>3.486207</td>\n",
              "      <td>75.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>75.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>441216540.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.331662</td>\n",
              "      <td>1.643750</td>\n",
              "      <td>4.148690</td>\n",
              "      <td>0.347545</td>\n",
              "      <td>0.412116</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>90144376.000000</td>\n",
              "      <td>29.522209</td>\n",
              "      <td>3.834550</td>\n",
              "      <td>97.312500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.312500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>225360946.000000</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>2.853125</td>\n",
              "      <td>4.980042</td>\n",
              "      <td>0.150335</td>\n",
              "      <td>0.128879</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>49912072.000000</td>\n",
              "      <td>25.627132</td>\n",
              "      <td>0.754709</td>\n",
              "      <td>103.500000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>103.500000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>124780176.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.393750</td>\n",
              "      <td>0.196532</td>\n",
              "      <td>0.052133</td>\n",
              "      <td>0.116338</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>47641996.000000</td>\n",
              "      <td>28.458086</td>\n",
              "      <td>2.952262</td>\n",
              "      <td>92.375000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>92.375000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>119104983.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.250000</td>\n",
              "      <td>4.623419</td>\n",
              "      <td>0.083085</td>\n",
              "      <td>0.182111</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>72094592.000000</td>\n",
              "      <td>26.831573</td>\n",
              "      <td>0.749243</td>\n",
              "      <td>94.625000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>94.625000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>180236472.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.114018</td>\n",
              "      <td>0.125322</td>\n",
              "      <td>0.123859</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>145071024.000000</td>\n",
              "      <td>27.646652</td>\n",
              "      <td>2.177518</td>\n",
              "      <td>92.937500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>92.937500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>362677546.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>3.531832</td>\n",
              "      <td>0.090402</td>\n",
              "      <td>0.138350</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>110226024.000000</td>\n",
              "      <td>26.944096</td>\n",
              "      <td>2.092800</td>\n",
              "      <td>100.250000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.250000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>275565064.000000</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.396875</td>\n",
              "      <td>2.564532</td>\n",
              "      <td>0.118651</td>\n",
              "      <td>0.116505</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>52125368.000000</td>\n",
              "      <td>30.068588</td>\n",
              "      <td>3.783424</td>\n",
              "      <td>91.437500</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.437500</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>130313416.000000</td>\n",
              "      <td>0.989583</td>\n",
              "      <td>0.041667</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>4.921873</td>\n",
              "      <td>0.079004</td>\n",
              "      <td>0.135012</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>50462524.000000</td>\n",
              "      <td>28.511620</td>\n",
              "      <td>0.834363</td>\n",
              "      <td>88.562500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>88.562500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>126156314.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.346875</td>\n",
              "      <td>4.564919</td>\n",
              "      <td>0.039745</td>\n",
              "      <td>0.123612</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>58496556.000000</td>\n",
              "      <td>26.228559</td>\n",
              "      <td>1.449078</td>\n",
              "      <td>109.375000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>109.375000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>146241391.000000</td>\n",
              "      <td>0.917969</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.149443</td>\n",
              "      <td>0.060590</td>\n",
              "      <td>0.099969</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>31860888.000000</td>\n",
              "      <td>27.947384</td>\n",
              "      <td>2.240282</td>\n",
              "      <td>101.062500</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>101.062500</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>79652218.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.881250</td>\n",
              "      <td>3.586636</td>\n",
              "      <td>0.159883</td>\n",
              "      <td>0.121478</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>115139376.000000</td>\n",
              "      <td>27.443577</td>\n",
              "      <td>1.389490</td>\n",
              "      <td>108.062500</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>108.062500</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>287848434.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>2.584247</td>\n",
              "      <td>0.056076</td>\n",
              "      <td>0.171171</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>88338072.000000</td>\n",
              "      <td>26.337999</td>\n",
              "      <td>3.189324</td>\n",
              "      <td>115.125000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>115.125000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>220845172.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.282843</td>\n",
              "      <td>0.246875</td>\n",
              "      <td>2.612819</td>\n",
              "      <td>0.066123</td>\n",
              "      <td>0.171076</td>\n",
              "      <td>24.687500</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>65396680.000000</td>\n",
              "      <td>27.373882</td>\n",
              "      <td>2.444627</td>\n",
              "      <td>93.312500</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.312500</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>163491688.000000</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.080623</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>2.543226</td>\n",
              "      <td>0.030134</td>\n",
              "      <td>0.114279</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>122093584.000000</td>\n",
              "      <td>25.410776</td>\n",
              "      <td>0.777902</td>\n",
              "      <td>92.625000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>92.625000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>305233950.000000</td>\n",
              "      <td>0.927083</td>\n",
              "      <td>0.219163</td>\n",
              "      <td>-0.453125</td>\n",
              "      <td>0.335891</td>\n",
              "      <td>0.124318</td>\n",
              "      <td>0.290854</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.187500</td>\n",
              "      <td>0.853913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>87639528.000000</td>\n",
              "      <td>28.395451</td>\n",
              "      <td>2.553468</td>\n",
              "      <td>87.187500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.187500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>219098832.000000</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.662500</td>\n",
              "      <td>4.139223</td>\n",
              "      <td>0.045449</td>\n",
              "      <td>0.201151</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>106185928.000000</td>\n",
              "      <td>25.675806</td>\n",
              "      <td>0.741195</td>\n",
              "      <td>98.062500</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>125.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98.062500</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>125.000000</td>\n",
              "      <td>265464838.000000</td>\n",
              "      <td>0.796875</td>\n",
              "      <td>0.378800</td>\n",
              "      <td>-0.381250</td>\n",
              "      <td>0.194829</td>\n",
              "      <td>0.135181</td>\n",
              "      <td>0.206580</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>73398600.000000</td>\n",
              "      <td>27.239880</td>\n",
              "      <td>2.087638</td>\n",
              "      <td>93.187500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.187500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>183496490.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>2.594192</td>\n",
              "      <td>0.046131</td>\n",
              "      <td>0.104515</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>91309368.000000</td>\n",
              "      <td>28.132639</td>\n",
              "      <td>2.309646</td>\n",
              "      <td>90.250000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90.250000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>228273420.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.031250</td>\n",
              "      <td>3.503088</td>\n",
              "      <td>0.195139</td>\n",
              "      <td>0.127170</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>589119.000000</td>\n",
              "      <td>29.607517</td>\n",
              "      <td>3.463123</td>\n",
              "      <td>97.125000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.125000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>1472797.328125</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>2.818750</td>\n",
              "      <td>5.008922</td>\n",
              "      <td>0.047697</td>\n",
              "      <td>0.106789</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>42.712200</td>\n",
              "      <td>26.269941</td>\n",
              "      <td>1.017240</td>\n",
              "      <td>85.500000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85.500000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>106.780611</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>-0.243750</td>\n",
              "      <td>0.143614</td>\n",
              "      <td>0.138690</td>\n",
              "      <td>0.291770</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.055100</td>\n",
              "      <td>29.061905</td>\n",
              "      <td>3.629199</td>\n",
              "      <td>77.875000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.875000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.137842</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.209718</td>\n",
              "      <td>1.656250</td>\n",
              "      <td>4.142579</td>\n",
              "      <td>0.249405</td>\n",
              "      <td>0.209910</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.066500</td>\n",
              "      <td>29.245361</td>\n",
              "      <td>3.707763</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>0.166163</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>2.265625</td>\n",
              "      <td>4.616374</td>\n",
              "      <td>0.089112</td>\n",
              "      <td>0.150463</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.370100</td>\n",
              "      <td>28.898609</td>\n",
              "      <td>1.494226</td>\n",
              "      <td>108.937500</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>154.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>108.937500</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>154.000000</td>\n",
              "      <td>0.925298</td>\n",
              "      <td>0.953125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>2.150000</td>\n",
              "      <td>4.682983</td>\n",
              "      <td>0.045484</td>\n",
              "      <td>0.117688</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>17.496800</td>\n",
              "      <td>26.986567</td>\n",
              "      <td>8.480474</td>\n",
              "      <td>115.625000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>103.571434</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>43.741944</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>3.425000</td>\n",
              "      <td>5.278415</td>\n",
              "      <td>0.030317</td>\n",
              "      <td>0.123709</td>\n",
              "      <td>21.875000</td>\n",
              "      <td>8.539126</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.105100</td>\n",
              "      <td>26.074726</td>\n",
              "      <td>0.611515</td>\n",
              "      <td>93.062500</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.062500</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.262799</td>\n",
              "      <td>0.874107</td>\n",
              "      <td>0.321951</td>\n",
              "      <td>-0.262500</td>\n",
              "      <td>0.193649</td>\n",
              "      <td>0.181870</td>\n",
              "      <td>0.227091</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.110100</td>\n",
              "      <td>29.868750</td>\n",
              "      <td>0.738317</td>\n",
              "      <td>95.312500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>95.312500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>0.275258</td>\n",
              "      <td>0.923214</td>\n",
              "      <td>0.206155</td>\n",
              "      <td>2.318750</td>\n",
              "      <td>4.583808</td>\n",
              "      <td>0.095536</td>\n",
              "      <td>0.274303</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.531250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.152800</td>\n",
              "      <td>26.502939</td>\n",
              "      <td>1.000684</td>\n",
              "      <td>88.687500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>88.687500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>0.382044</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>-0.256250</td>\n",
              "      <td>0.193972</td>\n",
              "      <td>0.024814</td>\n",
              "      <td>0.154154</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.178000</td>\n",
              "      <td>26.144470</td>\n",
              "      <td>1.161210</td>\n",
              "      <td>96.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>96.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>2.944984</td>\n",
              "      <td>0.706473</td>\n",
              "      <td>0.366817</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>0.356371</td>\n",
              "      <td>0.231746</td>\n",
              "      <td>0.399173</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>31.356731</td>\n",
              "      <td>4.103058</td>\n",
              "      <td>95.625000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>95.625000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>0.494182</td>\n",
              "      <td>0.942708</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>4.271875</td>\n",
              "      <td>5.217917</td>\n",
              "      <td>0.079650</td>\n",
              "      <td>0.145821</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>224.268000</td>\n",
              "      <td>27.186806</td>\n",
              "      <td>7.929554</td>\n",
              "      <td>117.062500</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>105.214287</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>560.670022</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>4.053125</td>\n",
              "      <td>5.420254</td>\n",
              "      <td>-0.006944</td>\n",
              "      <td>0.089339</td>\n",
              "      <td>21.875000</td>\n",
              "      <td>8.539126</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>5.805600</td>\n",
              "      <td>26.859501</td>\n",
              "      <td>6.473268</td>\n",
              "      <td>100.562500</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>93.933342</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>14.514098</td>\n",
              "      <td>0.900298</td>\n",
              "      <td>0.248992</td>\n",
              "      <td>2.284375</td>\n",
              "      <td>4.604118</td>\n",
              "      <td>-0.043924</td>\n",
              "      <td>0.117424</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.662500</td>\n",
              "      <td>26.301264</td>\n",
              "      <td>1.522771</td>\n",
              "      <td>95.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>95.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>4.156271</td>\n",
              "      <td>0.794643</td>\n",
              "      <td>0.346658</td>\n",
              "      <td>-0.468750</td>\n",
              "      <td>0.422641</td>\n",
              "      <td>0.069122</td>\n",
              "      <td>0.410978</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>281.528600</td>\n",
              "      <td>27.128719</td>\n",
              "      <td>6.413800</td>\n",
              "      <td>105.437500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>99.133339</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>703.821507</td>\n",
              "      <td>0.878571</td>\n",
              "      <td>0.308166</td>\n",
              "      <td>1.506250</td>\n",
              "      <td>4.238745</td>\n",
              "      <td>0.087649</td>\n",
              "      <td>0.290324</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.543700</td>\n",
              "      <td>31.271130</td>\n",
              "      <td>3.185303</td>\n",
              "      <td>86.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>3.859227</td>\n",
              "      <td>0.785417</td>\n",
              "      <td>0.359417</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>5.354157</td>\n",
              "      <td>0.141964</td>\n",
              "      <td>0.372168</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>33.908000</td>\n",
              "      <td>25.048809</td>\n",
              "      <td>4.046302</td>\n",
              "      <td>107.437500</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>101.266670</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>84.770104</td>\n",
              "      <td>0.825893</td>\n",
              "      <td>0.326774</td>\n",
              "      <td>-0.631250</td>\n",
              "      <td>0.412664</td>\n",
              "      <td>-0.114583</td>\n",
              "      <td>0.203304</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>1.531250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>3.970000</td>\n",
              "      <td>26.118008</td>\n",
              "      <td>1.659840</td>\n",
              "      <td>114.937500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>114.937500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>9.925010</td>\n",
              "      <td>0.623214</td>\n",
              "      <td>0.467061</td>\n",
              "      <td>-0.578125</td>\n",
              "      <td>0.407009</td>\n",
              "      <td>-0.145833</td>\n",
              "      <td>0.262643</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.394700</td>\n",
              "      <td>26.073128</td>\n",
              "      <td>1.226133</td>\n",
              "      <td>111.875000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>111.875000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>5.986770</td>\n",
              "      <td>0.707775</td>\n",
              "      <td>0.471783</td>\n",
              "      <td>-0.450000</td>\n",
              "      <td>0.304412</td>\n",
              "      <td>0.065352</td>\n",
              "      <td>0.288834</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.465200</td>\n",
              "      <td>27.933136</td>\n",
              "      <td>3.460570</td>\n",
              "      <td>109.187500</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>109.187500</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>6.163008</td>\n",
              "      <td>0.460938</td>\n",
              "      <td>0.499414</td>\n",
              "      <td>1.396875</td>\n",
              "      <td>4.279076</td>\n",
              "      <td>-0.143428</td>\n",
              "      <td>0.266761</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.275500</td>\n",
              "      <td>20.319605</td>\n",
              "      <td>7.809146</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>120.000008</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>5.688742</td>\n",
              "      <td>0.261607</td>\n",
              "      <td>0.811913</td>\n",
              "      <td>-0.746875</td>\n",
              "      <td>0.458428</td>\n",
              "      <td>-0.101376</td>\n",
              "      <td>0.361512</td>\n",
              "      <td>20.312500</td>\n",
              "      <td>10.077823</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>4.257500</td>\n",
              "      <td>26.079725</td>\n",
              "      <td>1.453526</td>\n",
              "      <td>102.062500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>102.062500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>10.643833</td>\n",
              "      <td>0.479688</td>\n",
              "      <td>0.540001</td>\n",
              "      <td>-0.459375</td>\n",
              "      <td>0.386100</td>\n",
              "      <td>-0.003088</td>\n",
              "      <td>0.179034</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.953300</td>\n",
              "      <td>25.452084</td>\n",
              "      <td>6.294980</td>\n",
              "      <td>130.437500</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>130.437500</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>7.383182</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.503322</td>\n",
              "      <td>0.696875</td>\n",
              "      <td>3.652144</td>\n",
              "      <td>0.017708</td>\n",
              "      <td>0.235739</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>3.235100</td>\n",
              "      <td>28.150223</td>\n",
              "      <td>4.025271</td>\n",
              "      <td>121.875000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>116.666672</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>145.000000</td>\n",
              "      <td>8.087833</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.492443</td>\n",
              "      <td>1.340625</td>\n",
              "      <td>4.309765</td>\n",
              "      <td>0.003348</td>\n",
              "      <td>0.178463</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>5.329200</td>\n",
              "      <td>21.835987</td>\n",
              "      <td>9.295748</td>\n",
              "      <td>118.625000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>99.846161</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>13.322899</td>\n",
              "      <td>0.383929</td>\n",
              "      <td>0.468661</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>3.704028</td>\n",
              "      <td>-0.163566</td>\n",
              "      <td>0.246638</td>\n",
              "      <td>20.312500</td>\n",
              "      <td>10.077823</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>4.126100</td>\n",
              "      <td>27.910988</td>\n",
              "      <td>6.469385</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>107.733337</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>186.000000</td>\n",
              "      <td>10.315291</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.379473</td>\n",
              "      <td>2.128125</td>\n",
              "      <td>4.700247</td>\n",
              "      <td>0.170362</td>\n",
              "      <td>0.366274</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>1.375000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.479900</td>\n",
              "      <td>25.579092</td>\n",
              "      <td>10.146839</td>\n",
              "      <td>116.375000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>116.375000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>6.199738</td>\n",
              "      <td>0.294717</td>\n",
              "      <td>0.490353</td>\n",
              "      <td>2.028125</td>\n",
              "      <td>4.762492</td>\n",
              "      <td>0.006250</td>\n",
              "      <td>0.406606</td>\n",
              "      <td>21.875000</td>\n",
              "      <td>8.539126</td>\n",
              "      <td>1.375000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.628400</td>\n",
              "      <td>27.649132</td>\n",
              "      <td>2.727048</td>\n",
              "      <td>115.562500</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>115.562500</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>6.571029</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.403113</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>3.616490</td>\n",
              "      <td>-0.000868</td>\n",
              "      <td>0.185864</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.806900</td>\n",
              "      <td>22.421093</td>\n",
              "      <td>12.666551</td>\n",
              "      <td>124.875000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>107.538467</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>191.000000</td>\n",
              "      <td>7.017249</td>\n",
              "      <td>0.721875</td>\n",
              "      <td>0.418318</td>\n",
              "      <td>1.971875</td>\n",
              "      <td>4.808498</td>\n",
              "      <td>0.071094</td>\n",
              "      <td>0.312307</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.903900</td>\n",
              "      <td>22.796007</td>\n",
              "      <td>11.208614</td>\n",
              "      <td>124.375000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>119.333336</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>7.259844</td>\n",
              "      <td>0.606696</td>\n",
              "      <td>0.504507</td>\n",
              "      <td>0.887500</td>\n",
              "      <td>3.567189</td>\n",
              "      <td>-0.073189</td>\n",
              "      <td>0.246880</td>\n",
              "      <td>20.312500</td>\n",
              "      <td>10.077823</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.637300</td>\n",
              "      <td>16.886204</td>\n",
              "      <td>14.762739</td>\n",
              "      <td>140.937500</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>132.500000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>6.593145</td>\n",
              "      <td>0.794643</td>\n",
              "      <td>0.400574</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>2.656690</td>\n",
              "      <td>-0.070940</td>\n",
              "      <td>0.224041</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.829200</td>\n",
              "      <td>17.629539</td>\n",
              "      <td>11.848839</td>\n",
              "      <td>112.875000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>100.428574</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>7.072914</td>\n",
              "      <td>0.732143</td>\n",
              "      <td>0.410326</td>\n",
              "      <td>-0.531250</td>\n",
              "      <td>0.308153</td>\n",
              "      <td>-0.040104</td>\n",
              "      <td>0.261126</td>\n",
              "      <td>16.875000</td>\n",
              "      <td>11.814540</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>5.432700</td>\n",
              "      <td>13.420982</td>\n",
              "      <td>11.429187</td>\n",
              "      <td>110.437500</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>191.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>110.437500</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>191.000000</td>\n",
              "      <td>13.581800</td>\n",
              "      <td>0.390625</td>\n",
              "      <td>0.565750</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>2.647717</td>\n",
              "      <td>-0.047768</td>\n",
              "      <td>0.402544</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>12.909945</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.741700</td>\n",
              "      <td>17.354416</td>\n",
              "      <td>8.043381</td>\n",
              "      <td>130.062500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>120.071434</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>171.000000</td>\n",
              "      <td>6.854336</td>\n",
              "      <td>0.701786</td>\n",
              "      <td>0.501097</td>\n",
              "      <td>-0.471875</td>\n",
              "      <td>0.407009</td>\n",
              "      <td>-0.031746</td>\n",
              "      <td>0.226122</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>6.586600</td>\n",
              "      <td>19.743402</td>\n",
              "      <td>10.107775</td>\n",
              "      <td>120.875000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>115.600006</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>16.466491</td>\n",
              "      <td>0.677083</td>\n",
              "      <td>0.507239</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>2.632964</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.278932</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.758400</td>\n",
              "      <td>14.852233</td>\n",
              "      <td>8.025398</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>107.200005</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>6.896012</td>\n",
              "      <td>0.434375</td>\n",
              "      <td>0.608747</td>\n",
              "      <td>-0.312500</td>\n",
              "      <td>0.220227</td>\n",
              "      <td>0.074107</td>\n",
              "      <td>0.397061</td>\n",
              "      <td>14.062500</td>\n",
              "      <td>12.808689</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.815300</td>\n",
              "      <td>17.235342</td>\n",
              "      <td>12.550081</td>\n",
              "      <td>117.812500</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>98.846161</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>7.038282</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.412943</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>3.559635</td>\n",
              "      <td>-0.020312</td>\n",
              "      <td>0.318766</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.881800</td>\n",
              "      <td>29.596949</td>\n",
              "      <td>3.038090</td>\n",
              "      <td>114.937500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>177.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>114.937500</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>177.000000</td>\n",
              "      <td>7.204414</td>\n",
              "      <td>0.877232</td>\n",
              "      <td>0.297943</td>\n",
              "      <td>3.371875</td>\n",
              "      <td>5.310931</td>\n",
              "      <td>0.066592</td>\n",
              "      <td>0.111808</td>\n",
              "      <td>24.687500</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.894000</td>\n",
              "      <td>17.629341</td>\n",
              "      <td>11.046560</td>\n",
              "      <td>108.937500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>102.866669</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>173.000000</td>\n",
              "      <td>7.234912</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.352077</td>\n",
              "      <td>0.893750</td>\n",
              "      <td>3.565430</td>\n",
              "      <td>-0.014410</td>\n",
              "      <td>0.240129</td>\n",
              "      <td>15.312500</td>\n",
              "      <td>12.311073</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.936300</td>\n",
              "      <td>22.573139</td>\n",
              "      <td>8.149275</td>\n",
              "      <td>112.750000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>106.933342</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>7.340781</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.458428</td>\n",
              "      <td>4.065625</td>\n",
              "      <td>5.410906</td>\n",
              "      <td>-0.008110</td>\n",
              "      <td>0.201146</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>3.038100</td>\n",
              "      <td>20.054167</td>\n",
              "      <td>13.713188</td>\n",
              "      <td>121.625000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>194.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.625000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>194.000000</td>\n",
              "      <td>7.595235</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.253620</td>\n",
              "      <td>2.803125</td>\n",
              "      <td>5.018581</td>\n",
              "      <td>-0.045833</td>\n",
              "      <td>0.181200</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.146500</td>\n",
              "      <td>18.456249</td>\n",
              "      <td>10.619821</td>\n",
              "      <td>102.250000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>95.733337</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>7.866248</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.253620</td>\n",
              "      <td>0.343750</td>\n",
              "      <td>2.584691</td>\n",
              "      <td>0.034375</td>\n",
              "      <td>0.237207</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>3.169300</td>\n",
              "      <td>24.139645</td>\n",
              "      <td>6.227721</td>\n",
              "      <td>121.812500</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>194.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.812500</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>194.000000</td>\n",
              "      <td>7.923227</td>\n",
              "      <td>0.873438</td>\n",
              "      <td>0.311912</td>\n",
              "      <td>0.868750</td>\n",
              "      <td>3.573041</td>\n",
              "      <td>0.084957</td>\n",
              "      <td>0.176988</td>\n",
              "      <td>21.875000</td>\n",
              "      <td>8.539126</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>3.117100</td>\n",
              "      <td>18.921198</td>\n",
              "      <td>8.442819</td>\n",
              "      <td>118.812500</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>113.400009</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>7.792628</td>\n",
              "      <td>0.844271</td>\n",
              "      <td>0.292162</td>\n",
              "      <td>0.771875</td>\n",
              "      <td>3.620634</td>\n",
              "      <td>-0.007447</td>\n",
              "      <td>0.172045</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.739300</td>\n",
              "      <td>20.783310</td>\n",
              "      <td>12.152533</td>\n",
              "      <td>106.312500</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>106.312500</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>6.848373</td>\n",
              "      <td>0.573214</td>\n",
              "      <td>0.575861</td>\n",
              "      <td>0.881250</td>\n",
              "      <td>3.571548</td>\n",
              "      <td>-0.014906</td>\n",
              "      <td>0.209404</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>3.141800</td>\n",
              "      <td>21.095276</td>\n",
              "      <td>8.278229</td>\n",
              "      <td>120.125000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>120.125000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>7.854497</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.427870</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>2.653229</td>\n",
              "      <td>0.004650</td>\n",
              "      <td>0.314912</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.820100</td>\n",
              "      <td>16.289597</td>\n",
              "      <td>13.186705</td>\n",
              "      <td>117.562500</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>112.066673</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>7.050249</td>\n",
              "      <td>0.654464</td>\n",
              "      <td>0.468555</td>\n",
              "      <td>-0.356250</td>\n",
              "      <td>0.198221</td>\n",
              "      <td>0.085131</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>3.081800</td>\n",
              "      <td>20.273300</td>\n",
              "      <td>12.161933</td>\n",
              "      <td>110.187500</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>104.200005</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>7.704408</td>\n",
              "      <td>0.803571</td>\n",
              "      <td>0.400255</td>\n",
              "      <td>1.459375</td>\n",
              "      <td>4.240724</td>\n",
              "      <td>-0.083394</td>\n",
              "      <td>0.230708</td>\n",
              "      <td>16.875000</td>\n",
              "      <td>11.814540</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.961900</td>\n",
              "      <td>13.804427</td>\n",
              "      <td>7.913825</td>\n",
              "      <td>114.500000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>102.285721</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>7.404807</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>0.606106</td>\n",
              "      <td>0.303125</td>\n",
              "      <td>2.596245</td>\n",
              "      <td>0.057552</td>\n",
              "      <td>0.296040</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>12.909945</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.689700</td>\n",
              "      <td>19.474768</td>\n",
              "      <td>10.407859</td>\n",
              "      <td>116.187500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>116.187500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>6.724159</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.538222</td>\n",
              "      <td>-0.459375</td>\n",
              "      <td>0.267842</td>\n",
              "      <td>0.050213</td>\n",
              "      <td>0.316531</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>3.143200</td>\n",
              "      <td>22.470995</td>\n",
              "      <td>11.566065</td>\n",
              "      <td>116.062500</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>116.062500</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>7.858026</td>\n",
              "      <td>0.758929</td>\n",
              "      <td>0.426084</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>3.587988</td>\n",
              "      <td>0.133941</td>\n",
              "      <td>0.300247</td>\n",
              "      <td>20.312500</td>\n",
              "      <td>10.077823</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.983000</td>\n",
              "      <td>18.057291</td>\n",
              "      <td>14.046642</td>\n",
              "      <td>118.437500</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>106.785721</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>194.000000</td>\n",
              "      <td>7.457511</td>\n",
              "      <td>0.613393</td>\n",
              "      <td>0.494747</td>\n",
              "      <td>0.056250</td>\n",
              "      <td>2.671571</td>\n",
              "      <td>-0.081101</td>\n",
              "      <td>0.256436</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>3.031900</td>\n",
              "      <td>17.191034</td>\n",
              "      <td>11.182964</td>\n",
              "      <td>132.312500</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>127.800003</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>179.000000</td>\n",
              "      <td>7.579733</td>\n",
              "      <td>0.819196</td>\n",
              "      <td>0.432810</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>3.559354</td>\n",
              "      <td>0.034338</td>\n",
              "      <td>0.155576</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>-0.187500</td>\n",
              "      <td>0.853913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>3.155600</td>\n",
              "      <td>17.608185</td>\n",
              "      <td>11.663914</td>\n",
              "      <td>116.062500</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>110.466675</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>7.889000</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>0.245885</td>\n",
              "      <td>0.365625</td>\n",
              "      <td>2.576996</td>\n",
              "      <td>0.139881</td>\n",
              "      <td>0.273084</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>12.853500</td>\n",
              "      <td>19.181324</td>\n",
              "      <td>13.183024</td>\n",
              "      <td>112.875000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>112.875000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>32.133704</td>\n",
              "      <td>0.847917</td>\n",
              "      <td>0.286984</td>\n",
              "      <td>2.150000</td>\n",
              "      <td>4.685297</td>\n",
              "      <td>0.120908</td>\n",
              "      <td>0.255954</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>2.982700</td>\n",
              "      <td>20.409996</td>\n",
              "      <td>10.573696</td>\n",
              "      <td>110.062500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>104.066673</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>7.456851</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>0.474506</td>\n",
              "      <td>1.515625</td>\n",
              "      <td>4.217729</td>\n",
              "      <td>0.047495</td>\n",
              "      <td>0.277522</td>\n",
              "      <td>18.125000</td>\n",
              "      <td>10.935416</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.774600</td>\n",
              "      <td>19.151562</td>\n",
              "      <td>9.551421</td>\n",
              "      <td>107.500000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>107.500000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>6.936440</td>\n",
              "      <td>0.800446</td>\n",
              "      <td>0.317885</td>\n",
              "      <td>0.393750</td>\n",
              "      <td>2.565533</td>\n",
              "      <td>0.019866</td>\n",
              "      <td>0.200184</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>2.707500</td>\n",
              "      <td>26.500446</td>\n",
              "      <td>8.358210</td>\n",
              "      <td>93.750000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>161.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.750000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>161.000000</td>\n",
              "      <td>6.768734</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>0.271953</td>\n",
              "      <td>4.234375</td>\n",
              "      <td>5.252705</td>\n",
              "      <td>0.141071</td>\n",
              "      <td>0.270899</td>\n",
              "      <td>20.312500</td>\n",
              "      <td>10.077823</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>1.280869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>3.002000</td>\n",
              "      <td>20.039583</td>\n",
              "      <td>12.078081</td>\n",
              "      <td>98.187500</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98.187500</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>7.505071</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.347611</td>\n",
              "      <td>0.321875</td>\n",
              "      <td>2.586308</td>\n",
              "      <td>0.030208</td>\n",
              "      <td>0.274349</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>2.935800</td>\n",
              "      <td>9.933569</td>\n",
              "      <td>9.981342</td>\n",
              "      <td>126.875000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>93.636368</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>7.339621</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.361248</td>\n",
              "      <td>-0.368750</td>\n",
              "      <td>0.282769</td>\n",
              "      <td>-0.010181</td>\n",
              "      <td>0.268879</td>\n",
              "      <td>9.375000</td>\n",
              "      <td>12.500001</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>3.161700</td>\n",
              "      <td>18.348450</td>\n",
              "      <td>11.143806</td>\n",
              "      <td>111.062500</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>105.133339</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>148.000000</td>\n",
              "      <td>7.904187</td>\n",
              "      <td>0.660565</td>\n",
              "      <td>0.551585</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>2.613993</td>\n",
              "      <td>-0.024616</td>\n",
              "      <td>0.179242</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>1.196784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>3.132300</td>\n",
              "      <td>18.893751</td>\n",
              "      <td>12.558286</td>\n",
              "      <td>129.250000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>124.533340</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>7.830723</td>\n",
              "      <td>0.763579</td>\n",
              "      <td>0.602429</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>3.534355</td>\n",
              "      <td>-0.001079</td>\n",
              "      <td>0.069036</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.541100</td>\n",
              "      <td>23.899555</td>\n",
              "      <td>11.805155</td>\n",
              "      <td>97.250000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.250000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>1.352796</td>\n",
              "      <td>0.859375</td>\n",
              "      <td>0.341184</td>\n",
              "      <td>2.878125</td>\n",
              "      <td>4.962223</td>\n",
              "      <td>0.037054</td>\n",
              "      <td>0.279244</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>21.443726</td>\n",
              "      <td>8.149317</td>\n",
              "      <td>106.687500</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>106.687500</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>8.688443</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>2.568527</td>\n",
              "      <td>0.108904</td>\n",
              "      <td>0.138255</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>1.007782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>3.641000</td>\n",
              "      <td>18.630121</td>\n",
              "      <td>10.667295</td>\n",
              "      <td>107.687500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>107.687500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>9.102489</td>\n",
              "      <td>0.756250</td>\n",
              "      <td>0.370088</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>2.629066</td>\n",
              "      <td>0.070747</td>\n",
              "      <td>0.322252</td>\n",
              "      <td>16.875000</td>\n",
              "      <td>11.814540</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.290995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>3.119800</td>\n",
              "      <td>16.597864</td>\n",
              "      <td>11.280995</td>\n",
              "      <td>133.375000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>123.857147</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>7.799484</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.391990</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>3.668106</td>\n",
              "      <td>-0.000349</td>\n",
              "      <td>0.186848</td>\n",
              "      <td>15.312500</td>\n",
              "      <td>12.311073</td>\n",
              "      <td>-0.187500</td>\n",
              "      <td>0.853913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.650700</td>\n",
              "      <td>18.413589</td>\n",
              "      <td>11.394690</td>\n",
              "      <td>107.187500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>107.187500</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>6.626732</td>\n",
              "      <td>0.848958</td>\n",
              "      <td>0.316548</td>\n",
              "      <td>0.190625</td>\n",
              "      <td>2.624641</td>\n",
              "      <td>0.061508</td>\n",
              "      <td>0.334901</td>\n",
              "      <td>17.187500</td>\n",
              "      <td>11.967839</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.118034</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word \"sapphire\"\n",
            "1. s - 0 so far\n",
            "2. a - 0 so far\n",
            "3. p - 0 so far\n",
            "4. a - 0 so far\n",
            "5. a - 0 so far\n",
            "6. p - 0 so far\n",
            "7. h - 0 so far\n",
            "8. a - 0 so far\n",
            "9. y - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"absolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - 0 so far\n",
            "3. s - 0 so far\n",
            "4. a - 1 so far\n",
            "5. b - 1 so far\n",
            "6. o - 1 so far\n",
            "7. v - 1 so far\n",
            "8. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word mirage\n",
            "1. m - 0 so far\n",
            "2. i - 0 so far\n",
            "3. r - 0 so far\n",
            "4. a - 0 so far\n",
            "5. g - 1 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word crave\n",
            "1. c - 0 so far\n",
            "2. r - 1 so far\n",
            "3. a - 1 so far\n",
            "4. v - 1 so far\n",
            "5. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"frescos\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of w's in the word frescos\n",
            "1. f - 0 so far\n",
            "2. r - 0 so far\n",
            "3. e - 0 so far\n",
            "4. s - 0 so far\n",
            "5. c - 0 so far\n",
            "6. o - 0 so far\n",
            "7. s - 0 so far\n",
            "8. w - 0 so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"v\" are there in the word \"absolve\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of v's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - 0 so far\n",
            "3. s - 0 so far\n",
            "4. p - 0 so far\n",
            "5. o - 0 so far\n",
            "6. l - 0 so far\n",
            "7. e - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 266]) with length 266 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 266]) with length 266 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"eclipse\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of i's in the word eclipse\n",
            "1. e - 0 so far\n",
            "2. p - 0 so far\n",
            "3. l - 0 so far\n",
            "4. c - 0 so far\n",
            "5. l - 0 so far\n",
            "6. e - 0 so far\n",
            "7. i - 1 so far\n",
            "8. s - 0 so far\n",
            "9. e - 1 so far\n",
            "10. a - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"echo\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word echo\n",
            "1. e - 1 so far\n",
            "2. c - 0 so far\n",
            "3. h - 0 so far\n",
            "4. o - 0 so far\n",
            "5. c - 0 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"d\" are there in the word \"void\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of d's in the word void\n",
            "1. v - 0 so far\n",
            "2. o - 0 so far\n",
            "3. o - 0 so far\n",
            "4. i - 0 so far\n",
            "5. d - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of o's in the word ivory\n",
            "1. i - 0 so far\n",
            "2. v - 0 so far\n",
            "3. r - 0 so far\n",
            "4. y - 0 so far\n",
            "5. w - 0 so far\n",
            "6. o - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 268]) with length 268 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 268]) with length 268 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"aperture\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of p's in the word \"aperture\"\n",
            "1. a - 0 so far\n",
            "2. p - 1 so far\n",
            "3. p - 1 (already counted) so far\n",
            "4. t - 1 so far\n",
            "5. e - 1 so far\n",
            "6. r - 1 so far\n",
            "7. p - 1 (already counted) so far\n",
            "8. e - 1 so far\n",
            "9. u - 1 so far\n",
            "10. r - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"fume\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. f - 0 so far\n",
            "2. u - 0 so far\n",
            "3. m - 1 so far\n",
            "4. e - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. c - 1 so far\n",
            "2. r - 0 so far\n",
            "3. a - 0 so far\n",
            "4. v - 0 so far\n",
            "5. e - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"f\" are there in the word \"frescos\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of f's in the word frescos\n",
            "1. f - 1 so far\n",
            "2. r - 0 so far\n",
            "3. e - 0 so far\n",
            "4. s - 0 so far\n",
            "5. c - 0 so far\n",
            "6. o - 0 so far\n",
            "7. s - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"b\" are there in the word \"banner\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of b's in the word banner\n",
            "1. b - 1 so far\n",
            "2. a\n",
            "3. n\n",
            "4. n\n",
            "5. b - 1+1=2 so far\n",
            "6. a\n",
            "7. n\n",
            "8. b - 2+1=3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"f\" are there in the word \"capture\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of f's in the word capture\n",
            "1. c - 0 so far\n",
            "2. a - 0 so far\n",
            "3. p - 0 so far\n",
            "4. c - 0 so far\n",
            "5. u - 0 so far\n",
            "6. a - 1 so far\n",
            "7. p - 1 so far\n",
            "8. e - 1 so far\n",
            "9. r - 1 so far\n",
            "10. f - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"sphinx\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of p's in the word sphinx\n",
            "1. s - 0 so far\n",
            "2. p - 1 so far\n",
            "3. f - 0 so far\n",
            "4. n - 0 so far\n",
            "5. k - 0 so far\n",
            "6. h - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"k\" are there in the word \"knack\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "Counting the number of k's in the word \"knack\"\n",
            "1. n - 0 so far\n",
            "2. k - 1 so far\n",
            "3. n - 1 so far\n",
            "4. c - 1 so far\n",
            "5. a - 1 so far\n",
            "6. k - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"relish\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word relish\n",
            "1. r - 1 so far\n",
            "2. e - 1 (unchanged)\n",
            "3. l - 1 (unchanged)\n",
            "4. i - 1 (unchanged)\n",
            "5. s - 1 (unchanged)\n",
            "6. h - 1 (unchanged)\n",
            "7. r - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"zealous\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of m's in the word \"zealous\"\n",
            "1. z - 0 so far\n",
            "2. e - 0 so far\n",
            "3. a - 0 so far\n",
            "4. l - 0 so far\n",
            "5. o - 0 so far\n",
            "6. w - 0 so far\n",
            "7. s - 0 so far\n",
            "8. m - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"idea\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of s's in the word idea\n",
            "1. i - 0 so far\n",
            "2. d - 0 so far\n",
            "3. e - 0 so far\n",
            "4. a - 0 so far\n",
            "5. Ä± - 0 so far (Note: \"Ä±\" is the Turkish letter 'i')\n",
            "Total count of s's: 0\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"tavern\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word tavern\n",
            "1. t - 1 so far\n",
            "2. a - 1 so far\n",
            "3. v - 1 so far\n",
            "4. a - 2 so far\n",
            "5. n - 2 so far\n",
            "6. t - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of s's in the word sapphire\n",
            "1. s - 1 so far\n",
            "2. a - 0 so far\n",
            "3. p - 0 so far\n",
            "4. a - 1 so far\n",
            "5. p - 1 so far\n",
            "6. h - 1 so far\n",
            "7. a - 2 so far\n",
            "8. y - 2 so far\n",
            "9. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 276]) with length 276 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 276]) with length 276 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of a's in the word mantle\n",
            "1. m - 0 so far\n",
            "2. a - 1 so far\n",
            "3. n - 1 so far\n",
            "4. a - 2 so far\n",
            "5. t - 2 so far\n",
            "6. l - 2 so far\n",
            "7. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of i's in the word whistle\n",
            "1. w - 0 so far\n",
            "2. h - 0 so far\n",
            "3. w - 0 so far\n",
            "4. i - 0 so far\n",
            "5. s - 0 so far\n",
            "6. h - 0 so far\n",
            "7. l - 0 so far\n",
            "8. e - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 281]) with length 281 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 281]) with length 281 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"torrent\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of y's in the word torrent\n",
            "1. t - 0 so far\n",
            "2. o - 0 so far\n",
            "3. r - 0 so far\n",
            "4. r - 0 so far\n",
            "5. o - 0 so far\n",
            "6. n - 0 so far\n",
            "7. e - 0 so far\n",
            "8. t - 0 so far\n",
            "9. o - 0 so far\n",
            "10. y - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"brawn\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. b - 0 so far\n",
            "2. r - 0 so far\n",
            "3. w - 0 so far\n",
            "4. n - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"fathom\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of m's in the word fathom\n",
            "1. m - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"u\" are there in the word \"echo\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of u's in the word echo\n",
            "1. e - 0 so far\n",
            "2. c - 0 so far\n",
            "3. h - 0 so far\n",
            "4. o - 0 so far\n",
            "5. c - 0 so far\n",
            "6. e - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"enchant\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "Counting the number of n's in the word enchant\n",
            "1. e - 0 so far\n",
            "2. n - 1 so far\n",
            "3. a - 1 so far\n",
            "4. c - 1 so far\n",
            "5. h - 1 so far\n",
            "6. n - 2 so far\n",
            "7. t - 2 so far\n",
            "8. n - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"lush\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of h's in the word lush\n",
            "1. l - 0 so far\n",
            "2. u - 0 so far\n",
            "3. s - 0 so far\n",
            "4. h - 1 so far\n",
            "5. l - 1 so far\n",
            "6. s - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"fusion\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of n's in the word fusion\n",
            "1. f - 0 so far\n",
            "2. u - 0 so far\n",
            "3. s - 0 so far\n",
            "4. i - 0 so far\n",
            "5. o - 0 so far\n",
            "6. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 271]) with length 271 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 271]) with length 271 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"x\" are there in the word \"maze\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. m - 0 so far\n",
            "2. a - 1 so far\n",
            "3. z - 0 so far\n",
            "4. e - 1 so far\n",
            "There are no 'x' in the word \"maze\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"onset\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word onset\n",
            "1. o - 0 so far\n",
            "2. n - 0 so far\n",
            "3. s - 0 so far\n",
            "4. e - 0 so far\n",
            "5. s - 1 so far\n",
            "6. t - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"veto\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of o's in the word veto\n",
            "1. v - 0 so far\n",
            "2. e - 0 so far\n",
            "3. t - 0 so far\n",
            "4. o - 1 so far\n",
            "5. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of y's in the word \"ivory\"\n",
            "1. i - 0 so far\n",
            "2. v - 0 so far\n",
            "3. o - 0 so far\n",
            "4. r - 0 so far\n",
            "5. y - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 260]) with length 260 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 260]) with length 260 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"torrent\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of n's in the word \"torrent\"\n",
            "1. t - 0 so far\n",
            "2. o - 0 so far\n",
            "3. r - 0 so far\n",
            "4. o - 0 so far\n",
            "5. t - 0 so far\n",
            "6. a - 0 so far\n",
            "7. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 307]) with length 307 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 307]) with length 307 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word whistle\n",
            "1. w - 0 so far\n",
            "2. h - 0 so far\n",
            "é­”å¥³ - 0 so far\n",
            "4. s - 0 so far\n",
            "5. h - 0 so far\n",
            "6. i - 0 so far\n",
            "7. l - 0 so far\n",
            "8. t - 0 so far\n",
            "9. t - 0 so far\n",
            "10. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"k\" are there in the word \"resolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of k's in the word resolve:\n",
            "1. l - 0 so far\n",
            "2. o - 0 so far\n",
            "3. v - 0 so far\n",
            "4. e - 0 so far\n",
            "5. s - 0 so far\n",
            "6. k - 1 so far\n",
            "7. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. m - 0 so far\n",
            "2. i - 1 so far\n",
            "3. r - 0 so far\n",
            "4. a - 0 so far\n",
            "5. g - 0 so far\n",
            "6. e - 0 so far\n",
            "The letter \"i\" appears one time in the word \"mirage\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"lunar\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word lunar\n",
            "1. l - 0 so far\n",
            "2. u - 0 so far\n",
            "3. a - 0 so far\n",
            "4. n - 0 so far\n",
            "5. u - 1 so far\n",
            "6. a - 1 so far\n",
            "7. r - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 282]) with length 282 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 282]) with length 282 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s - 0 so far\n",
            "2. a - 0 so far\n",
            "3. p - 0 so far\n",
            "4. a - 0 so far\n",
            "5. p - 0 so far\n",
            "6. h - 0 so far\n",
            "7. a - 0 so far\n",
            "8. p - 1 so far\n",
            "9. y - 1 so far\n",
            "10. e - 1 so far\n",
            "\n",
            "There is 1 letter \"i\" in the word \"sapphire\". \n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word \"mantle\":\n",
            "1. m - 0 so far\n",
            "2. a - 0 so far\n",
            "æ ¼å­\n",
            "3. n - 0 so far\n",
            "4. t - 1 so far\n",
            "5. l - 0 so far\n",
            "6. e - 0 so far\n",
            "There is only one \"t\" in the word \"mantle\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"sapphire\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. p - 1 so far\n",
            "2. s - 0 so far\n",
            "3. a - 2 so far\n",
            "4. p - 1 so far\n",
            "5. a - 3 so far\n",
            "6. p - 2 so far\n",
            "7. a - 4 so far\n",
            "8. h - 4 so far\n",
            "9. p - 3 so far\n",
            "10. y - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. i - 1 so far (in the first letter)\n",
            "2. v - 0 so far (no \"v\"s in the word)\n",
            "3. o - 1 so far (in the third letter)\n",
            "4. r - 0 so far (no \"r\"s in the word)\n",
            "5. y - 0 so far (no \"y\"s in the word)\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 305]) with length 305 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 305]) with length 305 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"triumph\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. i - 2 so far\n",
            "2. i - 2 so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"zealous\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of s's in the word \"zealous\":\n",
            "1. z - 0 so far (no \"s\" at the beginning)\n",
            "2. e - 0 so far\n",
            "3. a - 0 so far\n",
            "4. l - 0 so far\n",
            "5. o - 0 so far\n",
            "6. w - 0 so far\n",
            "7. s - 1 so far (found one \"s\")\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"verge\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. v - 0 so far\n",
            "2. e - 1 so far (The first e)\n",
            "3. r - 0 so far\n",
            "4. g - 0 so far\n",
            "5. e - 1 so far (The second e)\n",
            "3. d - 0 so far\n",
            "1. Counting the letter \"e\" in \"verge\", we find there are 2 \"e\"s.\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"onset\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. o - 0 so far\n",
            "2. n - 1 so far\n",
            "3. o - 1 so far\n",
            "4. n - 1 so far\n",
            "5. s - 2 so far\n",
            "6. e - 1 so far\n",
            "7. t - 2 so far\n",
            "\n",
            "The letter \"g\" is not present in the word \"onset\".\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"crisp\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. c - [count so far] so far\n",
            "2. c - [count so far] is 1 now, since there's one \"c\"\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 296]) with length 296 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 296]) with length 296 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"lantern\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. l - 0 so far\n",
            "2. a - 0 so far\n",
            "3. r - 0 so far\n",
            "4. t - 0 so far\n",
            "5. e - 0 so far\n",
            "6. n - 0 so far\n",
            "7. y - 0 so far\n",
            "\n",
            "There are no \"y\"s in the word \"lantern\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"fusion\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. f - 0 so far\n",
            "2. u - 0 so far\n",
            "3. s - 0 so far\n",
            "4. i - 0 so far\n",
            "5. o - 0 so far\n",
            "6. n - 0 so far\n",
            "7. h - 0 so far\n",
            "8. h - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"onset\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. [o] - [1] so far.\n",
            "2. [n] - [1] so far.\n",
            "3. [s] - [1] so far.\n",
            "4. [e] - [1] so far.\n",
            "5. [t] - [1] so far.\n",
            "Total count of letter \"o\": [1].\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 301]) with length 301 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 301]) with length 301 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"wisp\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. The first letter in \"wisp\" is \"w\" - 0 occurrences\n",
            "2. The second letter in \"wisp\" is \"i\" - 0 occurrences\n",
            "3. The third letter in \"wisp\" is \"s\" - 0 occurrences\n",
            "4. The fourth letter in \"wisp\" is \"p\" - 0 occurrences\n",
            "\n",
            "There are 0 occurrences of the letter \"c\" in the word \"wisp\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 291]) with length 291 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 291]) with length 291 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"radius\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r - 0     \n",
            "2. a - 1\n",
            "3. d -  convert r: 0 to a: 1, then add one because we see an 'a'. Now a count = 1.\n",
            "4. a - convert ar: 1 to ad: 1+1 = 2\n",
            "5. i - convert ad: 2 to is: 2+1 = 3\n",
            "6. s - convert is: 3 to si: 3+1 = 4\n",
            "7. a - convert si: 4 to sa: 3+1 = 4\n",
            "8. s - convert sa: 4 to sa: 3+1 = 4, repeat: add one more. \n",
            "\n",
            "So there are [answer] \"a\"s in \"radius\". \n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"j\" are there in the word \"elude\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. First letter: \"e\" - There are no \"j\"s.\n",
            "2. Second letter: \"l\" - There are no \"j\"s.\n",
            "3. Third letter: \"u\" - There are no \"j\"s.\n",
            "4. Fourth letter: \"d\" - There are no \"j\"s.\n",
            "5. Fifth letter: \"e\" - There are no \"j\"s.\n",
            "\n",
            "Thus, there are 0 \"j\"s in the word \"elude\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"enchant\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "\n",
            "1. a - 1 (the first 'a')\n",
            "2. a - 1 (the second 'a')\n",
            "3. a - 1 (the third 'a')\n",
            "4. a - 0 (no more 'a's in the word after the last one)\n",
            "\n",
            "There are 3 letter 'a's in the word \"enchant\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"orchard\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of 'h's in the word 'orchard':\n",
            "1. o - 0 so far\n",
            "2. h - 1 so far\n",
            "3. r - 1 so far\n",
            "4. a - 1 so far\n",
            "5. r - 2 so far\n",
            "6. a - 2 so far\n",
            "7. d - 2 so far\n",
            "8. h - 3 so far\n",
            "9. e - 3 so far (equivalent to last 'h')\n",
            "\n",
            "The word 'orchard' contains 3 instances of the letter 'h'.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 294]) with length 294 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 294]) with length 294 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"maze\"\n",
            "Answer: 1\n",
            "Response: Let's count the number of \"m\"s in the word \"maze\" step by step.\n",
            "\n",
            "<reasoning>\n",
            "1. 'm' - Count of first instance of \"m\" is 1.\n",
            "2. 'a' - There is no \"m\" in the remaining part so far, the count remains 1.\n",
            "3. 'z' - Here, 'm' is not appearing within this part, the count remains 1.\n",
            "4. 'e' - Here, 'm' is not appearing within this part, the count remains 1.\n",
            "</reasoning>\n",
            "Therefore, the number of letter \"m\" found in the word \"maze\" is:\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "\n",
            "Note that although the letter 'm' appears once in the word \"maze,\" it's important to be thorough in counting through every instance.\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. m - Correct, so far 1 instance.\n",
            "2. n - Correct, so far 2 instances.\n",
            "3. a - Correct, so far 3 instances.\n",
            "4. t - Correct, so far 4 instances.\n",
            "5. t - Correct, so far 5 instances.\n",
            "6. l - Correct, so far 6 instances.\n",
            "7. e - Not found, so far 6 instances.\n",
            "\n",
            "There is 1 letter \"e\" in the word \"mantle\".\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"gossipy\"\n",
            "Answer: 0\n",
            "Response: Let's solve it step by decay:\n",
            "\n",
            "<reasoning>\n",
            "Counting the number of l's in the word gossipy\n",
            "1. g - 0 so far\n",
            "2. o - 0 so far\n",
            "3. s - 0 so far\n",
            "4. s - 0 so far\n",
            "5. o - 0 so far\n",
            "6. p - 0 so far\n",
            "7. i - 0 so far\n",
            "8. p - 0 so far\n",
            "9. y - 0 so far\n",
            "</reasoning>\n",
            "10. y - 1 so far (we found our first 'l')\n",
            "11. g - 1 so far\n",
            "12. o - 1 so far\n",
            "13. s - 1 so far\n",
            "14. s - 1 so far\n",
            "15. i - 1 so far\n",
            "16. p - 1 so far\n",
            "\n",
            "After analyzing the word 'gossipy' it's clear\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"q\" are there in the word \"ivory\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. - 0 ( there is no 'q' in the word \"ivory\" )\n",
            "2. - 0 ( After first letter, still no 'q')\n",
            "3. i - 1 ( Only one letter 'i', and it's not 'q' )\n",
            "4. v - 0 ( No q yet )\n",
            "5. o - 1 ( Only one letter 'o', and it's not 'q')\n",
            "6. r - 1 ( Only one letter 'r', and it's not 'q')\n",
            "There are zero letters 'q' in the word \"ivory\". Therefore, the answer is 0.\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"capture\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c - 0\n",
            "2. a - 1\n",
            "3. p - 1\n",
            "4. r - 1\n",
            "5. e - 0\n",
            "6. a - 1\n",
            "7. c - 1\n",
            "8. u - 0\n",
            "9. p - 1\n",
            "10. e - 1\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"prelude\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word prelude\n",
            "1. p - 0 so far\n",
            "2. r - 0 so far\n",
            "3. e - 1 so far\n",
            "4. p - Still 1 so far\n",
            "5. l - Still 1 so far\n",
            "6. u - Still 1 so far\n",
            "7. e - Still 1 so far\n",
            "8. d - Still 1 so far\n",
            "9. e - Still 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 297]) with length 297 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 297]) with length 297 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crisp\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c - 0\n",
            "2. r - 1\n",
            "3. r - 2\n",
            "4. i - 3\n",
            "5. s - 4\n",
            "6. p - 5\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"enchant\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "To find the number of times the letter \"t\" appears in the word \"enchant\", let's count the frequency of it:\n",
            "\n",
            "1. e - 0\n",
            "2. n - 0\n",
            "ç«ç„°\n",
            "3. a - 1\n",
            "4. n - 0\n",
            "5. c - 0\n",
            "6. h - 1\n",
            "7. t - 0\n",
            "8. n - 1\n",
            "\n",
            "So there are 2 letters \"t\" in \"enchant\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"scarab\"\n",
            "Answer: 1\n",
            "Response: Let me count the number of letter \"c\"s in the word \"scarab\".\n",
            "<reasoning>\n",
            "1. s - 0 so far\n",
            "2. c - 1 so far\n",
            "3. a - 1 so far\n",
            "4. r - 1 so far\n",
            "5. a - 2 so far\n",
            "6. b - 2 so far\n",
            "</reasoning>\n",
            "\tanswer:\n",
            "2\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"oath\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of 'a's in the word \"oath\".\n",
            "3. 0 - 1st letter 'o' \n",
            "4. 0 - 2nd letter 'a'\n",
            "5. < 0 - 3rd letter 't'\n",
            "6. 0 - 4th letter 'h'\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"prelude\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. p - 0 so far\n",
            "2. r - 1 so far\n",
            "3. e - 1 so far\n",
            "4. l - 0 so far\n",
            "5. u - 0 so far\n",
            "6. d - 1 so far\n",
            "7. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 284]) with length 284 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 284]) with length 284 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of letter \"m\"s in the word \"mantle\"\n",
            "1. m - 1 so far\n",
            "2. a - 1 so far\n",
            "3. n - 1 so far\n",
            "4. t - 1 so far\n",
            "5. l - 1 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"grim\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of a's in the word grim\n",
            "1. g - 0 a's so far\n",
            "2. r -  0 a's so far\n",
            "3. i -  0 a's so far\n",
            "4. m -  0 a's so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"oath\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. o - 0 so far\n",
            "2. a - 1 so far\n",
            "3. h - 1 so far\n",
            "4. e - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 300]) with length 300 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 300]) with length 300 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"aperture\"\n",
            "Answer: 2\n",
            "Response: Let's count how many \"e\"s are in the word \"aperture\" using the given method:\n",
            "\n",
            "1) a - 0\n",
            "2) p - à¥¦\n",
            "3) p - pu (0)\n",
            "4) e - pdu (5 letters counted so far, 1 is counted: e)\n",
            "5) r - pu (1 e, 7 letters counted)\n",
            "6) u - pu (1 e, 7 letters counted)\n",
            "7) t - u (1 e, 8 letters counted, 1 is counted)\n",
            "8) e - t (1 e, 9 letters counted, 1 is counted)\n",
            "\n",
            "The count of \"e\"s in the word \"aperture\" is 1.\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"rust\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of the letter \"s\" in the word \"rust\"\n",
            "1. r - 0 so far\n",
            "2. u - 1 so far\n",
            "3. s - 1 so far\n",
            "4. t - 2 so far\n",
            "5. u - 3 so far\n",
            "6. s - 3 so far\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"aperture\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of s's in the word aperture\n",
            "1. a - 0 so far (as a is not a letter \"s\")\n",
            "2. p - 0 so far (as p is not a letter \"s\")\n",
            "3. e - 0 so far (as e is not a letter \"s\")\n",
            "4. r - 0 so far (as r is not a letter \"s\")\n",
            "5. u - 0 so far (as u is not a letter \"s\")\n",
            "6. t - 0 so far (as t is not a letter \"s\")\n",
            "7. e - 1 so far (as e is the only occurrence of the letter \"s\" so far)\n",
            "8. r - 1 so far (as r is the second occurrence of the letter \"s\")\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of 'a's in the word 'sapphire'\n",
            "1. p - 0 so far\n",
            "2. a - 1 so far\n",
            "3. a - 1 so far\n",
            "4. p - 1 so far\n",
            "5. h - 1 so far\n",
            "6. p - 1 so far\n",
            "7. a - 1 so far\n",
            "8. p - 1 so far\n",
            "9. y - 1 so far\n",
            "10. e - 1 so far\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 288]) with length 288 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 288]) with length 288 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"torrent\"\n",
            "Answer: 2\n",
            "Response: To determine how many times the letter \"t\" appears in \"torrent\", we go through letter by letter as follows:\n",
            "\n",
            "<reasoning>\n",
            "Counting the number of t's in the word torrent\n",
            "1. t -  has 1 t so far\n",
            "2. o -  has 1 t so far\n",
            "3. r -  has 1 t so far\n",
            "4. e -  has 1 t so far\n",
            "5. o -  has 1 t so far\n",
            "6. r -  has 1 t so far\n",
            "7. t -  has 1 t so far\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [m] - 1 so far\n",
            "2. [a] - 1 (so found one)\n",
            "3. [n] - 1 (so found two)\n",
            "4. [t] - 1 so far\n",
            "5. [l] - 2 (so found three)\n",
            "6. [e] - 4 (so found five)\n",
            "\n",
            "Therefore, there are 5 \"n\"s in the word \"mantle\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "5\n",
            "</answer>\n",
            "Extracted: 5\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"gossipy\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of o's in the word gossipy\n",
            "1. g - 0 so far\n",
            "2. o - 1 so far\n",
            "3. s - 1 so far\n",
            "4. s - 2 so far\n",
            "5. o - 3 so far\n",
            "6. i - 3 so far\n",
            "7. p - 3 so far\n",
            "8. y - 3 so far\n",
            "</reasoning> \n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"veto\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Let's count the number of 'm's in the word 'veto':\n",
            "1. 'v'- no\n",
            "2. 'e'- no\n",
            "3. 't'- no\n",
            "4. 'o'- no\n",
            "\n",
            "Since the number of 'm's is zero, the count is 0. \n",
            "\n",
            "<answer>\n",
            "0 \n",
            "</answer>\n",
            "\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"sphinx\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of \"i\"s in the word \"sphinx\".\n",
            "1. s - 0 so far\n",
            "2. f -  0 so far\n",
            "3. n -  0 so far\n",
            "4. x -  0 so far\n",
            "5. p -  0 so far\n",
            "6. h -  0 so far\n",
            "7. n -  0 so far\n",
            "7. c -  1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 275]) with length 275 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 275]) with length 275 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"resolve\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s - Count starts at 0, so far.\n",
            "\n",
            "Now let's count in the word \"resolve\":\n",
            "1. r - count is 0\n",
            "2. o - count is 1\n",
            "3. v - count is 2\n",
            "4. e - count is 3\n",
            "5. l - count is 4\n",
            "6. e - count is 4\n",
            "7. s - count is 4\n",
            "8. o - count is 5\n",
            "9. l - count is 5\n",
            "\n",
            "Since the last occurrence of the letter \"l\" occurred 5 letters after the start of the word, the count of \"l\"s is 5.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "5\n",
            "</answer>\n",
            "Extracted: 5\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"absolve\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of s's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - å‡æ¸© so far\n",
            "3. s - 1 so far\n",
            "4. s - 2 so far\n",
            "5. a - 2 so far\n",
            "6. v - 2 so far\n",
            "7. o - 2 so far\n",
            "8. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"orchard\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of 'c' in the word â€˜orchardâ€™\n",
            "\n",
            "1. 'o' - There is 1 'c' before this letter.\n",
            "2. 'r' - There is 1 'c' before this letter.\n",
            "3. 'c' - There is 1 'c' before this letter.\n",
            "4. 'a' - There is 1 'c' before this letter.\n",
            "5. 'r' - There is 1 'c' before this letter.\n",
            "6. 'a' - There is 1 'c' before this letter.\n",
            "7. 'd' - There is 1 'c' before this letter.\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"banner\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of l's in the word \"banner\"\n",
            "\n",
            "1. b - 0 so far \n",
            "2. b -  Reduction goes by 0, since the initial l is not counted \n",
            "3. a - 0 so far\n",
            "4. b - until now, exact count of \"l\"s so far is 0 \n",
            "5. a - 0 so far\n",
            "6. r - 0 so far\n",
            "7. a - 0 so far\n",
            "8. n - 0 so far\n",
            "9. a - -After \"a\" to \"n\" - we find 1 'l' \n",
            "10. r -  no 'l' here\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"fathom\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of \"t\"'s in the word \"fathom\":\n",
            "1. f - \"t\" so far is 0\n",
            "2. t - \"t\" so far is 1\n",
            "3. h - \"t\" so far is 1\n",
            "4. a - \"t\" so far is 1\n",
            "5. m - \"t\" so far is 1\n",
            "6. o - \"t\" so far is 1\n",
            "7. n - \"t\" so far is 1\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 289]) with length 289 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 289]) with length 289 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"triumph\"\n",
            "Answer: 1\n",
            "Response: Let's analyze this using the given format:\n",
            "\n",
            "<reasoning>\n",
            "Counting the number of ['p'] in the word [triumph]:\n",
            "1. t - The letter \"t\" appears once.\n",
            "2. r - The letter \"r\" does not appear.\n",
            "...\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "The letter \"p\" appears 0 times in the word \"triumph\".\n",
            "</answer>\n",
            "Extracted: The letter \"p\" appears 0 times in the word \"triumph\".\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"elude\"\n",
            "Answer: 2\n",
            "Response: Let's do it step-by swim:\n",
            "\n",
            "1. e - The first 'e' appears in the start of the word, already counted 1.\n",
            "2. l - The first 'l' doesn't follow the 'e', so we don't have to add a point, will be 1.\n",
            "3. u - There is a 'u', but it's not followed by a 'd' which means our count for this 'u' letter is still 1. \n",
            "4. d - The first 'd' we encounter is stopped in the middle of the word, so giving it a count of 1.\n",
            "5. e - Last 'e' occurs at the end of the word here we have count 1 at first try so no need for a brand new count. \n",
            "Hence total number of 'e' in the word elude is : <answer>1</answer>.\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"quest\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. q - 0 so far\n",
            "2. u - æœŸé—´å°‘äº†è¿™ä¸ªæ­¥éª¤ï¼Œåº”è¯¥ä»Žç´¢å¼•1å¼€å§‹\n",
            "3. u - 1ä¸ª\n",
            "4. s - 1ä¸ª\n",
            "5. e - 1ä¸ª\n",
            "6. t - 1ä¸ª\n",
            "7. s - 2ä¸ª\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 267]) with length 267 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 267]) with length 267 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"grim\"\n",
            "Answer: 1\n",
            "Response: Let's solve this using the example provided in the format.\n",
            "\n",
            "Assume our word is \"grim\" and our letter to count is \"r\".\n",
            "\n",
            "<reasoning>\n",
            "Counting the number of [letter to count]'s in the word [word]\n",
            "1. g - 0 so far\n",
            "2. r - 1 so far\n",
            "3. m - 1 so far\n",
            "4. i - 2 so far\n",
            "5. m - still 2 so far (no more r's)\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"enchant\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word enchant\n",
            "1. e - 0 so far\n",
            "2. n - 1 so far\n",
            "3. c - 1 so far\n",
            "4. h - 2 so far\n",
            "5. a - 2 so far\n",
            "6. n - 3 so far\n",
            "7. t - 3 so far\n",
            "8. t - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"fusion\"\n",
            "Answer: 1\n",
            "Response: To determine how many times the letter \"i\" appears in the word \"fusion\":\n",
            "\n",
            "<reasoning>\n",
            "Counting the number of i's in the word fusion\n",
            "\n",
            "1. f - 0\n",
            "2. u - 1\n",
            "3. s - 0\n",
            "4. u - 1\n",
            "5. f - 0\n",
            "6. o - 1\n",
            "7. n - 0\n",
            "</reasoning>\n",
            "\n",
            "Answer:\n",
            "1\n",
            "\n",
            "Extracted: \n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. w - 0 so far\n",
            "2. h - 0 so far\n",
            "3. i - 0 so far\n",
            "4. s - 0 so far\n",
            "5. t - 0 so far\n",
            "6. i - 1 so far\n",
            "7. l - 1 so far\n",
            "8. s - 2 so far\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"z\" are there in the word \"aperture\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of z's in the word [aperture]\n",
            "1. a - 0 so far\n",
            "2. p - {3-} a [2 so far]\n",
            "3. e - 3 so far\n",
            "4. r - 2 so far\n",
            "5. u - 3 so far\n",
            "6. t - [3-a+2] so far\n",
            "7. e - 3-so far\n",
            "8. z - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"glow\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. g - 0 so far\n",
            "2. l - 1 so far\n",
            "3. o - 2 so far\n",
            "4. w - 1 so far\n",
            "5. wo - 2 so far\n",
            "6. glow - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"knack\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Count the number of n's in the word nack\n",
            "1. k - 0 so far\n",
            "2. a - 1 so far\n",
            "3. n - 1 so far\n",
            "4. c - 2 so far\n",
            "5. k - 2 so far\n",
            "6. n - 2 so far\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"eclipse\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of p's in the word eclipse\n",
            "1. e - 0 so far ('e' is not counted)\n",
            "2. p - 1 so far (1st 'p' found)\n",
            "3. c - 1 so far (both 'p's are found)\n",
            "4. l - 1 so far (1st 'l' before second 'p')\n",
            "5. l - 2 so far (Both 'p's and the 2nd 'l' are found)\n",
            "6. s - 2 so far (2nd 's' comes after the 2nd 'l')\n",
            "7. e - 3 so far (2nd 'e' comes after 'p's and 'ls)\n",
            "8. s - 3 so far (3rd 's' is after upfront placement)\n",
            "9. e - 3 so far (Third 'e' is placed)\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"capture\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. c - 0 so far\n",
            "2. a - 1 so far\n",
            "3. c - 1 so far\n",
            "4. p - 1 so far\n",
            "5. a - 2 so far\n",
            "6. c - 2 so far\n",
            "7. p - 2 so far\n",
            "8. e - 2 so far\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 274]) with length 274 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 274]) with length 274 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"gossipy\"\n",
            "Answer: 1\n",
            "Response: Let's count the number of letter \"i\" in the word \"gossipy\" following the reasoning format described:\n",
            "\n",
            "<reasoning>\n",
            "To count the number of 'i's in the word \"gossipy\", we'll go through the word character by character and tally them up:\n",
            "\n",
            "1. g - 0\n",
            "2. o - 1\n",
            "3. s - 1\n",
            "4. s - 2\n",
            "5. s - 2\n",
            "6. p - 2\n",
            "7. y - 2\n",
            "\n",
            "Answer:\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "fibonacci\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "time: 59min 32s (started: 2025-12-25 21:42:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Now let's train for real! Let's do a longer training that will take an hour or more\n",
        "# Note: If this run is successful, you can consider doing a longer train\n",
        "# to see what happens, but that's beyond the scope of this project.\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Fix the recompilation limit error\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.cache_size_limit = 256  # Increase from default 64\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.reset()\n",
        "\n",
        "# Also add this warning suppressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Full training\n",
        "training_args = GRPOConfig(\n",
        "    **COMMON_GRPO_TRAINING_PARAMS,\n",
        "    # Configure the maximum number of steps to take about 30mins of time for\n",
        "    # a medium-sized experiment. (See how long the previous example took and\n",
        "    # scale up appropriately using your best guess.)\n",
        "    # max_steps=**********,  # ~60min\n",
        "    max_steps = 100,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=REWARD_FUNCS,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "trainer_res = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available columns: dict_keys(['loss', 'grad_norm', 'learning_rate', 'num_tokens', 'completions/mean_length', 'completions/min_length', 'completions/max_length', 'completions/clipped_ratio', 'completions/mean_terminated_length', 'completions/min_terminated_length', 'completions/max_terminated_length', 'rewards/numbering_reward_func/mean', 'rewards/numbering_reward_func/std', 'rewards/spelling_reward_func/mean', 'rewards/spelling_reward_func/std', 'rewards/counting_reward_func/mean', 'rewards/counting_reward_func/std', 'rewards/format_reward_func/mean', 'rewards/format_reward_func/std', 'rewards/correct_answer_reward_func/mean', 'rewards/correct_answer_reward_func/std', 'reward', 'reward_std', 'frac_reward_zero_std', 'completion_length', 'kl', 'epoch', 'step'])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 153 ms (started: 2025-12-25 22:42:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show the total (sum) of the rewards as well as the correct_answer_reward_func (means with in the batch)\n",
        "# Do you see the rewards increasing? Does the model get the correct answer\n",
        "# more frequently toward the end?\n",
        "# No changes needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want to graph other columns, check these out\n",
        "print(f\"available columns: {trainer.state.log_history[0].keys()}\")\n",
        "\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "log_df[\"reward\"].plot()\n",
        "log_df[\"rewards/correct_answer_reward_func/mean\"].plot()\n",
        "\n",
        "# Show the legend\n",
        "plt.legend([\"reward\", \"rewards/correct_answer_reward_func/mean\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View the results\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('grpo_saved_lora/tokenizer_config.json',\n",
              " 'grpo_saved_lora/special_tokens_map.json',\n",
              " 'grpo_saved_lora/chat_template.jinja',\n",
              " 'grpo_saved_lora/vocab.json',\n",
              " 'grpo_saved_lora/merges.txt',\n",
              " 'grpo_saved_lora/added_tokens.json',\n",
              " 'grpo_saved_lora/tokenizer.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.93 s (started: 2025-12-25 22:42:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Save the LoRA adapters\n",
        "# No changes needed in this cell\n",
        "\n",
        "# Save the LoRA model\n",
        "model.save_pretrained(\"grpo_saved_lora\")\n",
        "tokenizer.save_pretrained(\"grpo_saved_lora\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.47 ms (started: 2025-12-25 22:42:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create a function to run both the original model and the updated model\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "\"\"\"def compare_old_and_new_model(messages):\n",
        "    from vllm import SamplingParams\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "    old = (\n",
        "        model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "        )[0]\n",
        "        .outputs[0]\n",
        "        .text\n",
        "    )\n",
        "\n",
        "    new = (\n",
        "        model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "            lora_request=model.load_lora(\"grpo_saved_lora\"),\n",
        "        )[0]\n",
        "        .outputs[0]\n",
        "        .text\n",
        "    )\n",
        "\n",
        "    print(\"===OLD===\\n\")\n",
        "    print(old)\n",
        "\n",
        "    print(\"\\n\\n===NEW===\\n\")\n",
        "    print(new)\"\"\"\n",
        "\n",
        "def compare_old_and_new_model(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate with OLD (base) model - adapters disabled\n",
        "    model.disable_adapter_layers()\n",
        "    outputs_old = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    old = tokenizer.decode(outputs_old[0], skip_special_tokens=True)\n",
        "\n",
        "    # Generate with NEW (fine-tuned) model - adapters enabled\n",
        "    model.enable_adapter_layers()\n",
        "    outputs_new = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    new = tokenizer.decode(outputs_new[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"===OLD===\\n\")\n",
        "    print(old)\n",
        "\n",
        "    print(\"\\n\\n===NEW===\\n\")\n",
        "    print(new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the old and new models on the letter-counting task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===OLD===\n",
            "\n",
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Counting the number of a's in the word idea\n",
            "1. i - 0 so far\n",
            "2. d - 1 so far\n",
            "3. e - 2 so far\n",
            "4. a - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "\n",
            "\n",
            "===NEW===\n",
            "\n",
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Let's count the number of letter 'a' in the word 'idea'.\n",
            "1. i - 0 so far \n",
            "2. d - 3 so far (i and d both contain no 'a')\n",
            "3. e - 4 so far (d contains no 'a' and this 'e' also contains no 'a')\n",
            "4. a - 6 so far (e contains no 'a' but this 'a' counts)\n",
            "5.  a - 7 so far (this 'a' counts, making it the second 'a')\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "time: 13.6 s (started: 2025-12-25 22:42:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's try spelling the first word from the dataset\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Load the first item from the dataset (index 0) and compare the old and new models\n",
        "# **********\n",
        "\"\"\"text = tokenizer.apply_chat_template(\n",
        "    ds[0][\"prompt\"], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== PROMPT ===\")\n",
        "print(text)\n",
        "print(\"\\n=== FINE-TUNED MODEL OUTPUT ===\")\n",
        "print(output)\"\"\"\n",
        "\n",
        "compare_old_and_new_model(ds[0][\"prompt\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model is better at spelling and counter letters in words! Depending on your reward functions, the size of your model, and the amount of steps trained, results may vary.\n",
        "\n",
        "For about an hour of training time, your model may not be perfect (or maybe it is), but it's definitely moving in the right direction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make sure the model did not forget basic facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===OLD===\n",
            "\n",
            "system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "user\n",
            "What is the capital of Spain?\n",
            "assistant\n",
            "The capital of Spain is Madrid.\n",
            "\n",
            "\n",
            "===NEW===\n",
            "\n",
            "system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "user\n",
            "What is the capital of Spain?\n",
            "assistant\n",
            "The capital of Spain is Madrid.\n",
            "time: 1.35 s (started: 2025-12-25 22:42:30 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's see if the model still remembers some of the facts from its original training\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Ask both the old and new models a question the model is likely to know,\n",
        "# e.g. a well-known capital city\n",
        "# **********\n",
        "question = \"What is the capital of Spain?\"\n",
        "\n",
        "\"\"\"text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": question}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== QUESTION ===\")\n",
        "print(question)\n",
        "print(\"\\n=== FINE-TUNED MODEL ANSWER ===\")\n",
        "print(output)\"\"\"\n",
        "\n",
        "compare_old_and_new_model([{\"role\": \"user\", \"content\": question}])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great job! Congrats on completing the project! ðŸŽ‰ðŸ¤—"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (venv2)",
      "language": "python",
      "name": "venv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
