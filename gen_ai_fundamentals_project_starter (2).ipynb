{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project: Teaching an LLM to Reason\n",
        "\n",
        "In this project, you will teach an LLM to use step-by-step reasoning to answer the question: \"How many X's are there in the word Y?\"\n",
        "\n",
        "Counting letters in a word is a surprisingly complex task for an LLM. Just as human beings would not be able to answer such a question for longer words without breaking down the word into its individual letters and then counting them, LLMs cannot be similarly expected to be able to respond without using smaller reasoning steps.\n",
        "\n",
        "For example, to count the number of o's in the word room, one could use the following reasoning:\n",
        "\n",
        "```\n",
        "Question: How many of the letter \"o\" are there in the word \"room\"\n",
        "Answer: 2\n",
        "Response:\n",
        "\n",
        "<reasoning>\n",
        "Letter-by-letter spelling:\n",
        "1. r - 0 o's so far\n",
        "2. o - 1 o's so far\n",
        "3. o - 2 o's so far\n",
        "4. m - 2 o's so far\n",
        "\n",
        "The letter \"o\" appears 2 times in the word \"room\".\n",
        "</reasoning>\n",
        "<answer>\n",
        "2\n",
        "</answer>\n",
        "```\n",
        "\n",
        "In this project we will use the reinforcement learning method GRPO (Group Relative Policy Optimization, of DeepSeek fame) to take a large language model that has been fine-tuned for following instructions and teach it how to break a word down into its letters and then count the requested letter.\n",
        "\n",
        "We will complete the following steps:\n",
        "\n",
        "* Set up the notebook\n",
        "* Create a letter-counting dataset\n",
        "* Create the reward functions\n",
        "* Train the model\n",
        "* View the results\n",
        "\n",
        "NOTE: This notebook will have you focus on several important aspects of training a GPRO model using LoRA:\n",
        "\n",
        "1. Configuring LoRA adapters for parameter-efficient fine tuning\n",
        "2. Selecting reward functions that help the model efficiently find its way to the correct answer (also called reward shaping)\n",
        "3. Finding hyperparameters that help the model increase the rewards earned more quickly and reliably\n",
        "4. Learning how to start with smaller experiments and to work your way up to longer experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the notebook\n",
        "\n",
        "We'll install dependencies needed for the project, namely `unsloth` and `vllm`, which are useful for fine-tuning LLMs with even just 15GB of VRAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 207 Î¼s (started: 2025-12-24 18:48:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Load ipython-autotime to see how long each cell take to run\n",
        "# No changes needed in this cell\n",
        "\n",
        "!pip install -q ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Dec 24 18:48:45 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
            "| N/A   25C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "time: 347 ms (started: 2025-12-24 18:48:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Verify we have enough GPU memory to run this project (at least 15360MiB)\n",
        "# No changes needed in this cell\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMMENTS:\n",
        "\n",
        "#### lora_rank = 64 was chosen because it is a good middle ground choice between capacity and efficiency. Also becaue we had memory constrains.\n",
        "#### fast_interface=False.  I try multiple choices and was not able to make this run unless I have it as False.\n",
        "#### target_modules. Our task will requiere the different roles, the Attention layers and MLP layers. That is, we are giving the model the flexibility to learn what to attend to and how to process and format it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/voc/data/venv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 12-24 18:49:12 [__init__.py:241] Automatically detected platform cuda.\n",
            "ERROR 12-24 18:49:15 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.7: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.9.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 59.5 s (started: 2025-12-24 18:48:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Load the `Qwen 2.5 3B Instruct`, and set parameters for the project\n",
        "# The first time unsloth is imported, it will do its magic and patch the modules\n",
        "# it works with. This may 2-5 minutes.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "import unsloth\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 256  # Increase if you get errors about the sequence length\n",
        "\n",
        "# Set the LoRA rank to an appropriate value\n",
        "# Read about setting LoRA rank:\n",
        "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "# lora_rank = **********  # Explain your choice\n",
        "lora_rank = 64\n",
        "\n",
        "# Load the Instruct model in 4-bit mode\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,  # We'll use quantization!\n",
        "    fast_inference=False,  # Changed from True to False\n",
        "    max_lora_rank=lora_rank,\n",
        "    gpu_memory_utilization=0.75,  # You can reduce this if you get an memory error\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        # Read about choosing adapters for LoRA:\n",
        "        # https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "        # Choose the target modules/adapters for your LoRA model\n",
        "        # ********** # Explain your choice\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        # **********\n",
        "        'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth enables longer contexts\n",
        "    # See: https://github.com/unslothai/unsloth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try Prompt Engineering to Count Letters\n",
        "\n",
        "Let's work on the system prompt a little to see if we can get the model to count the number of the letter `g` in `engage`.\n",
        "\n",
        "\n",
        "Here you must:\n",
        "* Write clear instructions\n",
        "* Break the problem down into steps (Chain-of-Thought prompting)\n",
        "* Provide at least one example for the model to follow (Few-shot prompting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TEXT FOR COMPLETION ===\n",
            "<|im_start|>system\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "How many of the letter \"g\" are there in the word \"engage\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "=== GENERATED OUTPUT ===\n",
            "system\n",
            "\n",
            "user\n",
            "How many of the letter \"g\" are there in the word \"engage\"\n",
            "assistant\n",
            "In the word \"engage\", there is only one letter \"g\".\n",
            "time: 3.46 s (started: 2025-12-24 18:49:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# First, let's see what happens when we have a blank system prompt\n",
        "# No changes needed in this cell\n",
        "SYSTEM_PROMPT = \"\"\"\"\"\"\n",
        "USER_PROMPT = 'How many of the letter \"g\" are there in the word \"engage\"'\n",
        "\n",
        "# Convert the chat messages to a single string so the model can complete it\n",
        "text_for_completion = tokenizer.apply_chat_template(\n",
        "    conversation=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        },\n",
        "    ],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text_for_completion], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate the text completion using standard generate (not fast_generate)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the text input for the model and the model's output\n",
        "print(\"=== TEXT FOR COMPLETION ===\")\n",
        "print(text_for_completion)\n",
        "print(\"=== GENERATED OUTPUT ===\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without any prompting the model will generate an output such as this:\n",
        "\n",
        "```\n",
        "=== GENERATED OUTPUT ===\n",
        "There is one letter \"g\" in the word \"engage\".\n",
        "```\n",
        "\n",
        "Now let's work on the system prompt to help the model break this problem down into steps, which might help it get the right answer (2 `g`'s in `engage`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TEXT FOR COMPLETION ===\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that counts letters in words step-by-step.\n",
            "\n",
            "To count letters, you should:\n",
            "1. Break the word down letter by letter\n",
            "2. Count how many times the target letter appears\n",
            "3. Provide your reasoning in <reasoning>...</reasoning> tags\n",
            "4. Provide your final answer in <answer>...</answer> tags\n",
            "\n",
            "Example:\n",
            "Question: How many of the letter \"o\" are there in the word \"room\"\n",
            "Answer: 2\n",
            "\n",
            "Response:\n",
            "<reasoning>\n",
            "Letter-by-letter spelling:\n",
            "1. r - 0 o's so far\n",
            "2. o - 1 o's so far\n",
            "3. o - 2 o's so far\n",
            "4. m - 2 o's so far\n",
            "\n",
            "The letter \"o\" appears 2 times in the word \"room\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "How many of the letter \"g\" are there in the word \"engage\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "=== GENERATED OUTPUT ===\n",
            "system\n",
            "You are a helpful assistant that counts letters in words step-by-step.\n",
            "\n",
            "To count letters, you should:\n",
            "1. Break the word down letter by letter\n",
            "2. Count how many times the target letter appears\n",
            "3. Provide your reasoning in <reasoning>...</reasoning> tags\n",
            "4. Provide your final answer in <answer>...</answer> tags\n",
            "\n",
            "Example:\n",
            "Question: How many of the letter \"o\" are there in the word \"room\"\n",
            "Answer: 2\n",
            "\n",
            "Response:\n",
            "<reasoning>\n",
            "Letter-by-letter spelling:\n",
            "1. r - 0 o's so far\n",
            "2. o - 1 o's so far\n",
            "3. o - 2 o's so far\n",
            "4. m - 2 o's so far\n",
            "\n",
            "The letter \"o\" appears 2 times in the word \"room\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"g\" are there in the word \"engage\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Letter-by-letter spelling:\n",
            "1. e - 0 g's so far\n",
            "2. a - 1 g' (third letter) so far\n",
            "3. n - 1 g' (fourth letter) so far\n",
            "4. g - 2 g's so far (fifth letter)\n",
            "5. e - 2 g's so far\n",
            "6. n - 3 g's so far\n",
            "7. g - 4 g's so far\n",
            "\n",
            "The letter \"g\" appears 4 times in the word \"engage\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "time: 8.51 s (started: 2025-12-24 18:49:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's work on a new system prompt that will help the model break this problem\n",
        "# down into steps, for example, using \"letter-by-letter\" spelling.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "# Use a CoT prompt with at least one example\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that counts letters in words step-by-step.\n",
        "\n",
        "To count letters, you should:\n",
        "1. Break the word down letter by letter\n",
        "2. Count how many times the target letter appears\n",
        "3. Provide your reasoning in <reasoning>...</reasoning> tags\n",
        "4. Provide your final answer in <answer>...</answer> tags\n",
        "\n",
        "Example:\n",
        "Question: How many of the letter \"o\" are there in the word \"room\"\n",
        "Answer: 2\n",
        "\n",
        "Response:\n",
        "<reasoning>\n",
        "Letter-by-letter spelling:\n",
        "1. r - 0 o's so far\n",
        "2. o - 1 o's so far\n",
        "3. o - 2 o's so far\n",
        "4. m - 2 o's so far\n",
        "\n",
        "The letter \"o\" appears 2 times in the word \"room\".\n",
        "</reasoning>\n",
        "<answer>\n",
        "2\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT = 'How many of the letter \"g\" are there in the word \"engage\"'\n",
        "\n",
        "# Convert the chat messages to a single string so the model can complete it\n",
        "text_for_completion = tokenizer.apply_chat_template(\n",
        "    conversation=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        },\n",
        "    ],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text_for_completion], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate the text completion using standard generate (not fast_generate)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Print the text input for the model and the model's output\n",
        "print(\"=== TEXT FOR COMPLETION ===\")\n",
        "print(text_for_completion)\n",
        "print(\"=== GENERATED OUTPUT ===\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did your new prompt get the right answer? Did the model follow all of your instructions?\n",
        "\n",
        "Maybe yes, maybe no. Either way, we'll want the model to reliably complete this challenge. So let's use GRPO to help it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a letter-counting dataset\n",
        "\n",
        "To train a model, we'll first need to create a dataset. We'll use the HuggingFace `datasets` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['idea',\n",
              " 'glow',\n",
              " 'rust',\n",
              " 'maze',\n",
              " 'echo',\n",
              " 'wisp',\n",
              " 'veto',\n",
              " 'lush',\n",
              " 'gaze',\n",
              " 'knit']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 5.89 ms (started: 2025-12-24 18:49:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create a list of words of different lengths\n",
        "# No changes are needed in this cell.\n",
        "\n",
        "ALL_WORDS = [\n",
        "    \"idea\",\n",
        "    \"glow\",\n",
        "    \"rust\",\n",
        "    \"maze\",\n",
        "    \"echo\",\n",
        "    \"wisp\",\n",
        "    \"veto\",\n",
        "    \"lush\",\n",
        "    \"gaze\",\n",
        "    \"knit\",\n",
        "    \"fume\",\n",
        "    \"plow\",\n",
        "    \"void\",\n",
        "    \"oath\",\n",
        "    \"grim\",\n",
        "    \"crisp\",\n",
        "    \"lunar\",\n",
        "    \"fable\",\n",
        "    \"quest\",\n",
        "    \"verge\",\n",
        "    \"brawn\",\n",
        "    \"elude\",\n",
        "    \"aisle\",\n",
        "    \"ember\",\n",
        "    \"crave\",\n",
        "    \"ivory\",\n",
        "    \"mirth\",\n",
        "    \"knack\",\n",
        "    \"wryly\",\n",
        "    \"onset\",\n",
        "    \"mosaic\",\n",
        "    \"velvet\",\n",
        "    \"sphinx\",\n",
        "    \"radius\",\n",
        "    \"summit\",\n",
        "    \"banner\",\n",
        "    \"cipher\",\n",
        "    \"glisten\",\n",
        "    \"mantle\",\n",
        "    \"scarab\",\n",
        "    \"expose\",\n",
        "    \"fathom\",\n",
        "    \"tavern\",\n",
        "    \"fusion\",\n",
        "    \"relish\",\n",
        "    \"lantern\",\n",
        "    \"enchant\",\n",
        "    \"torrent\",\n",
        "    \"capture\",\n",
        "    \"orchard\",\n",
        "    \"eclipse\",\n",
        "    \"frescos\",\n",
        "    \"triumph\",\n",
        "    \"absolve\",\n",
        "    \"gossipy\",\n",
        "    \"prelude\",\n",
        "    \"whistle\",\n",
        "    \"resolve\",\n",
        "    \"zealous\",\n",
        "    \"mirage\",\n",
        "    \"aperture\",\n",
        "    \"sapphire\",\n",
        "]\n",
        "\n",
        "print(len(ALL_WORDS))\n",
        "\n",
        "ALL_WORDS[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'words': 'idea', 'letters': 'a', 'counts': 1}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 199 ms (started: 2025-12-24 18:49:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset as a Hugging Face Dataset using Dataset.from_generator\n",
        "# No changes needed in this cell\n",
        "\n",
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "\n",
        "# Go through the letters from the words (as well as letters not in the words),\n",
        "# and create a labelled dataset with all the different combinations.\n",
        "# For example for the word gaze:\n",
        "# 1. How many i's are in idea? <-- count should be 1\n",
        "# 2. How many d's are in idea? <-- count should be 1\n",
        "# 3. How many e's are in idea? <-- count should be 1\n",
        "# 4. How many a's are in idea? <-- count should be 1\n",
        "# 5. How many b's are in idea? <-- a letter not in word (count should be zero)\n",
        "def generate_records():\n",
        "    for word in ALL_WORDS:\n",
        "        for letter in sorted(set(word)):\n",
        "            yield {\"words\": word, \"letters\": letter, \"counts\": word.count(letter)}\n",
        "\n",
        "        # pick random letters not in the word\n",
        "        num_letters_not_in_word_left = int(len(word) // 7 + 1)\n",
        "\n",
        "        random.seed(hash(word))\n",
        "\n",
        "        all_letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "        random.shuffle(all_letters)\n",
        "        for letter in all_letters:\n",
        "            if letter not in word:\n",
        "                yield {\"words\": word, \"letters\": letter, \"counts\": 0}\n",
        "                num_letters_not_in_word_left -= 1\n",
        "            if num_letters_not_in_word_left == 0:\n",
        "                break\n",
        "\n",
        "\n",
        "ds = Dataset.from_generator(generate_records)\n",
        "\n",
        "# Show the first item\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'words': 'idea',\n",
              " 'letters': 'a',\n",
              " 'counts': 1,\n",
              " 'prompt': [{'content': \"\\nRespond in the following format:\\n<reasoning>\\nCounting the number of [letter_to_count]'s in the word [word]\\n1. [first letter] - [count of requested letter so far] so far\\n2. [second letter] - [count of requested letter so far] so far\\n...\\n</reasoning>\\n<answer>\\n[number]\\n</answer>\\n\",\n",
              "   'role': 'system'},\n",
              "  {'content': 'How many of the letter \"a\" are there in the word \"idea\"',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 13.3 ms (started: 2025-12-24 18:49:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Add the entire prompt (system + user) and the answer to the dataset\n",
        "# We'll use a prompt that spells out the word letter-by-letter\n",
        "# No changes needed in this cell\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Simple CoT prompt (zero-shot)\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "Counting the number of [letter_to_count]'s in the word [word]\n",
        "1. [first letter] - [count of requested letter so far] so far\n",
        "2. [second letter] - [count of requested letter so far] so far\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "[number]\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "ds = ds.map(\n",
        "    lambda x: {  # type: ignore\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": 'How many of the letter \"{}\" are there in the word \"{}\"'.format(\n",
        "                    x[\"letters\"], x[\"words\"]\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Counting the number of a's in the word idea\n",
            "1. i - 0 so far\n",
            "2. d - 1 so far\n",
            "3. e - 2 so far\n",
            "4. a - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "time: 4.52 s (started: 2025-12-24 18:49:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's see how well the model runs out-of-the-box\n",
        "# No changes needed in this cell\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    ds[0][\"prompt\"], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using standard generate method\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Reward Functions\n",
        "\n",
        "One goal of creating reward functions is to guide the model toward behaviors that help it reach its goal (counting the occurrences of a letter within a word) more easily. Since there is more than one way to carry out any step-by-step task (e.g. whether or not you use bullet points to separate your steps), there's a bit of judgement involved in choosing what behaviors to reward, i.e. how do we provide partial credit or \"shape\" our rewards?\n",
        "\n",
        "In this case we will encourage the model to (whether or not this structure is best):\n",
        "* use numbers for bullet points when spelling out the word\n",
        "* to spell the word correctly\n",
        "* to count the requested letter correctly\n",
        "* to use the requested reasoning format\n",
        "* to get the final answer correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numbering reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.0, 0.5]\n",
            "time: 2.97 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's work on a function that the numbering in the bullet points is correct\n",
        "# When using GRPO, we lean on reward functions that are relatively easy to\n",
        "# compute, thus removing the need to have a second large model just for\n",
        "# evaluation.\n",
        "# In this case, we'll use regular expressions quite a bit.\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_letter_numbering(response):\n",
        "    \"\"\"Extract the numbers at the beginning of the line\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    4. a - 2 so far\n",
        "    5. l - 2 so far\n",
        "    returns [1, 2, 3, 4, 5]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # We use a regular expression to find lines of the form:\n",
        "    # '\\n[number]. [letter]'\n",
        "    pattern = r\"\\n(\\d+). [a-z]\"\n",
        "\n",
        "    # Use `re` to find all matches of the pattern in the response\n",
        "    # matches = **********\n",
        "    matches = re.findall(pattern, response)\n",
        "    if matches:\n",
        "        return [int(m) for m in matches]\n",
        "    return []\n",
        "\n",
        "\n",
        "assert extract_letter_numbering(\n",
        "    \"\"\"\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. a - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == [1, 2, 3, 4, 5]\n",
        "\n",
        "\n",
        "def numbering_reward_func(completions, words, **kwargs) -> list[float]:\n",
        "    \"\"\"Provides a reward for getting the numbering at the beginning of the line correct\n",
        "\n",
        "    1. g - 1 so far <-- Good in-order numbering\n",
        "    2. o - 1 so far <-- Good in-order numbering\n",
        "    3. a - 2 so far <-- Good in-order numbering\n",
        "    3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "    1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "    1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "\n",
        "    \"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "    for response, word in zip(responses, words):\n",
        "        reward = 0\n",
        "\n",
        "        for ix, spell_number in enumerate(extract_letter_numbering(response)):\n",
        "            line_number = ix + 1\n",
        "\n",
        "            # Get points for in-order numbering\n",
        "            if spell_number == line_number:\n",
        "                # TODO: Provide a reward for in-order numbering\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward += ***********\n",
        "                reward += 1\n",
        "            # Otherwise lose points\n",
        "            else:\n",
        "                # TODO: Provide a reward for out-of-order numbering\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward -= ***********\n",
        "                reward -= 1\n",
        "\n",
        "            # Lose extra points for continuing beyond the length of the word\n",
        "            if line_number > len(word):  # We use the index of the line\n",
        "                # TODO: Provide a reward for continuing beyond the length of the word\n",
        "                # (positive for good behavior, negative for bad)\n",
        "                # reward -= ***********\n",
        "                reward -= 2\n",
        "\n",
        "        res.append(reward / len(word))\n",
        "    return res\n",
        "\n",
        "\n",
        "res = numbering_reward_func(\n",
        "    completions=[\n",
        "        [\n",
        "            {  # Worse response\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far <-- Good in-order numbering\n",
        "2. o - 1 so far <-- Good in-order numbering\n",
        "3. a - 2 so far <-- Good in-order numbering\n",
        "3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "1. l - 2 so far <-- Bad numbering, extra letter and out-of-order\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {  # Better response\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far <-- Good in-order numbering\n",
        "2. o - 1 so far <-- Good in-order numbering\n",
        "3. a - 2 so far <-- Good in-order numbering\n",
        "3. l - 2 so far <-- Bad numbering, out-of-order, 3 should be 4\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            },\n",
        "        ],\n",
        "    ],\n",
        "    words=[\"goal\", \"goal\"],\n",
        ")\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spelling reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-234.0, -210.0]\n",
            "time: 54.9 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward correct spelling of the word\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_spelling(response):\n",
        "    \"\"\"Extract the spelling from the response\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    3. l - 2 so far\n",
        "    5. l - 2 so far\n",
        "    Returns \"goall\"\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"\\n\\d+\\. ([a-z])\"\n",
        "    matches = re.findall(pattern, response, flags=re.IGNORECASE)\n",
        "    if matches:\n",
        "        return \"\".join([m for m in matches])\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "extract_spelling(\n",
        "    \"\"\"Here is a letter by letter spelling:\n",
        "\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "3. l - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == \"goall\"\n",
        "\n",
        "\n",
        "def spelling_reward_func(completions, words, **kwargs) -> list[float]:\n",
        "    \"\"\"A spelling reward function.\"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for word, response in zip(words, responses):\n",
        "        reward = 0.0\n",
        "\n",
        "        # Provide a reward for exactly correct spelling\n",
        "        # reward += **********\n",
        "        if response.strip().lower() == word.lower():\n",
        "            reward += 10  # Large bonus for perfect spelling\n",
        "\n",
        "        # Provide a reward for each letter of difference in length\n",
        "        # reward -= **********\n",
        "        length_diff = abs(len(response.strip()) - len(word))\n",
        "        reward -= length_diff * 0.5  # Penalty per character difference\n",
        "\n",
        "\n",
        "        # Provide a reward for each letter that is not in the target word\n",
        "        # reward -= **********\n",
        "        word_counter = Counter(word.lower())\n",
        "        response_counter = Counter(response.strip().lower())\n",
        "        extra_letters = sum((response_counter - word_counter).values())\n",
        "        reward -= extra_letters * 1  # Penalty for extra letters\n",
        "\n",
        "        # Provide a reward for each letter that is in the target word but not in the response\n",
        "        # reward -= **********\n",
        "        missing_letters = sum((word_counter - response_counter).values())\n",
        "        reward -= missing_letters * 1  # Penalty for missing letters\n",
        "\n",
        "        res.append(reward)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = spelling_reward_func(\n",
        "    completions=[\n",
        "        [  # Worse response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. l - 2 so far\n",
        "5. l - 2 so far\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        [  # Better Response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\n",
        "Here is a letter by letter spelling:\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. l - 2 so far\n",
        "</reasoning>\n",
        "<answer>2</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    words=[\"goal\", \"goal\"],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.2, 0.2]\n",
            "time: 53.4 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's reward the model for properly counting the occurrences of a letter in a word\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "# No changes needed in this cell, but feel free to experiment with variations on the prompt\n",
        "\n",
        "\n",
        "def get_resp_letters_and_counts(response):\n",
        "    \"\"\"Extract the letters and counts from the response\n",
        "\n",
        "    Example:\n",
        "    1. g - 1 so far\n",
        "    2. o - 1 so far\n",
        "    3. a - 2 so far\n",
        "    4. a - 2 so far\n",
        "    5. l - 2 so far\n",
        "    returns [('g', 1), ('o', 1), ('a', 2), ('a', 2), ('l', 2)]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"\\n(\\d+)\\. ([a-z])\\D*(\\d+)\"\n",
        "\n",
        "    # Find strings matching e.g. \"2. a - 2 so far\"\n",
        "    matches = re.findall(pattern, response, flags=re.IGNORECASE)\n",
        "\n",
        "    if not matches:\n",
        "        return []\n",
        "\n",
        "    return [\n",
        "        (matched_letter, matched_count_so_far)\n",
        "        for _, matched_letter, matched_count_so_far in matches\n",
        "    ]\n",
        "\n",
        "\n",
        "assert get_resp_letters_and_counts(\n",
        "    \"\"\"\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 2 so far\n",
        "4. a - 2 so far\n",
        "5. l - 2 so far\n",
        "\"\"\"\n",
        ") == [(\"g\", \"1\"), (\"o\", \"1\"), (\"a\", \"2\"), (\"a\", \"2\"), (\"l\", \"2\")]\n",
        "\n",
        "\n",
        "def counting_reward_func(completions, letters, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    # Iterate over each of the letter-response pairs\n",
        "    for letter, response in zip(letters, responses):\n",
        "        reward = 0\n",
        "\n",
        "        letters_and_counts = get_resp_letters_and_counts(response)\n",
        "\n",
        "        # If there are no matches, provide a negative reward\n",
        "        if not letters_and_counts:\n",
        "            res.append(-1)\n",
        "            continue\n",
        "\n",
        "        # Start counting the matching letters\n",
        "        actual_count = 0\n",
        "        for resp_letter, resp_count in letters_and_counts:\n",
        "            # If there's a match, count the letter\n",
        "            if letter == resp_letter:\n",
        "                actual_count += 1\n",
        "\n",
        "            # If the count is accurate, add a reward, else subtract a reward\n",
        "            # if ... **********\n",
        "            # else ... **********\n",
        "                if int(resp_count) == actual_count:\n",
        "                    reward += 1\n",
        "                else:\n",
        "                    reward -= 1\n",
        "\n",
        "        # Return the reward normalized by the length of the matches\n",
        "        # res.append(**********)\n",
        "        res.append(reward / len(letters_and_counts))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "res = counting_reward_func(\n",
        "    completions=[\n",
        "        [  # Worse response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\\nHere is a letter by letter spelling:\n",
        "\n",
        "1. g - 0 so far\n",
        "2. o - 0 so far\n",
        "3. a - 1 so far\n",
        "4. a - 2 so far\n",
        "5. l - 0 so far\n",
        "\n",
        "\\n</reasoning>\\n<answer>\\nThis is my answer.\\n</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        [  # Better response\n",
        "            {\n",
        "                \"content\": \"\"\"<reasoning>\\nHere is a letter by letter spelling:\n",
        "\n",
        "1. g - 1 so far\n",
        "2. o - 1 so far\n",
        "3. a - 1 so far\n",
        "4. a - 1 so far\n",
        "5. l - 1 so far\n",
        "\n",
        "\\n</reasoning>\\n<answer>\\nThis is my answer.\\n</answer>\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    letters=[\"g\", \"g\"],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Formatting reward functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-10.0, 20.0]\n",
            "time: 50.7 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward the model for providing the response in a specific format\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"Extracts the string between <answer> and </answer> tags.\"\"\"\n",
        "    import re\n",
        "\n",
        "    pattern = r\"<answer>(.*?)</answer>\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    \n",
        "    return \"\"\n",
        "\n",
        "assert (\n",
        "    extract_xml_answer(\"\"\"\n",
        "<reasoning>\n",
        "This is my reasoning.\n",
        "</reasoning>\n",
        "<answer>SUPERCALIFRAGILISTICEXPIALIDOCIOUS</answer>\n",
        "\"\"\")\n",
        "    == \"SUPERCALIFRAGILISTICEXPIALIDOCIOUS\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"\\s*<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for completion in completions:\n",
        "        reward = 0.0\n",
        "\n",
        "        # Extract the response content\n",
        "        response = completion[0][\"content\"]\n",
        "\n",
        "        # Check if the response matches the pattern\n",
        "        match = re.match(pattern, response, flags=re.MULTILINE | re.DOTALL)\n",
        "\n",
        "        # If it matches, return 0.5, otherwise return 0.0\n",
        "        # if ... **********\n",
        "        if match:\n",
        "            reward += 15.0\n",
        "        # Extract the answer from the response\n",
        "        # extracted_answer = **********\n",
        "            extracted_answer = extract_xml_answer(response)\n",
        "        # If the answer is an integer, add 0.5 to the reward\n",
        "        # if ... **********\n",
        "            try:\n",
        "                int(extracted_answer)\n",
        "                reward += 5.0\n",
        "            except ValueError:\n",
        "                reward -= 3.0\n",
        "        else:\n",
        "            reward = -10.0\n",
        "\n",
        "        res.append(reward)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = format_reward_func(\n",
        "    completions=[\n",
        "        [{\"content\": \"This is my answer\"}],\n",
        "        [\n",
        "            {\n",
        "                \"content\": \"<reasoning>\\nThis is my reasoning.\\n</reasoning>\\n<answer>\\n3\\n</answer>\"\n",
        "            }\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task correctness reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many...\n",
            "Answer: 0\n",
            "Response: <reasoning>.../reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "Extracted: 3\n",
            "Correct: False!\n",
            "    \n",
            "[-1.0, 2.0]\n",
            "time: 55.2 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reward the model for providing the correct answer\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "\n",
        "def correct_answer_reward_func(prompts, completions, counts, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward the final answer if it is correct.\"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    # Print a nice summary of the first prompt, answer, and response to see while training\n",
        "    print(f\"\"\"\n",
        "{\"-\" * 20}\n",
        "Question: {prompts[0][-1][\"content\"]}\n",
        "Answer: {counts[0]}\n",
        "Response: {responses[0]}\n",
        "Extracted: {extracted_responses[0]}\n",
        "Correct: {str(extracted_responses[0]) == str(counts[0])}!\n",
        "    \"\"\")\n",
        "\n",
        "    res = [\n",
        "        # Provide reward for exactly correct answer\n",
        "        # **********  # Complete the list comprehension\n",
        "        2.0 if str(r) == str(a) else -1.0\n",
        "        for r, a in zip(extracted_responses, counts)\n",
        "    ]\n",
        "    return res\n",
        "\n",
        "\n",
        "res = correct_answer_reward_func(\n",
        "    prompts=[\n",
        "        [{\"content\": \"\"\"How many...\"\"\"}],\n",
        "        [{\"content\": \"\"\"How many...\"\"\"}],\n",
        "    ],\n",
        "    completions=[\n",
        "        [{\"content\": \"\"\"<reasoning>.../reasoning>\\n<answer>\\n3\\n</answer>\"\"\"}],\n",
        "        [{\"content\": \"\"\"<reasoning>.../reasoning>\\n<answer>\\n3\\n</answer>\"\"\"}],\n",
        "    ],\n",
        "    letters=[\"g\", \"g\"],\n",
        "    counts=[0, 3],\n",
        ")\n",
        "\n",
        "print(res)\n",
        "\n",
        "assert res[1] > res[0], \"The better response should have a higher reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List the reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 47.6 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# List out the reward functions we will use\n",
        "# No changes needed in this cell\n",
        "\n",
        "REWARD_FUNCS = [\n",
        "    numbering_reward_func,\n",
        "    spelling_reward_func,\n",
        "    counting_reward_func,\n",
        "    format_reward_func,\n",
        "    correct_answer_reward_func,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "Now set up GRPO Trainer and configurations!\n",
        "\n",
        "As you run the trainer, the goal is to see the various `reward` columns increase.\n",
        "\n",
        "After 50 steps or more, you may notice some of the reward standard deviations begin to decrease, meaning that the different predictions are starting to converge on solutions that give similar rewards. If your model has learned the task, then you'll see the `correct_answer_reward_function` increase to its highest value (check the function to see what that is).\n",
        "\n",
        "Here is an example, which successfully converged on a higher reward. Note, the values you see here will probably be different from yours, especially if your reward amounts are different.\n",
        "\n",
        "| Step | Training Loss | reward   | reward_std | ... | kl      | rewards / correct_answer_reward_function / mean | rewards / correct_answer_reward_function / std |\n",
        "|------|---------------|----------|------------|-----|---------|------------------------------------------|-----------------------------------------|\n",
        "| 1    | 0.000000      | 7.961805 | 2.368493   | ... | 0.020369| 0.875000                                 | 1.024695                                |\n",
        "| 2    | 0.000000      | 7.937500 | 1.352467   | ... | 0.016483| 0.875000                                 | 1.024695                                |\n",
        "| 3    | 0.000000      | 1.894792 | 6.462189   | ... | 0.013677| 0.375000                                 | 0.806226                                |\n",
        "| ...  | ...           | ...      | ...        | ... | ...     | ...                                      | ...                                     |\n",
        "| 398  | 0.000100      | 13.000000| 0.000000   | ... | 0.088529| 2.000000                                 | 0.000000                                |\n",
        "| 399  | 0.000100      | 13.000000| 0.000000   | ... | 0.088617| 2.000000                                 | 0.000000                                |\n",
        "| 400  | 0.000100      | 13.000000| 0.000000   | ... | 0.096202| 2.000000                                 | 0.000000                                |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 53.9 ms (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Fill in the GRPO Parameters we'll use throughout this project\n",
        "# TODO: Fill in the missing parts marked with **********\n",
        "\n",
        "# Read about the GRPO params here https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "COMMON_GRPO_TRAINING_PARAMS = dict(\n",
        "    # Set appropriate values for `learning_rate` and `beta`\n",
        "    # See: https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "    # See: https://huggingface.co/docs/trl/main/en/grpo_trainer\n",
        "    learning_rate=5e-5,  # Conservative learning rate for stable training\n",
        "    beta=0.1,  # KL divergence penalty weight (typical range: 0.01-0.5)\n",
        "    # Set the batch size appropriately for your hardware. For GRPO there are a number of parameters to set.\n",
        "    # If you are not sure about your GPU, assume you have a T4. See the memory specs here:\n",
        "    # https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/T4%20Product%20Brief.pdf\n",
        "    per_device_train_batch_size=4,  # per_device_train_batch_size / num_generations determines the number of simultaneous prompts to consider.\n",
        "    # Note: Set per_device_train_batch_size to at most 16 on the Vocareum T4 for best stability\n",
        "    num_generations=4,  # Determines the number of completions/generations to compute for each single prompt\n",
        "    gradient_accumulation_steps=4,  # This parameter allow us to consider multiple steps in a single optimization step\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,  # Set to 1 for a full training run\n",
        "    save_steps=250,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"none\",  # Setting this value lets us use Weights and Biases\n",
        "    output_dir=\"outputs\",\n",
        "    use_vllm=False,  # vll speeds up inference! See https://github.com/vllm-project/vllm\n",
        "    torch_compile=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick train\n",
        "\n",
        "Let's train the model for just 5 steps (`max_steps=5`). As it runs we can double check we've set up our prompts correctly before running for a longer amount of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 1 | Total steps = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n",
            "Unsloth: Input IDs of shape torch.Size([16, 262]) with length 262 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 262]) with length 262 > the model's max sequence length of 256.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"glisten\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word glisten\n",
            "1. g - 1 so far\n",
            "2. l - 1 so far\n",
            "3. i - 1 so far\n",
            "4. s - 1 so far\n",
            "5. t - 1 so far\n",
            "6. e - 1 so far\n",
            "7. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 01:47, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / numbering_reward_func / mean</th>\n",
              "      <th>rewards / numbering_reward_func / std</th>\n",
              "      <th>rewards / spelling_reward_func / mean</th>\n",
              "      <th>rewards / spelling_reward_func / std</th>\n",
              "      <th>rewards / counting_reward_func / mean</th>\n",
              "      <th>rewards / counting_reward_func / std</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / correct_answer_reward_func / mean</th>\n",
              "      <th>rewards / correct_answer_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-311.244629</td>\n",
              "      <td>92.521286</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.756696</td>\n",
              "      <td>0.266554</td>\n",
              "      <td>-333.656250</td>\n",
              "      <td>141.137131</td>\n",
              "      <td>0.217411</td>\n",
              "      <td>0.324859</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>1.209339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-243.907990</td>\n",
              "      <td>22.320728</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>-265.406250</td>\n",
              "      <td>59.312233</td>\n",
              "      <td>0.123264</td>\n",
              "      <td>0.164670</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-251.954468</td>\n",
              "      <td>30.292261</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.891369</td>\n",
              "      <td>0.213485</td>\n",
              "      <td>-273.375000</td>\n",
              "      <td>54.992275</td>\n",
              "      <td>0.216667</td>\n",
              "      <td>0.263459</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-244.930557</td>\n",
              "      <td>24.602139</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.136083</td>\n",
              "      <td>-265.500000</td>\n",
              "      <td>28.481573</td>\n",
              "      <td>-0.097222</td>\n",
              "      <td>0.162161</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-232.445618</td>\n",
              "      <td>42.672146</td>\n",
              "      <td>77.375000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.375000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.874256</td>\n",
              "      <td>0.220756</td>\n",
              "      <td>-254.281250</td>\n",
              "      <td>59.116543</td>\n",
              "      <td>0.086384</td>\n",
              "      <td>0.306983</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word sapphire\n",
            "1. s - 0 so far\n",
            "2. a - 1 so far\n",
            "3. p - 1 so far\n",
            "4. a - 2 so far\n",
            "5. r - 2 so far\n",
            "6. p - 3 so far\n",
            "7. h - 3 so far\n",
            "8. a - 4 so far\n",
            "9. y - 4 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "Extracted: 4\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"absolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - 0 so far\n",
            "3. s - 1 so far\n",
            "4. p - 0 so far\n",
            "5. o - 1 so far\n",
            "6. l - 1 so far\n",
            "7. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word \"mirage\"\n",
            "1. m - 0 so far\n",
            "2. i - 0 so far\n",
            "3. r - 0 so far\n",
            "4. a - 0 so far\n",
            "5. g - 1 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word crave\n",
            "1. c - 0 so far\n",
            "2. r - 1 so far\n",
            "3. a - 1 so far\n",
            "4. v - 1 so far\n",
            "5. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "time: 2min 30s (started: 2025-12-24 18:50:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Train for just a few steps for a few minutes\n",
        "# This will allow us to observe the results and make any changes to our reward functions\n",
        "# before starting a longer run. Note, you won't see much change in the average.\n",
        "# reward values\n",
        "# No changes are needed here\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# Short train to check on reward functions\n",
        "training_args = GRPOConfig(\n",
        "    **COMMON_GRPO_TRAINING_PARAMS,\n",
        "    # We'll just run for a modest 5 steps to make sure everything works and to\n",
        "    # estimate the amount of time it will take to run the full training.\n",
        "    max_steps=5,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=REWARD_FUNCS,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "trainer_res = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available columns: dict_keys(['loss', 'grad_norm', 'learning_rate', 'num_tokens', 'completions/mean_length', 'completions/min_length', 'completions/max_length', 'completions/clipped_ratio', 'completions/mean_terminated_length', 'completions/min_terminated_length', 'completions/max_terminated_length', 'rewards/numbering_reward_func/mean', 'rewards/numbering_reward_func/std', 'rewards/spelling_reward_func/mean', 'rewards/spelling_reward_func/std', 'rewards/counting_reward_func/mean', 'rewards/counting_reward_func/std', 'rewards/format_reward_func/mean', 'rewards/format_reward_func/std', 'rewards/correct_answer_reward_func/mean', 'rewards/correct_answer_reward_func/std', 'reward', 'reward_std', 'frac_reward_zero_std', 'completion_length', 'kl', 'epoch', 'step'])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.92 s (started: 2025-12-24 18:52:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show the total (sum) of the rewards as well as the correct_answer_reward_func (means with in the batch)\n",
        "# No changes needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want to graph other columns, check these out\n",
        "print(f\"available columns: {trainer.state.log_history[0].keys()}\")\n",
        "\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "log_df[\"reward\"].plot()\n",
        "log_df[\"rewards/correct_answer_reward_func/mean\"].plot()\n",
        "\n",
        "# Show the legend\n",
        "plt.legend([\"reward\", \"rewards/correct_answer_reward_func/mean\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slower train (1+ hour)\n",
        "\n",
        "If everything looks good, let's go for a longer training session!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"glisten\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word glisten\n",
            "1. g - 1 so far\n",
            "2. l - 1 so far\n",
            "3. i - 1 so far\n",
            "4. s - 1 so far\n",
            "5. t - 1 so far\n",
            "6. e - 1 so far\n",
            "7. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 17:49, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / numbering_reward_func / mean</th>\n",
              "      <th>rewards / numbering_reward_func / std</th>\n",
              "      <th>rewards / spelling_reward_func / mean</th>\n",
              "      <th>rewards / spelling_reward_func / std</th>\n",
              "      <th>rewards / counting_reward_func / mean</th>\n",
              "      <th>rewards / counting_reward_func / std</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / correct_answer_reward_func / mean</th>\n",
              "      <th>rewards / correct_answer_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-311.244629</td>\n",
              "      <td>92.521286</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.756696</td>\n",
              "      <td>0.266554</td>\n",
              "      <td>-333.656250</td>\n",
              "      <td>141.137131</td>\n",
              "      <td>0.217411</td>\n",
              "      <td>0.324859</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>1.209339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-243.907990</td>\n",
              "      <td>22.320728</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80.812500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>-265.406250</td>\n",
              "      <td>59.312233</td>\n",
              "      <td>0.123264</td>\n",
              "      <td>0.164670</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-251.954468</td>\n",
              "      <td>30.292261</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.937500</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.891369</td>\n",
              "      <td>0.213485</td>\n",
              "      <td>-273.375000</td>\n",
              "      <td>54.992275</td>\n",
              "      <td>0.216667</td>\n",
              "      <td>0.263459</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-244.930557</td>\n",
              "      <td>24.602139</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.136083</td>\n",
              "      <td>-265.500000</td>\n",
              "      <td>28.481573</td>\n",
              "      <td>-0.097222</td>\n",
              "      <td>0.162161</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-232.445618</td>\n",
              "      <td>42.672146</td>\n",
              "      <td>77.375000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.375000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.874256</td>\n",
              "      <td>0.220756</td>\n",
              "      <td>-254.281250</td>\n",
              "      <td>59.116543</td>\n",
              "      <td>0.086384</td>\n",
              "      <td>0.306983</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-242.999420</td>\n",
              "      <td>19.662172</td>\n",
              "      <td>80.187500</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80.187500</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.850446</td>\n",
              "      <td>0.130396</td>\n",
              "      <td>-264.937500</td>\n",
              "      <td>44.992916</td>\n",
              "      <td>0.212649</td>\n",
              "      <td>0.157479</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>-228.387650</td>\n",
              "      <td>52.014923</td>\n",
              "      <td>72.687500</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>72.687500</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.007068</td>\n",
              "      <td>0.691964</td>\n",
              "      <td>0.187117</td>\n",
              "      <td>-250.031250</td>\n",
              "      <td>62.155575</td>\n",
              "      <td>0.264137</td>\n",
              "      <td>0.284534</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>-264.346802</td>\n",
              "      <td>23.847281</td>\n",
              "      <td>89.437500</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>89.437500</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>0.023698</td>\n",
              "      <td>0.882738</td>\n",
              "      <td>0.103424</td>\n",
              "      <td>-285.562500</td>\n",
              "      <td>32.347527</td>\n",
              "      <td>0.020461</td>\n",
              "      <td>0.147012</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>-182.495972</td>\n",
              "      <td>58.021702</td>\n",
              "      <td>64.937500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>64.937500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>0.083866</td>\n",
              "      <td>0.744792</td>\n",
              "      <td>0.318905</td>\n",
              "      <td>-204.250000</td>\n",
              "      <td>67.750771</td>\n",
              "      <td>0.321726</td>\n",
              "      <td>0.369714</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.081400</td>\n",
              "      <td>-167.833328</td>\n",
              "      <td>14.868142</td>\n",
              "      <td>64.437500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>64.437500</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.203583</td>\n",
              "      <td>0.904018</td>\n",
              "      <td>0.117165</td>\n",
              "      <td>-188.875000</td>\n",
              "      <td>25.731628</td>\n",
              "      <td>0.012649</td>\n",
              "      <td>0.206771</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>-141.675690</td>\n",
              "      <td>47.241661</td>\n",
              "      <td>52.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>0.201941</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.221406</td>\n",
              "      <td>-164.218750</td>\n",
              "      <td>70.005943</td>\n",
              "      <td>0.343056</td>\n",
              "      <td>0.436798</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>1.024695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.118100</td>\n",
              "      <td>-102.464951</td>\n",
              "      <td>21.109987</td>\n",
              "      <td>40.687500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.687500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>0.295325</td>\n",
              "      <td>0.422545</td>\n",
              "      <td>0.359846</td>\n",
              "      <td>-125.250000</td>\n",
              "      <td>49.019726</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.527187</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.812500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.138900</td>\n",
              "      <td>-94.943756</td>\n",
              "      <td>14.927973</td>\n",
              "      <td>37.812500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.812500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.347156</td>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.262823</td>\n",
              "      <td>-117.875000</td>\n",
              "      <td>46.437595</td>\n",
              "      <td>0.722917</td>\n",
              "      <td>0.434054</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.812500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.125200</td>\n",
              "      <td>-112.911758</td>\n",
              "      <td>6.297800</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.313014</td>\n",
              "      <td>0.548214</td>\n",
              "      <td>0.339162</td>\n",
              "      <td>-135.656250</td>\n",
              "      <td>53.758633</td>\n",
              "      <td>0.571280</td>\n",
              "      <td>0.453262</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>1.024695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>-121.845535</td>\n",
              "      <td>66.166786</td>\n",
              "      <td>38.437500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>38.437500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>0.252245</td>\n",
              "      <td>0.301786</td>\n",
              "      <td>0.240004</td>\n",
              "      <td>-144.093750</td>\n",
              "      <td>116.401787</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.592183</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.255200</td>\n",
              "      <td>-64.570534</td>\n",
              "      <td>6.020833</td>\n",
              "      <td>27.125000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.125000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>0.637909</td>\n",
              "      <td>0.221131</td>\n",
              "      <td>0.146142</td>\n",
              "      <td>-87.750000</td>\n",
              "      <td>12.891858</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.095500</td>\n",
              "      <td>-128.857483</td>\n",
              "      <td>90.374863</td>\n",
              "      <td>45.187500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>45.187500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.238698</td>\n",
              "      <td>0.349107</td>\n",
              "      <td>0.295175</td>\n",
              "      <td>-150.312500</td>\n",
              "      <td>94.747894</td>\n",
              "      <td>0.418403</td>\n",
              "      <td>0.701664</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.266400</td>\n",
              "      <td>-72.558037</td>\n",
              "      <td>16.857620</td>\n",
              "      <td>27.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.666113</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>-94.343750</td>\n",
              "      <td>22.319439</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.957427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.138400</td>\n",
              "      <td>-94.289284</td>\n",
              "      <td>38.073292</td>\n",
              "      <td>34.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>34.562500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>0.345886</td>\n",
              "      <td>0.273214</td>\n",
              "      <td>0.214055</td>\n",
              "      <td>-115.468750</td>\n",
              "      <td>65.069313</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.712244</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.078100</td>\n",
              "      <td>-63.935120</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.195164</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-86.250000</td>\n",
              "      <td>2.594867</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.216800</td>\n",
              "      <td>-65.061607</td>\n",
              "      <td>0.420362</td>\n",
              "      <td>25.437500</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.437500</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.542029</td>\n",
              "      <td>0.157143</td>\n",
              "      <td>0.025555</td>\n",
              "      <td>-84.218750</td>\n",
              "      <td>8.166944</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.683130</td>\n",
              "      <td>18.125000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.112600</td>\n",
              "      <td>-63.443451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.281608</td>\n",
              "      <td>0.181548</td>\n",
              "      <td>0.042034</td>\n",
              "      <td>-85.875000</td>\n",
              "      <td>2.045320</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.448100</td>\n",
              "      <td>-68.755951</td>\n",
              "      <td>13.042133</td>\n",
              "      <td>25.062500</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.062500</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1.120162</td>\n",
              "      <td>0.181548</td>\n",
              "      <td>0.042034</td>\n",
              "      <td>-83.687500</td>\n",
              "      <td>15.652343</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>13.416409</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.157000</td>\n",
              "      <td>-62.970837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.392380</td>\n",
              "      <td>0.154167</td>\n",
              "      <td>0.032489</td>\n",
              "      <td>-85.375000</td>\n",
              "      <td>1.176152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.391200</td>\n",
              "      <td>-67.799698</td>\n",
              "      <td>7.937500</td>\n",
              "      <td>26.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.978061</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-90.968750</td>\n",
              "      <td>16.201305</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.141000</td>\n",
              "      <td>-64.232140</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.352508</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-86.000000</td>\n",
              "      <td>2.309401</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.683130</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.354100</td>\n",
              "      <td>-72.814583</td>\n",
              "      <td>16.213089</td>\n",
              "      <td>27.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.885225</td>\n",
              "      <td>0.154167</td>\n",
              "      <td>0.047392</td>\n",
              "      <td>-94.156250</td>\n",
              "      <td>21.841642</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.339300</td>\n",
              "      <td>-84.370537</td>\n",
              "      <td>24.040388</td>\n",
              "      <td>29.875000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>29.875000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.848242</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.067512</td>\n",
              "      <td>-106.281250</td>\n",
              "      <td>30.677879</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.704154</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.436141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.313400</td>\n",
              "      <td>-75.495529</td>\n",
              "      <td>25.062500</td>\n",
              "      <td>28.312500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.312500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.783571</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>-98.406250</td>\n",
              "      <td>26.401527</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.812500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.405500</td>\n",
              "      <td>-69.433037</td>\n",
              "      <td>8.562501</td>\n",
              "      <td>26.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.013778</td>\n",
              "      <td>0.223214</td>\n",
              "      <td>0.047916</td>\n",
              "      <td>-91.156250</td>\n",
              "      <td>16.245737</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.388700</td>\n",
              "      <td>-78.318451</td>\n",
              "      <td>17.093266</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.971773</td>\n",
              "      <td>0.181548</td>\n",
              "      <td>0.042034</td>\n",
              "      <td>-99.062500</td>\n",
              "      <td>26.072895</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.403113</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.471000</td>\n",
              "      <td>-69.959824</td>\n",
              "      <td>8.312501</td>\n",
              "      <td>26.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.750000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.177430</td>\n",
              "      <td>0.196429</td>\n",
              "      <td>0.055328</td>\n",
              "      <td>-92.156250</td>\n",
              "      <td>15.943357</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.062500</td>\n",
              "      <td>1.436141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.386100</td>\n",
              "      <td>-68.770836</td>\n",
              "      <td>8.437500</td>\n",
              "      <td>26.812500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.812500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.197917</td>\n",
              "      <td>0.055902</td>\n",
              "      <td>-91.906250</td>\n",
              "      <td>16.936371</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.183800</td>\n",
              "      <td>-64.441071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.459553</td>\n",
              "      <td>0.183929</td>\n",
              "      <td>0.046181</td>\n",
              "      <td>-86.375000</td>\n",
              "      <td>2.617250</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.338100</td>\n",
              "      <td>-75.618752</td>\n",
              "      <td>18.319443</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.845248</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.025820</td>\n",
              "      <td>-98.468750</td>\n",
              "      <td>26.342279</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.403113</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.812500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.298600</td>\n",
              "      <td>-76.479172</td>\n",
              "      <td>16.823929</td>\n",
              "      <td>28.312500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.312500</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>0.746503</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.043033</td>\n",
              "      <td>-99.687500</td>\n",
              "      <td>25.724745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.323100</td>\n",
              "      <td>-75.893448</td>\n",
              "      <td>24.625000</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.807714</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-98.312500</td>\n",
              "      <td>26.401943</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>-61.724701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.342000</td>\n",
              "      <td>0.150298</td>\n",
              "      <td>0.018120</td>\n",
              "      <td>-84.875000</td>\n",
              "      <td>1.118034</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.160600</td>\n",
              "      <td>-64.418152</td>\n",
              "      <td>1.312500</td>\n",
              "      <td>25.875000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.875000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.401618</td>\n",
              "      <td>0.175595</td>\n",
              "      <td>0.045488</td>\n",
              "      <td>-86.843750</td>\n",
              "      <td>2.612271</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.209200</td>\n",
              "      <td>-61.993153</td>\n",
              "      <td>2.828044</td>\n",
              "      <td>25.625000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.625000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.522902</td>\n",
              "      <td>0.163095</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-83.656250</td>\n",
              "      <td>4.898023</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.211800</td>\n",
              "      <td>-51.330956</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.529466</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-74.500000</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.225200</td>\n",
              "      <td>-53.295837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.563047</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>0.030732</td>\n",
              "      <td>-76.500000</td>\n",
              "      <td>0.816497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.177000</td>\n",
              "      <td>-54.964287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.442463</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.053769</td>\n",
              "      <td>-77.375000</td>\n",
              "      <td>2.513298</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.266700</td>\n",
              "      <td>-54.687500</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.666852</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.037268</td>\n",
              "      <td>-77.125000</td>\n",
              "      <td>1.857418</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.279000</td>\n",
              "      <td>-53.653870</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.697601</td>\n",
              "      <td>0.158631</td>\n",
              "      <td>0.029010</td>\n",
              "      <td>-74.750000</td>\n",
              "      <td>1.390444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>1.436141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.244500</td>\n",
              "      <td>-54.580956</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.611196</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-77.000000</td>\n",
              "      <td>2.097618</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.169500</td>\n",
              "      <td>-53.961906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.423726</td>\n",
              "      <td>0.163095</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-77.125000</td>\n",
              "      <td>2.045320</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>-55.455956</td>\n",
              "      <td>0.433013</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.749960</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-77.500000</td>\n",
              "      <td>1.751190</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.226200</td>\n",
              "      <td>-55.808334</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.565619</td>\n",
              "      <td>0.191667</td>\n",
              "      <td>0.014907</td>\n",
              "      <td>-77.125000</td>\n",
              "      <td>1.765408</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.806226</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.619000</td>\n",
              "      <td>-59.028870</td>\n",
              "      <td>6.895833</td>\n",
              "      <td>24.312500</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.312500</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>1.547451</td>\n",
              "      <td>0.158631</td>\n",
              "      <td>0.047228</td>\n",
              "      <td>-81.375000</td>\n",
              "      <td>13.622898</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.981071</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.410200</td>\n",
              "      <td>-53.211906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1.025473</td>\n",
              "      <td>0.163095</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-75.625000</td>\n",
              "      <td>0.763763</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.755800</td>\n",
              "      <td>-53.724701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.062500</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.062500</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>1.889559</td>\n",
              "      <td>0.150298</td>\n",
              "      <td>0.018120</td>\n",
              "      <td>-74.875000</td>\n",
              "      <td>0.763763</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.518400</td>\n",
              "      <td>-55.310120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1.296108</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-77.750000</td>\n",
              "      <td>1.183216</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.395100</td>\n",
              "      <td>-54.510120</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.987827</td>\n",
              "      <td>0.177381</td>\n",
              "      <td>0.024926</td>\n",
              "      <td>-76.375000</td>\n",
              "      <td>0.763763</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.432300</td>\n",
              "      <td>-53.810120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1.080829</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-76.250000</td>\n",
              "      <td>1.183216</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>-52.816669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.809690</td>\n",
              "      <td>0.183333</td>\n",
              "      <td>0.017213</td>\n",
              "      <td>-75.250000</td>\n",
              "      <td>1.064581</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.623600</td>\n",
              "      <td>-54.889286</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>24.125000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.125000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>1.558914</td>\n",
              "      <td>0.173214</td>\n",
              "      <td>0.052651</td>\n",
              "      <td>-77.937500</td>\n",
              "      <td>1.590335</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>-54.592857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.625960</td>\n",
              "      <td>0.157143</td>\n",
              "      <td>0.025555</td>\n",
              "      <td>-77.250000</td>\n",
              "      <td>1.983263</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>-52.060120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.569987</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-75.250000</td>\n",
              "      <td>1.183216</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.188700</td>\n",
              "      <td>-53.060120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.471827</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-76.250000</td>\n",
              "      <td>1.390444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.290200</td>\n",
              "      <td>-53.330956</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.725411</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-76.500000</td>\n",
              "      <td>1.591645</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.228300</td>\n",
              "      <td>-56.580956</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.570634</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-77.500000</td>\n",
              "      <td>2.607681</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.271500</td>\n",
              "      <td>-54.372620</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.678765</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-76.250000</td>\n",
              "      <td>1.183216</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>-54.476189</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.958971</td>\n",
              "      <td>0.148810</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>-76.875000</td>\n",
              "      <td>0.991632</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.186300</td>\n",
              "      <td>-54.107140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.465655</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-76.500000</td>\n",
              "      <td>2.065591</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>-53.795837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.569428</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>0.030732</td>\n",
              "      <td>-77.000000</td>\n",
              "      <td>2.160247</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.201500</td>\n",
              "      <td>-53.891071</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.503635</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.029508</td>\n",
              "      <td>-76.500000</td>\n",
              "      <td>1.264911</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>1.209339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.225600</td>\n",
              "      <td>-52.464287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.563922</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>-75.625000</td>\n",
              "      <td>1.607275</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.192400</td>\n",
              "      <td>-56.637501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.480953</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.022361</td>\n",
              "      <td>-79.125000</td>\n",
              "      <td>1.284523</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.201600</td>\n",
              "      <td>-53.074402</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.503985</td>\n",
              "      <td>0.175595</td>\n",
              "      <td>0.045488</td>\n",
              "      <td>-75.500000</td>\n",
              "      <td>2.449490</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.206100</td>\n",
              "      <td>-52.824402</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.515175</td>\n",
              "      <td>0.175595</td>\n",
              "      <td>0.045488</td>\n",
              "      <td>-76.000000</td>\n",
              "      <td>0.966092</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>-55.053570</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.705704</td>\n",
              "      <td>0.196429</td>\n",
              "      <td>0.055328</td>\n",
              "      <td>-76.750000</td>\n",
              "      <td>1.653280</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.412700</td>\n",
              "      <td>-52.552086</td>\n",
              "      <td>1.183013</td>\n",
              "      <td>23.812500</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.812500</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1.031739</td>\n",
              "      <td>0.197917</td>\n",
              "      <td>0.055902</td>\n",
              "      <td>-75.375000</td>\n",
              "      <td>2.156386</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>1.024695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.312200</td>\n",
              "      <td>-54.584820</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.780520</td>\n",
              "      <td>0.165179</td>\n",
              "      <td>0.051135</td>\n",
              "      <td>-76.250000</td>\n",
              "      <td>1.653280</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.323500</td>\n",
              "      <td>-54.151787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.808748</td>\n",
              "      <td>0.223214</td>\n",
              "      <td>0.047916</td>\n",
              "      <td>-77.375000</td>\n",
              "      <td>1.910497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.272900</td>\n",
              "      <td>-56.458038</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.682198</td>\n",
              "      <td>0.166964</td>\n",
              "      <td>0.034737</td>\n",
              "      <td>-78.125000</td>\n",
              "      <td>1.910497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.257400</td>\n",
              "      <td>-54.091370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.643498</td>\n",
              "      <td>0.158631</td>\n",
              "      <td>0.029010</td>\n",
              "      <td>-77.250000</td>\n",
              "      <td>2.144761</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.221500</td>\n",
              "      <td>-52.441071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.553722</td>\n",
              "      <td>0.183929</td>\n",
              "      <td>0.046181</td>\n",
              "      <td>-74.875000</td>\n",
              "      <td>1.522060</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.315200</td>\n",
              "      <td>-54.572918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.787897</td>\n",
              "      <td>0.177083</td>\n",
              "      <td>0.046894</td>\n",
              "      <td>-77.000000</td>\n",
              "      <td>2.422120</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.338700</td>\n",
              "      <td>-52.155357</td>\n",
              "      <td>1.875000</td>\n",
              "      <td>23.625000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.625000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.846852</td>\n",
              "      <td>0.157143</td>\n",
              "      <td>0.025555</td>\n",
              "      <td>-74.000000</td>\n",
              "      <td>2.366432</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>1.537043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.372700</td>\n",
              "      <td>-53.685120</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>23.812500</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.812500</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.931801</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-76.875000</td>\n",
              "      <td>2.061553</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.378600</td>\n",
              "      <td>-54.322620</td>\n",
              "      <td>1.616025</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.437500</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.946500</td>\n",
              "      <td>0.177381</td>\n",
              "      <td>0.024926</td>\n",
              "      <td>-76.750000</td>\n",
              "      <td>3.224903</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.449800</td>\n",
              "      <td>-50.345238</td>\n",
              "      <td>3.232051</td>\n",
              "      <td>22.875000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.875000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1.124530</td>\n",
              "      <td>0.154762</td>\n",
              "      <td>0.012295</td>\n",
              "      <td>-73.500000</td>\n",
              "      <td>3.366502</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.782100</td>\n",
              "      <td>-49.342857</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>21.750000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.750000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1.955317</td>\n",
              "      <td>0.157143</td>\n",
              "      <td>0.025555</td>\n",
              "      <td>-71.750000</td>\n",
              "      <td>3.044120</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.718100</td>\n",
              "      <td>-47.961906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.795171</td>\n",
              "      <td>0.163095</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-71.125000</td>\n",
              "      <td>2.291288</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.785900</td>\n",
              "      <td>-46.580956</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.964749</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-69.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.655300</td>\n",
              "      <td>-47.824402</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.638136</td>\n",
              "      <td>0.175595</td>\n",
              "      <td>0.045488</td>\n",
              "      <td>-71.000000</td>\n",
              "      <td>1.751190</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.681200</td>\n",
              "      <td>-47.711906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.703067</td>\n",
              "      <td>0.163095</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-69.625000</td>\n",
              "      <td>2.642600</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.637700</td>\n",
              "      <td>-47.816669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.594266</td>\n",
              "      <td>0.183333</td>\n",
              "      <td>0.017213</td>\n",
              "      <td>-70.250000</td>\n",
              "      <td>2.435843</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.714300</td>\n",
              "      <td>-47.789284</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.785681</td>\n",
              "      <td>0.210714</td>\n",
              "      <td>0.045625</td>\n",
              "      <td>-71.000000</td>\n",
              "      <td>1.825742</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.761800</td>\n",
              "      <td>-48.901787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.904623</td>\n",
              "      <td>0.223214</td>\n",
              "      <td>0.047916</td>\n",
              "      <td>-71.375000</td>\n",
              "      <td>0.991632</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.680400</td>\n",
              "      <td>-49.039284</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.700974</td>\n",
              "      <td>0.210714</td>\n",
              "      <td>0.045625</td>\n",
              "      <td>-71.500000</td>\n",
              "      <td>1.211060</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.658400</td>\n",
              "      <td>-46.830956</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.646113</td>\n",
              "      <td>0.169048</td>\n",
              "      <td>0.021010</td>\n",
              "      <td>-70.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.669700</td>\n",
              "      <td>-48.310120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.674224</td>\n",
              "      <td>0.189881</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>-70.750000</td>\n",
              "      <td>1.390444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.750700</td>\n",
              "      <td>-48.724701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.876830</td>\n",
              "      <td>0.150298</td>\n",
              "      <td>0.018120</td>\n",
              "      <td>-69.875000</td>\n",
              "      <td>1.384437</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.549193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.679100</td>\n",
              "      <td>-49.025002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.697750</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.025820</td>\n",
              "      <td>-72.250000</td>\n",
              "      <td>1.653280</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.651500</td>\n",
              "      <td>-47.842857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.628655</td>\n",
              "      <td>0.157143</td>\n",
              "      <td>0.025555</td>\n",
              "      <td>-70.250000</td>\n",
              "      <td>2.113449</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.739100</td>\n",
              "      <td>-47.941071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.847646</td>\n",
              "      <td>0.183929</td>\n",
              "      <td>0.046181</td>\n",
              "      <td>-70.375000</td>\n",
              "      <td>1.176152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.795700</td>\n",
              "      <td>-48.105652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.989242</td>\n",
              "      <td>0.144345</td>\n",
              "      <td>0.015292</td>\n",
              "      <td>-70.000000</td>\n",
              "      <td>1.414214</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.725200</td>\n",
              "      <td>-47.214287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.812895</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>-69.625000</td>\n",
              "      <td>1.607275</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.341641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word sapphire\n",
            "1. s - 0 so far\n",
            "2. a - 1 so far\n",
            "3. p - 1 so far\n",
            "4. a - 2 so far\n",
            "5. r - 2 so far\n",
            "6. p - 3 so far\n",
            "7. h - 3 so far\n",
            "8. a - 4 so far\n",
            "9. y - 4 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "Extracted: 4\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"absolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of t's in the word absolve\n",
            "1. a - 0 so far\n",
            "2. b - 0 so far\n",
            "3. s - 1 so far\n",
            "4. p - 0 so far\n",
            "5. o - 1 so far\n",
            "6. l - 1 so far\n",
            "7. e - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of g's in the word \"mirage\"\n",
            "1. m - 0 so far\n",
            "2. i - 0 so far\n",
            "3. r - 0 so far\n",
            "4. a - 0 so far\n",
            "5. g - 1 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of r's in the word crave\n",
            "1. c - 0 so far\n",
            "2. r - 1 so far\n",
            "3. a - 1 so far\n",
            "4. v - 1 so far\n",
            "5. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"frescos\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "Counting the number of w's in the word frescos\n",
            "1. f - 0 so far\n",
            "2. r - 0 so far\n",
            "3. e - 0 so far\n",
            "4. s - 0 so far\n",
            "5. c - 0 so far\n",
            "6. o - 0 so far\n",
            "7. w - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"v\" are there in the word \"absolve\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of v's in the word \"absolve\"\n",
            "1. a - 1 so far\n",
            "2. b - 1 so far\n",
            "3. s - 1 so far\n",
            "4. v - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"eclipse\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. e - 0 so far\n",
            "2. c - 0 so far\n",
            "3. l - 0 so far\n",
            "4. p - 0 so far\n",
            "5. l - 1 so far\n",
            "6. c - 2 so far\n",
            "7. e - 3 so far\n",
            "So, the letter \"i\" is not present in the word \"eclipse\".\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"echo\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "Counting the number of e's in the word echo\n",
            "1. e - 1 so far\n",
            "2. c - 0 so far\n",
            "3. h - 0 so far\n",
            "4. o - 0 so far\n",
            "5. c - 0 so far\n",
            "6. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"d\" are there in the word \"void\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. v - 1 so far\n",
            "2. o - 2 so far\n",
            "3. i - 3 so far\n",
            "4. d - 4 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "4\n",
            "</answer>\n",
            "Extracted: 4\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i - 1 so far\n",
            "2. v - 1 so far\n",
            "3. r - 1 so far\n",
            "4. o - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"aperture\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. a - 1 so far\n",
            "2. p - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"fume\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. m - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"crave\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"f\" are there in the word \"frescos\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. f - [1st letter] - 1 so far\n",
            "2. r - 0 so far\n",
            "3. e - 0 so far\n",
            "4. s - 0 so far\n",
            "5. o - 0 so far\n",
            "6. c - 0 so far\n",
            "7. s - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"b\" are there in the word \"banner\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. b - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"f\" are there in the word \"capture\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. f - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"sphinx\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. p - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"k\" are there in the word \"knack\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. k - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"relish\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"zealous\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. m - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"idea\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. s - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"tavern\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. t - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. a - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i - 2 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "2\n",
            "</answer>\n",
            "Extracted: 2\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"torrent\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. y - 0 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"brawn\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. w - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"fathom\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. m - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"u\" are there in the word \"echo\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. u - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"enchant\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"lush\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. h - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"fusion\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"x\" are there in the word \"maze\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. x - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"onset\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. t - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"veto\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. o - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. y - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"torrent\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. n - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. e - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"k\" are there in the word \"resolve\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. k - 1 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"mirage\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"lunar\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. t - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"sapphire\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. p - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"ivory\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"triumph\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"zealous\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"verge\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. e - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"onset\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. g - 0\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"crisp\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"y\" are there in the word \"lantern\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. y - 0\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"fusion\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. h - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"onset\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. o - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"wisp\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. c - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"radius\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. a - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"j\" are there in the word \"elude\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. j - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"enchant\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. a - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"orchard\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. h - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"maze\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. m - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. e - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"gossipy\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. l - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"q\" are there in the word \"ivory\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. q - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"capture\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"prelude\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. e - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"crisp\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"enchant\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. t - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"scarab\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"oath\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. a - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"prelude\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. h - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. m - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"grim\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. a - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"h\" are there in the word \"oath\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. h - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"aperture\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. e - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"rust\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"aperture\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. s - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"a\" are there in the word \"sapphire\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. a - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"torrent\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. t - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"mantle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. n - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"o\" are there in the word \"gossipy\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. o - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"m\" are there in the word \"veto\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. m - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"sphinx\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i - 1\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"resolve\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. l\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"absolve\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"orchard\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"l\" are there in the word \"banner\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. l\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"t\" are there in the word \"fathom\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. t\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"triumph\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. p\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"elude\"\n",
            "Answer: 2\n",
            "Response: <reasoning>\n",
            "1. e\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"e\" are there in the word \"quest\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. e\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"r\" are there in the word \"grim\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. r\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"g\" are there in the word \"enchant\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. g\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: False!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"fusion\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"s\" are there in the word \"whistle\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. s\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"z\" are there in the word \"aperture\"\n",
            "Answer: 0\n",
            "Response: <reasoning>\n",
            "1. z\n",
            "</reasoning>\n",
            "<answer>\n",
            "0\n",
            "</answer>\n",
            "Extracted: 0\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"w\" are there in the word \"glow\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. w\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"n\" are there in the word \"knack\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. n\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"p\" are there in the word \"eclipse\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. p\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"c\" are there in the word \"capture\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. c\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "\n",
            "--------------------\n",
            "Question: How many of the letter \"i\" are there in the word \"gossipy\"\n",
            "Answer: 1\n",
            "Response: <reasoning>\n",
            "1. i\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "Extracted: 1\n",
            "Correct: True!\n",
            "    \n",
            "time: 18min 27s (started: 2025-12-24 18:52:34 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Now let's train for real! Let's do a longer training that will take an hour or more\n",
        "# Note: If this run is successful, you can consider doing a longer train\n",
        "# to see what happens, but that's beyond the scope of this project.\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Fix the recompilation limit error\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.cache_size_limit = 256  # Increase from default 64\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.reset()\n",
        "\n",
        "# Also add this warning suppressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Full training\n",
        "training_args = GRPOConfig(\n",
        "    **COMMON_GRPO_TRAINING_PARAMS,\n",
        "    # Configure the maximum number of steps to take about 30mins of time for\n",
        "    # a medium-sized experiment. (See how long the previous example took and\n",
        "    # scale up appropriately using your best guess.)\n",
        "    # max_steps=**********,  # ~60min\n",
        "    max_steps = 100,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=REWARD_FUNCS,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "trainer_res = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available columns: dict_keys(['loss', 'grad_norm', 'learning_rate', 'num_tokens', 'completions/mean_length', 'completions/min_length', 'completions/max_length', 'completions/clipped_ratio', 'completions/mean_terminated_length', 'completions/min_terminated_length', 'completions/max_terminated_length', 'rewards/numbering_reward_func/mean', 'rewards/numbering_reward_func/std', 'rewards/spelling_reward_func/mean', 'rewards/spelling_reward_func/std', 'rewards/counting_reward_func/mean', 'rewards/counting_reward_func/std', 'rewards/format_reward_func/mean', 'rewards/format_reward_func/std', 'rewards/correct_answer_reward_func/mean', 'rewards/correct_answer_reward_func/std', 'reward', 'reward_std', 'frac_reward_zero_std', 'completion_length', 'kl', 'epoch', 'step'])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 161 ms (started: 2025-12-24 19:11:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show the total (sum) of the rewards as well as the correct_answer_reward_func (means with in the batch)\n",
        "# Do you see the rewards increasing? Does the model get the correct answer\n",
        "# more frequently toward the end?\n",
        "# No changes needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want to graph other columns, check these out\n",
        "print(f\"available columns: {trainer.state.log_history[0].keys()}\")\n",
        "\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "log_df[\"reward\"].plot()\n",
        "log_df[\"rewards/correct_answer_reward_func/mean\"].plot()\n",
        "\n",
        "# Show the legend\n",
        "plt.legend([\"reward\", \"rewards/correct_answer_reward_func/mean\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View the results\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('grpo_saved_lora/tokenizer_config.json',\n",
              " 'grpo_saved_lora/special_tokens_map.json',\n",
              " 'grpo_saved_lora/chat_template.jinja',\n",
              " 'grpo_saved_lora/vocab.json',\n",
              " 'grpo_saved_lora/merges.txt',\n",
              " 'grpo_saved_lora/added_tokens.json',\n",
              " 'grpo_saved_lora/tokenizer.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 3.33 s (started: 2025-12-24 19:11:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Save the LoRA adapters\n",
        "# No changes needed in this cell\n",
        "\n",
        "# Save the LoRA model\n",
        "model.save_pretrained(\"grpo_saved_lora\")\n",
        "tokenizer.save_pretrained(\"grpo_saved_lora\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.6 ms (started: 2025-12-24 19:11:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Create a function to run both the original model and the updated model\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "\"\"\"def compare_old_and_new_model(messages):\n",
        "    from vllm import SamplingParams\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "    old = (\n",
        "        model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "        )[0]\n",
        "        .outputs[0]\n",
        "        .text\n",
        "    )\n",
        "\n",
        "    new = (\n",
        "        model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "            lora_request=model.load_lora(\"grpo_saved_lora\"),\n",
        "        )[0]\n",
        "        .outputs[0]\n",
        "        .text\n",
        "    )\n",
        "\n",
        "    print(\"===OLD===\\n\")\n",
        "    print(old)\n",
        "\n",
        "    print(\"\\n\\n===NEW===\\n\")\n",
        "    print(new)\"\"\"\n",
        "\n",
        "def compare_old_and_new_model(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate with OLD (base) model - adapters disabled\n",
        "    model.disable_adapter_layers()\n",
        "    outputs_old = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    old = tokenizer.decode(outputs_old[0], skip_special_tokens=True)\n",
        "\n",
        "    # Generate with NEW (fine-tuned) model - adapters enabled\n",
        "    model.enable_adapter_layers()\n",
        "    outputs_new = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    new = tokenizer.decode(outputs_new[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"===OLD===\\n\")\n",
        "    print(old)\n",
        "\n",
        "    print(\"\\n\\n===NEW===\\n\")\n",
        "    print(new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the old and new models on the letter-counting task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===OLD===\n",
            "\n",
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "Counting the number of a's in the word idea\n",
            "1. i - 0 so far\n",
            "2. d - 1 so far\n",
            "3. e - 2 so far\n",
            "4. a - 3 so far\n",
            "</reasoning>\n",
            "<answer>\n",
            "3\n",
            "</answer>\n",
            "\n",
            "\n",
            "===NEW===\n",
            "\n",
            "system\n",
            "\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "Counting the number of [letter_to_count]'s in the word [word]\n",
            "1. [first letter] - [count of requested letter so far] so far\n",
            "2. [second letter] - [count of requested letter so far] so far\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "[number]\n",
            "</answer>\n",
            "\n",
            "user\n",
            "How many of the letter \"a\" are there in the word \"idea\"\n",
            "assistant\n",
            "<reasoning>\n",
            "1. a\n",
            "</reasoning>\n",
            "<answer>\n",
            "1\n",
            "</answer>\n",
            "time: 5.28 s (started: 2025-12-24 19:11:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's try spelling the first word from the dataset\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Load the first item from the dataset (index 0) and compare the old and new models\n",
        "# **********\n",
        "\"\"\"text = tokenizer.apply_chat_template(\n",
        "    ds[0][\"prompt\"], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== PROMPT ===\")\n",
        "print(text)\n",
        "print(\"\\n=== FINE-TUNED MODEL OUTPUT ===\")\n",
        "print(output)\"\"\"\n",
        "\n",
        "compare_old_and_new_model(ds[0][\"prompt\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model is better at spelling and counter letters in words! Depending on your reward functions, the size of your model, and the amount of steps trained, results may vary.\n",
        "\n",
        "For about an hour of training time, your model may not be perfect (or maybe it is), but it's definitely moving in the right direction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make sure the model did not forget basic facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===OLD===\n",
            "\n",
            "system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "user\n",
            "What is the capital of Spain?\n",
            "assistant\n",
            "The capital of Spain is Madrid.\n",
            "\n",
            "\n",
            "===NEW===\n",
            "\n",
            "system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "user\n",
            "What is the capital of Spain?\n",
            "assistant\n",
            "The capital of Spain is Madrid.\n",
            "time: 1.24 s (started: 2025-12-24 19:11:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Let's see if the model still remembers some of the facts from its original training\n",
        "# TODO: Fill out the areas where you find **********\n",
        "\n",
        "# Ask both the old and new models a question the model is likely to know,\n",
        "# e.g. a well-known capital city\n",
        "# **********\n",
        "question = \"What is the capital of Spain?\"\n",
        "\n",
        "\"\"\"text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": question}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== QUESTION ===\")\n",
        "print(question)\n",
        "print(\"\\n=== FINE-TUNED MODEL ANSWER ===\")\n",
        "print(output)\"\"\"\n",
        "\n",
        "compare_old_and_new_model([{\"role\": \"user\", \"content\": question}])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great job! Congrats on completing the project! ðŸŽ‰ðŸ¤—"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (venv2)",
      "language": "python",
      "name": "venv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
